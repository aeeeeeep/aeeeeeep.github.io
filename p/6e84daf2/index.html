<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>Faster-RCNN原理笔记 | Aeeeeeep Blog</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.0.0"></head><body><main class="content"><section class="outer"><article id="post-Faster-RCNN原理笔记" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">Faster-RCNN原理笔记</h1></header><div class="article-meta"><a href="/p/6e84daf2/" class="article-date"><time datetime="2022-03-30T07:02:16.000Z" itemprop="datePublished">2022-03-30</time></a><div class="article-category"><a class="article-category-link" href="/categories/%E6%87%B5%E9%80%BC%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">懵逼的深度学习</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>详细记录了 Faster RCNN 网络原理和个人的理解</p><span id="more"></span><ul><li>[论文下载](</li><li>[论文源码](</li></ul><h2 id="前期"><a href="#前期" class="headerlink" title="前期"></a>前期</h2><p>[Tensorflow 2.0 基础](</p><p>[RCNN 原理](</p><p>[Fast RCNN 原理](</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="论文中的网络结构图解"><a href="#论文中的网络结构图解" class="headerlink" title="论文中的网络结构图解"></a>论文中的网络结构图解</h3><p><img src="/image/Faster-RCNN原理笔记/faster-rcnn-network.webp" style="zoom:30%"></p><p>主要步骤是</p><ol><li>输入图片</li><li>对图片进行卷积，提取特征</li><li>使用 RPN 网络生成 Anchor box，对其裁剪过滤后，通过 softmax 对前景和后景分类，同时，bounding box regression 修正 anchor box，形成校正后的 proposals</li><li>将 proposals 映射到 feature maps 上</li><li>通过 RoI pooling 层使每个 RoI 生成固定尺寸的 feature map</li><li>利用 Softmax Loss 和 Smooth L1 Loss 对分类概率和边框回归联合训练</li></ol><h3 id="Faster-RCNN-具体的网络结构图"><a href="#Faster-RCNN-具体的网络结构图" class="headerlink" title="Faster RCNN 具体的网络结构图"></a>Faster RCNN 具体的网络结构图</h3><p><img src="/image/Faster-RCNN原理笔记/construction.webp" alt="construction"></p><h3 id="主干特征提取网络"><a href="#主干特征提取网络" class="headerlink" title="主干特征提取网络"></a>主干特征提取网络</h3><p>可选 ResNet，MobileNet，VGG16 等网络，本模型使用的是 VGG16 网络，由卷积层模块后接全连接层模块构成，每个卷积层的参数分别为 <code>kernel_size=(3,3), padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_regularizer=&#39;l2&#39;</code>，最大池化层的参数为 <code>pool_size=(2,2), padding=&#39;same&#39;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">____________________________________________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                                 Output Shape                            Param <span class="comment">#        </span></span><br><span class="line">====================================================================================================</span><br><span class="line">input_1 (InputLayer)                         [(<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">3</span>)]                   <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d (Conv2D)                              (<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">64</span>)                    <span class="number">1792</span>           </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)                            (<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">64</span>)                    <span class="number">36928</span>          </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D)                 (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">64</span>)                    <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)                            (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">128</span>)                   <span class="number">73856</span>          </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D)                            (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">128</span>)                   <span class="number">147584</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">128</span>)                   <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_4 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">295168</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_5 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">590080</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_6 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">590080</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">256</span>)                     <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_7 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">1180160</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_8 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_9 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_10 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_11 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_12 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dense (Dense)                                (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">10</span>)                      <span class="number">5130</span>           </span><br><span class="line">====================================================================================================</span><br><span class="line">Total params: <span class="number">14</span>,<span class="number">719</span>,<span class="number">818</span></span><br><span class="line">Trainable params: <span class="number">14</span>,<span class="number">719</span>,<span class="number">818</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">____________________________________________________________________________________________________</span><br></pre></td></tr></table></figure><p>使用 VGG16 网络不像resnet那么复杂，更深的网络理论上效果也更好</p><h3 id="RPN-Region-Proposal-Networks"><a href="#RPN-Region-Proposal-Networks" class="headerlink" title="RPN (Region Proposal Networks)"></a>RPN (Region Proposal Networks)</h3><p>在图像中产生所有可能为目标的候选区域，用来解决生成检测框耗时较多的问题。RPN 根据 CNN 生成的特征图，在 img 的尺度上生成多个锚框，对生成的锚框进行分类和回归。</p><p><img src="/image/Faster-RCNN原理笔记/rpn.webp" alt="rpn" style="zoom:67%"></p><p>网络分为2条线，上面一条通过softmax分类 anchors 获得positive 和 negative 分类，下面一条用于计算对于 anchors 的 bounding box regression 偏移量，获得精确的 proposal。最后的 Proposal layer 负责综合 positive anchors 和对应 bounding box regression 偏移量获取 proposals，同时剔除太小和超出边界的 proposals</p><h4 id="ahchors"><a href="#ahchors" class="headerlink" title="ahchors"></a>ahchors</h4><p>是一种多尺度方法，以一个像素点为中心，生成一组描述 9 个矩形的矩阵，每行4个值 $(x_min, y_min, x_max, y_max)$ 表示矩形左上和右下角点坐标，长宽比为 $ width:height \in { 1:1, 1:2, 2:1 } $</p><p><img src="/image/Faster-RCNN原理笔记/anchors.webp" alt="anchors"></p><p>其中，anchors size 是根据检测图像设置的，Faster RCNN网络会把所有输入的图像 reshape 成固定大小，在论文中，会为 feature map 中的每个像素点生成 anchors，后面的2次 bounding box regression 会修正 anchors 检测框位置</p><p><img src="/image/Faster-RCNN原理笔记/anchors.webp" alt="anchors" style="zoom:67%"></p><p>上图截取自论文，其中</p><ul><li>256-d: 论文中主干特征提取网络的最后一层 num_output=256，对应生成的 feature map 是256维的</li><li>sliding window: feature map 在进入 RPN 网络后，又进行了一次 3x3 的卷积，256-d 没有变</li><li>$cls \quad layer$: 已知每个像素点上有 k 个 anchor(图中 k = 9)，每个 anchor 要分前景(positive)和背景(negative)，所以每个点由 256-d 的 feature map 转化为 2k scores</li><li>$reg \quad layer$: 已知每个像素点上有 k 个 anchor(图中 k = 9)，每个 anchor 有 $(x, y, w, h)$ 对应的4个偏移量，所以每个点由 256-d 的 feature map 转化为 4k coordinates</li></ul><p><img src="/image/Faster-RCNN原理笔记/gernerate_anchors.webp" alt="gernerate_anchors" style="zoom:60%"></p><p>上图以 图片大小 500x500 为例，计算生成的 gernerate anchors 的数量</p><script type="math/tex;mode=display">\operatorname{ceil}(500 / 16) \times \operatorname{ceil}(500 / 16) \times 9=32 \times 32 \times 9= 9216</script><p>ceil()为向上取整，因为图中VGG16网络输出的 feature map size 为整数</p><h4 id="判定-positive-negative"><a href="#判定-positive-negative" class="headerlink" title="判定 positive/negative"></a>判定 positive/negative</h4><p>主要步骤：</p><ol><li>RPN 网络图中上面一条输入为共享层卷积的输出</li><li>进行通道数为2k(k=num_anchors)的 1x1 卷积</li><li>reshape 成两个通道</li><li>对通道层做归一化，使类别预测的概率和为 1</li><li>取最终的预测类别和概率</li><li>reshape 回复原状 <code>[1, h, w, 9*2]</code></li></ol><p>论文作者在源码中的 softmax_loss_layer.cpp 对最后 reshape层 的解释:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Number of labels must match number of predictions; &quot;</span></span><br><span class="line"><span class="string">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;</span></span><br><span class="line"><span class="string">&quot;label count (number of labels) must be N*H*W, &quot;</span></span><br><span class="line"><span class="string">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h4><p>图中所示，绿色框为苹果的 ground truth，红色为提取的 positive anchors，即便红色的框被分类器识别为苹果，但是由于红色的框定位不准，这张图相当于没有正确的检测出苹果。所以需要采用一种方法对红色的框进行微调，使得 positive anchors 和 ground truth 更加接近</p><p><img src="/image/Faster-RCNN原理笔记/bounding.webp" alt="bounding" style="zoom:75%"></p><p>对于窗口一般使用四维向量 $(x, y, w, h)$ 表示，分别表示窗口的中心点坐标和宽高，红框代表原始的positive anchors，绿框代表目标的 ground truth，使得输入原始的 anchor 经过映射得到一个跟 ground truth 更接近的回归窗口，即</p><p>positive anchors: $A = (A_x, A_y, A_w, A_h)$</p><p>ground truth: $GT = (G_x, G_y, G_w, G_h)$</p><p>寻找 $F$，使 $F(A) = (G<em>{x}^{\prime}, G</em>{y}^{\prime}, G<em>{w}^{\prime}, G</em>{h}^{\prime})$</p><p>其中 $(G<em>{x}^{\prime}, G</em>{y}^{\prime}, G<em>{w}^{\prime}, G</em>{h}^{\prime}) \approx (G<em>{x}, G</em>{y}, G<em>{w}, G</em>{h})$</p><p><img src="/image/Faster-RCNN原理笔记/fag.webp" alt="fag" style="zoom:33%"></p><p>通过变换 $F$从 $A$ 变换到 $G’$，我们要做的是</p><p>平移</p><script type="math/tex;mode=display">G_x^\prime = A_w \cdot d_x(A) + A_x \\
G_y^\prime = A_h \cdot d_y(A) + A_y</script><p>缩放</p><script type="math/tex;mode=display">G_w^\prime = A_w \cdot exp(d_w(A))\\
G_h^\prime = A_h \cdot exp(d_h(A))</script><p>需要学习的是 $d_x(A), d_y(A), d_w(A) ,d_h(A)$ 这四个变换。当输入的 $A$ 与 $GT$ 相差较小时，认为这种变换是一种线性变换， 用线性回归来建模对窗口进行微调，当 $A$ 和 $GT$ 比较接近时，认为是复杂的非线性问题</p><p>已知线性回归公式 $Y = WX$，$X$ 为 feature map，定义为 $\phi$，训练传入$A$与$GT$之间的变换量 $(t_x, t_y, t_w, t_h, )$，$Y$为 $(d_x(A), d_y(A), d_w(A) ,d_h(A))$，则目标函数为</p><script type="math/tex;mode=display">d_*(A) = W_*^T\phi(A)</script><p>其中 $\phi(A)$ 是对应 anchor 的 feature map 组成的特征向量，$W<em>{*}$ 是需要学习的参数，$d</em>{*}(A)$ 是得到的预测值</p><p>在 Faster RCNN 论文中，positive anchor 与 ground truth 之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$ 如下</p><script type="math/tex;mode=display">t_x = (x-x_a)/w_a \quad t_y = (y-y_a)/h_a \\
t_w = \log(w/w_a) \quad t_h = \log(h/h_a)</script><p>为了让预测值 $ {d<em>{*}(A) }$ 与真实值差距最小， ${\operatorname{smooth}</em>{L_{1}}}$ 损失函数为</p><script type="math/tex;mode=display">Loss = \begin{cases} 
0.5 \cdot (\sum_i^N (t_*^i -W_*^T \cdot \phi(A^i))^2 & \text{if}|x| <1 \\
\sum_i^N |t_*^i -W_*^T \cdot \phi(A^i)| - 0.5 & \text{otherwise} \\
\end{cases}</script><p>优化目标函数为</p><script type="math/tex;mode=display">\hat{W}_* = {argmin}_{W_*} \sum_i^n (t_*^i -W_*^T \cdot \phi(A^i))^2 + \lambda \| W_* \|</script><p>之后可通过梯度下降等方法修正 anchor 位置，注意当 $A$ 和 $GT$ 比较接近时，才可近似认为上述线性变换及优化目标函数成立</p><h4 id="对-proposals-进行-bounding-box-regression"><a href="#对-proposals-进行-bounding-box-regression" class="headerlink" title="对 proposals 进行 bounding box regression"></a>对 proposals 进行 bounding box regression</h4><p>在第二条线路中，num_output=36，即经过该卷积输出图像为 WxHx36，存储为 <code>[1, 4x9, H, W]</code>，这里相当于 feature maps 每个点都有9个 anchors，每个 anchors 又都有4个用于回归的 $(d_x(A), d_y(A), d_w(A) ,d_h(A))$ 变换量</p><p>VGG16 网络输出 $32 <em>32 </em>512$ 的特征，对应设置 $32<em>32</em>k$ 个 anchors，因此RPN输出</p><ul><li><p>大小为 $32 <em>32 </em>2k$ 的 positive/negative softmax 分类特征矩阵</p></li><li><p>大小为 $32 <em>32 </em>4k$ 的 regression 坐标回归特征矩阵</p></li></ul><p>对应 RPN 的 positive/negative 分类和 bounding box regression 坐标回归</p><h4 id="Proposal-Layer"><a href="#Proposal-Layer" class="headerlink" title="Proposal Layer"></a>Proposal Layer</h4><p>Proposal Layer负责综合所有 $(d_x(A), d_y(A), d_w(A), d_h(A))$ 变换量和 positive anchors，计算出精准的proposal，送入后续 RoI Pooling Layer</p><p><img src="/image/Faster-RCNN原理笔记/proposal_layer.webp" style="zoom:67%"></p><p>Proposal Layer有3个输入：positive/negative anchors 分类器结果 rpn_cls_score，$(d_x(A), d_y(A), d_w(A), d_h(A))$ 的变换量 rpn_bbox_pred，img_info(包含 feat_stride = 16)</p><p>img_info: 对于一副任意大小 PxQ 图像，传入 Faster RCNN 前_prob概首先reshape到固定 MxN，im_info=[M, N, scale_factor] 保存了此次缩放的所有信息。然后经过 VGG16，经过4次 max_pooling2d 变为 WxH=(M/16)x(N/16) 大小，其中 feature_stride=16 则保存了该信息，用于计算 anchor 偏移量</p><p>Proposal Layer forward（前传函数）按照以下顺序依次处理:</p><ol><li>生成anchors，利用 $ (d_x(A), d_y(A), d_w(A), d_h(A)) $ 对所有的 anchors 做 bbox regression 回归（这里的 anchors 生成和训练时相同）</li><li>按照输入的 positive softmax scores 由大到小排序 anchors，提取前 pre_nms_top N(e.g. 5000) 个anchors，即提取修正位置后的 positive anchors</li><li>限定超出图像边界的 positive anchors 为图像边界，防止后续 ROIpooling 时 proposal 超出图像边界</li><li>剔除尺寸非常小的 positive anchors</li><li>对剩余的 positive anchors 进行NMS(极大值抑制)</li><li>Proposal Layer 有3个输入: positive 和 negative anchors 分类器结果 rpn_cls_score，对应的 bbox reg 的 (e.g. 300) 结果作为 proposal 输出</li></ol><p>输出 proposal 为 <code>[x_min, y_min, x_max, y_max]</code>，由于需要将 anchors 映射回原图判断是否超出边界，所以 proposal 对应的图像尺度为 MxN</p><h3 id="ROIHead"><a href="#ROIHead" class="headerlink" title="ROIHead"></a>ROIHead</h3><p>在传统的CNN网络中，当训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的 vector or matrix，如果输入图像大小不定，过去有2种解决办法:</p><ul><li>从图像中 crop 一部分传入网络</li><li>将图像warp成需要的大小后传入网络</li></ul><p><img src="/image/Faster-RCNN原理笔记/crop_warp.webp" alt="crop_warp" style="zoom:67%"></p><p>crop后破坏了图像的完整结构，warp后破坏了图像原始形状信息，两种方法都不好</p><p>为了使网络可以接收不同大小的图像，Faster RCNN 中提出了 ROIPooling，ROIPooling 从 [Spatial Pyramid Pooling]( 发展而来，这里不展开讨论</p><h4 id="ROI-pooling"><a href="#ROI-pooling" class="headerlink" title="ROI pooling"></a>ROI pooling</h4><p>ROIpooling 对 proposal 对 feature map 裁剪后的 ROIs 进行 maxpooling 使输入的 shape 相同，生成 proposal feature maps，它有3个参数:</p><ul><li>pooled_w: proposal feature maps 的 width</li><li>pooled_h: proposal feature maps 的 width</li><li>spatial_scale: 是 VGG16 提取 feature map 后对图像尺度的改变，也就是 feature_stride=16</li></ul><p>由于 proposal 是对应 MxN 尺度的，所以首先使用 spatial_scale 将其映射回 (M/16)x(N/16) 大小的 feature map 尺度，再将每个 proposal 对应的 feature map 区域水平分为 pooled_w x pooled_h 的网格，对网格的每一份都进行max pooling处理</p><p>例:</p><p>假定输入 feature map 为</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample1.webp" alt="pool_sample1" style="zoom:35%"></p><p>假定区域建议为</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample2.webp" alt="pool_sample2" style="zoom:35%"></p><p>假定 pooled_w=2, pooled_h=2</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample3.webp" alt="pool_sample3" style="zoom:35%"></p><p>对网格的每一份都进行 max pooling 处理</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample4.webp" alt="pool_sample4" style="zoom:40%"></p><p>这种方法显著加快了训练和测试时间</p><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><p><img src="/image/Faster-RCNN原理笔记/classfication.webp" alt="classfication" style="zoom:67%"></p><p>利用 ROIpooling 输出的 proposal feature maps，通过 1x1的conv2d 层与 softmax 计算每个 proposal 具体属于那个类别，输出 cls_prob 概率向量，同时再次利用 bounding box regression 获得每个 proposal 的位置偏移量 bbox_pred，用于回归更加精确的目标检测框</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>论文源码中训练 Faster RCNN 有两种方式，一种是四步交替训练法，一种是 end-to-end 训练法，本文只讨论四步交替训练法</p><p>由前面我们可知，Faster RCNN 大概可以分为 RPN 网络和 Fast RCNN 网络部分</p><ol><li><p>训练 RPN，用 feature map 初始化 RPN 网络，并端到端微调，生成 region proposal</p></li><li><p>用 feature map 初始化 Fast RCNN 网络部分，利用第一步的 RPN 生成的 region proposals 作为输入数据，接着训练 Fast RCNN部分，这时两个网络没有共享卷积层</p></li><li><p>用第二步的 Fast RCNN model 初始化 RPN 第二次进行训练，但固定共享的卷积层，并且只微调 RPN 独有的层，现在两个网络共享卷积层</p></li><li><p>由第三步的 RPN model 初始化 Fast RCNN 网络部分，输入数据为第三步生成的 proposals，保持共享的卷积层固定，微调 Fast RCNN 网络部分 Classification 中的卷积层，两个网络共享相同的卷积层，构成一个统一的网络，也就是论文中的 unified network</p></li></ol></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag">目标检测</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/p/ddce6477/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">制作目标检测数据集常用python脚本整理</div></a><a href="/p/ec86e25e/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">卷积运算的输入输出shape</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li><a target="_blank" href=https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024 style="display:inline-block;text-decoration:none;"> <img src="/images/备案图标.webp" style="float:left;width:14px;height:14px;margin-top:2px;margin-right:-3px"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备32090202001024号</p><a target="_blank" href="https://beian.miit.gov.cn" style="display:inline-block;text-decoration:none"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">&nbsp苏ICP备2023045098号</p></a></li><li>Aeeeeeep Blog &copy; 2024</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>theme <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes...</span><script>var now=new Date,grt=new Date("11/11/2021 00:08:39");function daysInYear(e){return e%4==0&&e%100!=0||e%400==0?366:365}function createTime(){now.setTime(now.getTime()+250);for(var e=0,t=new Date(grt.getTime());t<=now;){var n=daysInYear(t.getFullYear());t.setDate(t.getDate()+n),t<=now&&e++}t.setDate(t.getDate()-daysInYear(t.getFullYear()));var a=Math.floor((now-t)/864e5),r=now.getHours(),o=now.getMinutes(),i=r<10?"0"+r:r,g=o<10?"0"+o:o;document.getElementById("timeDate").innerHTML="Site has been running for "+e+"y "+a+"d ",document.getElementById("times").innerHTML=i+"h "+g+"m "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop" style="font-size:24px"><i class="fe fe-drop-up"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>