<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CUDA编程: CUDA流,并发与上下文</title>
      <link href="/2024/02/06/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%B5%81-%E5%B9%B6%E5%8F%91%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87/"/>
      <url>/2024/02/06/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%B5%81-%E5%B9%B6%E5%8F%91%E4%B8%8E%E4%B8%8A%E4%B8%8B%E6%96%87/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>深入介绍流的使用，包括流的并发，多线程调度流，并发限制，创建流间依赖关系，重叠主机与设备的执行等，同时，结合实例讲解了 CUDA 上下文以及 MPS 的概念，简单介绍了 CUDA Driver API 对 Context 的管理。</p><span id="more"></span><h2 id="并发流的执行"><a href="#并发流的执行" class="headerlink" title="并发流的执行"></a>并发流的执行</h2><h3 id="非空流中的并发"><a href="#非空流中的并发" class="headerlink" title="非空流中的并发"></a>非空流中的并发</h3><p>以下面的核函数为例，定义一个非空流，将多个核函数加入到该流中。再循环定义多个流</p><blockquote><p>注意核函数要足够复杂才能让非空流并行</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_1</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">double</span> val = <span class="number">0.1</span> + tid * <span class="number">0.001</span>;</span><br><span class="line">    <span class="type">double</span> res = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++) &#123;</span><br><span class="line">        res += <span class="built_in">tan</span>(val) * <span class="built_in">tan</span>(val);</span><br><span class="line">    &#125;</span><br><span class="line">    sum += res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_2</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">double</span> val = <span class="number">0.1</span> + tid * <span class="number">0.001</span>;</span><br><span class="line">    <span class="type">double</span> res = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++) &#123;</span><br><span class="line">        res += <span class="built_in">tan</span>(val) * <span class="built_in">tan</span>(val);</span><br><span class="line">    &#125;</span><br><span class="line">    sum += res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_3</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">double</span> val = <span class="number">0.1</span> + tid * <span class="number">0.001</span>;</span><br><span class="line">    <span class="type">double</span> res = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++) &#123;</span><br><span class="line">        res += <span class="built_in">tan</span>(val) * <span class="built_in">tan</span>(val);</span><br><span class="line">    &#125;</span><br><span class="line">    sum += res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_4</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">double</span> sum=<span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">double</span> val = <span class="number">0.1</span> + tid * <span class="number">0.001</span>;</span><br><span class="line">    <span class="type">double</span> res = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000000</span>; i++) &#123;</span><br><span class="line">        res += <span class="built_in">tan</span>(val) * <span class="built_in">tan</span>(val);</span><br><span class="line">    &#125;</span><br><span class="line">    sum += res;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> n_stream=<span class="number">12</span>;</span><br><span class="line">    cudaStream_t *stream=(cudaStream_t*)<span class="built_in">malloc</span>(n_stream*<span class="built_in">sizeof</span>(cudaStream_t));</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line">    cudaEvent_t start,stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start,<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;n_stream; i++) &#123;</span><br><span class="line">        kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">        kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamSynchronize</span>(stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop,<span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsed_time;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsed_time,start,stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;elapsed time:%f ms\n&quot;</span>,elapsed_time);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamDestroy</span>(stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    <span class="built_in">free</span>(stream);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这些内核启动的执行配置被指定为单一线程块中的单一线程，以保证有足够的 GPU 资源能并发运行所有的内核。因为每个内核启动相对主机来说都是异步的，所以可以通过使用单一主机线程同时调度多个内核到不同的流中</p><p>下图可以看到 12 个流以最大 8 个工作队列并行运行</p><p><img src="/image/CUDA编程-CUDA流-并发与上下文/1.png" alt="1"></p><h3 id="使用-OpenMP-调度流"><a href="#使用-OpenMP-调度流" class="headerlink" title="使用 OpenMP 调度流"></a>使用 OpenMP 调度流</h3><p>前面我们都是使用的单一主机线程调度流，我们可以在 CPU 上利用并行编程，使多个 CPU 线程管理多个流。这里，我们介绍一下，在 MPI (Message Passing Interface)、OpenMP 和 Pthread 这三种常见的 CPU 并行编程库：</p><ul><li>MPI：MPI 是一种消息传递库，通常用于分布式内存环境中。MPI 库提供了一组函数，可以在多个计算节点之间发送和接收消息。通过使用 MPI，程序可以在多个计算节点上同时运行，从而实现并行计算</li><li>OpenMP：OpenMP 是一种共享内存并行编程库，它可以用于在单个计算节点的多个 CPU 核之间并行执行代码。OpenMP 提供了一组指令，可以将并行计算任务分配到不同的线程上执行。这些线程共享进程的内存空间，可以在程序的不同部分之间共享数据</li><li>Pthread：Pthread 是 POSIX 线程库的简称，也是一种共享内存并行编程库，与 OpenMP 类似，可以在单个计算节点上的多个 CPU 核之间并行执行代码。Pthread 提供了一组函数，用于创建和管理线程。这些线程共享进程的内存空间，可以在程序的不同部分之间共享数据</li></ul><p>其中 OpenMP 的编译需要添加编译器预处理指令<code>#pragma</code>，创建线程等后续工作要编译器来完成。而 Pthread 所有的并行线程创建都需要我们自己完成，较 OpenMP 麻烦一点，但是更为灵活</p><p>所以我们下面学习使用 OpenMP 库同时调用多个线程，使用一个线程来管理每个流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;omp.h&gt;</span></span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">omp_set_num_thread</span>(n_stream);</span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> omp parallel</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> i=<span class="built_in">omp_get_thread_num</span>();</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>调用 OpenMP 的 API 创建 n_stream 个线程，<code>#pragma omp parallel</code>宏指令告诉编译器下面花括号中的部分就是每个线程都要执行的部分，括号中的部分可以称为并行单元</p><p>使用下面的命令使 nvcc 支持 OpenMP 指令编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc &#123;&#125;.cu -Xcompiler -fopenmp</span><br></pre></td></tr></table></figure><p><img src="/image/CUDA编程-CUDA流-并发与上下文/2.png" alt="2"></p><p>关于 OpenMP 与 CUDA 之间更复杂的操作我们会在之后的文章和大家细细道来</p><h3 id="使用环境变量调整流行为"><a href="#使用环境变量调整流行为" class="headerlink" title="使用环境变量调整流行为"></a>使用环境变量调整流行为</h3><p>目前 Nvidia 支持的最大 Hyper-Q 工作队列数是 32，但是在默认情况下并不是全部开启，而是被限制成 8 个，原因是每个工作队列只要开启就会有资源消耗，如果用不到 32 个可以把资源留给需要的 8 个队列，修改这个配置的方法是修改主机系统的环境变量</p><p>对于Linux系统中，可以导入环境变量修改</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_DEVICE_MAX_CONNECTIONS=32</span><br></pre></td></tr></table></figure><p>将 <code>n_stream</code> 改为 24，再次编译，下图可以看到并行工作队列数提高很多</p><p><img src="/image/CUDA编程-CUDA流-并发与上下文/3.png" alt="3"></p><h3 id="并发限制"><a href="#并发限制" class="headerlink" title="并发限制"></a>并发限制</h3><p>有限的内核资源可以抑制应用程序中可能出现的内核并发的数量。在实际应用中，内核启动时通常会创建大量线程，这时，可用的硬件资源可能会成为并发的主要限制因素，因为它们阻止启动符合条件的内核。下面更改</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>为</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">16</span>,<span class="number">32</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br></pre></td></tr></table></figure><p>将使用的 CUDA 流增加到 24，下图可以看到只实现了 4 路并发，因为 GPU 无法分配足够的资源，这里需要我们根据需求修改各个参数</p><h3 id="空流的阻塞行为"><a href="#空流的阻塞行为" class="headerlink" title="空流的阻塞行为"></a>空流的阻塞行为</h3><p>为了演示在空流中的的阻塞行为，将 <code>n_stream</code> 改回 12，<code>block</code>和<code>grid</code>改回1，我们将深度优先调度循环改为在空流的调用 kernel_3</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_3&lt;&lt;&lt;grid,block&gt;&gt;&gt;();</span><br><span class="line">    kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 可以看到所有 kernel_3 启动以后，所有其他的流中的操作全部被阻塞，空流对于非空流具有阻塞作用</p><p><img src="/image/CUDA编程-CUDA流-并发与上下文/5.png" alt="5"></p><h3 id="创建流间依赖关系"><a href="#创建流间依赖关系" class="headerlink" title="创建流间依赖关系"></a>创建流间依赖关系</h3><p>理想情况下，流之间不应存在非预期的依赖关系（即虚假的依赖关系）。然而在实际使用时，我们需要一个流等待另一个流中的操作完成。可以使用事件在流之间创建依赖关系。首先，使用标志<code>cudaEventDisableTiming</code>创建同步事件</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t * event=(cudaEvent_t *)<span class="built_in">malloc</span>(n_stream*<span class="built_in">sizeof</span>(cudaEvent_t));</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">    <span class="built_in">cudaEventCreateWithFlags</span>(&amp;event[i],cudaEventDisableTiming);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，使用<code>cudaEventRecord</code>在每个流完成时记录一个不同的事件，再使用<code>cudaStreamWaitEvent</code>来强制最后一个流（即流[n_streams-1]）等待其他所有流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;n_stream;i++) &#123;</span><br><span class="line">    kernel_1&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_2&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_3&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    kernel_4&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(event[i],stream[i]);</span><br><span class="line">    <span class="built_in">cudaStreamWaitEvent</span>(stream[n_stream<span class="number">-1</span>],event[i],<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从下图的时间轴可以看到我们成功创建了流间的依赖关系，最后一个流会等到前面所有流中的事件完成，再运行</p><p><img src="/image/CUDA编程-CUDA流-并发与上下文/6.png" alt=""></p><h2 id="重叠内核执行和数据传输"><a href="#重叠内核执行和数据传输" class="headerlink" title="重叠内核执行和数据传输"></a>重叠内核执行和数据传输</h2><p>前面的章节我们已经了解了数据传输队列（HtD, DtH），不是经过同一条队列的，这两个操作可以重叠完成，但是同向数据传输的时候不能进行此操作。此外，还需要检查数据传输和内核执行之间的关系：</p><ul><li>如果内核使用数据 A，对 A 进行数据传输必须要在内核启动之前，且必须在同一个流中</li><li>如果内核不使用数据A，内核执行和数据传输可以位于不同的流中重叠执行</li></ul><p>第二种情况就是重叠内核执行和数据传输的基本做法，当数据传输和内核执行被分配到不同的流中时，CUDA 执行的时候默认这是安全的，我们要保证它们之间的依赖关系。但是第一种情况也可以进行重叠，需要对核函数进行一定的分割，我们用向量加法核函数来举例</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraysSum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*res, <span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; N) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;N_REPEAT;j++)</span><br><span class="line">            res[idx]=a[idx]+b[idx];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>向量加法的过程是为</p><ol><li>将两个输入向量从主机传入设备</li><li>内核计算结果</li><li>将结果从设备回传到主机</li></ol><p>由于这个问题就是一个一步问题，我们没办法让内核和数据传输重叠，因为内核需要全部的数据，但是由于向量加法的每一位都互不干扰，我们可以把向量分块，并且每块中的数据只用于每块的内核，而跟其它分块的内核没有关系，这样就可以把整个过程分成 N_SEGMENT 份，也就是 N_SEGMENT 个流分别执行</p><h3 id="深度优先调度重叠"><a href="#深度优先调度重叠" class="headerlink" title="深度优先调度重叠"></a>深度优先调度重叠</h3><p>我们首先使用深度优先调度的方式。这里需要注意数据传输是异步的，所以必须声明为固定内存，不能是分页内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N_REPEAT 10</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> N_SEGMENT 4</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraysSum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res,<span class="type">int</span> N)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="keyword">if</span>(idx &lt; N) &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">0</span>;j&lt;N_REPEAT;j++)</span><br><span class="line">            res[idx]=a[idx]+b[idx];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Vector size:%d\n&quot;</span>,nElem);</span><br><span class="line">    <span class="type">int</span> nByte=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line">    <span class="type">float</span> * a_h,*b_h,*res_h,*res_from_gpu_h;</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;a_h,nByte,cudaHostAllocDefault);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;b_h,nByte,cudaHostAllocDefault);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;res_h,nByte,cudaHostAllocDefault);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;res_from_gpu_h,nByte,cudaHostAllocDefault);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemset</span>(res_h,<span class="number">0</span>,nByte);</span><br><span class="line">    <span class="built_in">cudaMemset</span>(res_from_gpu_h,<span class="number">0</span>,nByte);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *a_d,*b_d,*res_d;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;a_d,nByte);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;b_d,nByte);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;res_d,nByte);</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">512</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((nElem<span class="number">-1</span>)/block.x+<span class="number">1</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//asynchronous calculation</span></span><br><span class="line">    <span class="type">int</span> iElem=nElem/N_SEGMENT;</span><br><span class="line">    cudaStream_t stream[N_SEGMENT];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    cudaEvent_t start,stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start,<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++) &#123;</span><br><span class="line">        <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]);</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]);</span><br><span class="line">        ArraysSum&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++) &#123;</span><br><span class="line">        <span class="built_in">cudaStreamDestroy</span>(stream[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaFree</span>(a_d);</span><br><span class="line">    <span class="built_in">cudaFree</span>(b_d);</span><br><span class="line">    <span class="built_in">cudaFree</span>(a_h);</span><br><span class="line">    <span class="built_in">cudaFree</span>(b_h);</span><br><span class="line">    <span class="built_in">cudaFree</span>(res_h);</span><br><span class="line">    <span class="built_in">cudaFree</span>(res_from_gpu_h);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>nvvp 可视化如下</p><p><img src="/image/CUDA编程-CUDA流-并发与上下文/7.png" alt=""></p><h3 id="广度优先调度重叠"><a href="#广度优先调度重叠" class="headerlink" title="广度优先调度重叠"></a>广度优先调度重叠</h3><p>循环修改为如下代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    ArraysSum&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">    <span class="built_in">CHECK</span>(<span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/image/CUDA编程-CUDA流-并发与上下文/8.png" alt="8"></p><p>nvvp 可视化和深度优先调度重叠一模一样，所以我们不需要关注深度还是广度的调度顺序</p><h2 id="重叠主机与设备的执行"><a href="#重叠主机与设备的执行" class="headerlink" title="重叠主机与设备的执行"></a>重叠主机与设备的执行</h2><p>实现 GPU 和 CPU 的执行重合是相对直接的，因为所有的内核默认情况下是异步启动的。因此，只要启动一个内核，并立即在主机线程中实现有效操作，就会自动产生 GPU 和 CPU 执行的重叠</p><p>以下面的加法核函数为例</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">float</span> *g_data, <span class="type">float</span> value)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    g_data[idx] = g_data[idx] + value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在这个例子中，有两个拷贝和一个内核启动操作，记录了一个停止事件，以标记所有 CUDA 操作的完成</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemcpyAsync</span>(d_a, h_a, nbytes, cudaMemcpyHostToDevice);</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(d_a, value);</span><br><span class="line"><span class="built_in">cudaMemcpyAsync</span>(h_a, d_a, nbytes, cudaMemcpyDeviceToHost);</span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop);</span><br></pre></td></tr></table></figure><p>这些操作与主机都是异步的，它们都被绑定到默认流中，一旦最后一个<code>cudaMemcpyAsync</code>被发出，控制权将立即返回到主机。一旦控制权返回给主机，主机就可以做任何不依赖内核输出的有用的计算。在下面的代码段中，主机只是简单地进行迭代，等待所有CUDA操作的完成，同时递增一个计数器。在每次迭代中，主机线程查询停止事件。一旦该事件满足，主机线程就会继续</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> counter = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">cudaEventQuery</span>(stop) == cudaErrorNotReady) &#123;</span><br><span class="line">counter++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;Counter: %ld\n&quot;</span>,counter);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 输出</span></span><br><span class="line"><span class="comment">Vector size:1048576</span></span><br><span class="line"><span class="comment">Counter: 7206</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h3 id="流回调"><a href="#流回调" class="headerlink" title="流回调"></a>流回调</h3><p>流回调是另一种可以到 CUDA 流排列等待的操作类型。 一旦流回调之前的流中的所有操作都已完成，CUDA 运行时将调用流回调指定的主机端函数，该函数由应用程序提供，这允许将任意主机端逻辑插入到 CUDA 流中。 流回调是另一种 CPU 到 GPU 同步机制，但是流回调时，回调函数中不可以调用 CUDA 的 API，且不可以执行同步</p><p>流函数有特殊的参数规格，必须写成下面形式参数的函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> CUDART_CB <span class="title">my_callback</span><span class="params">(cudaStream_t stream, cudaError_t status, <span class="type">void</span> *data)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;callback from stream %d\n&quot;</span>, *((<span class="type">int</span> *)data));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数有三个参数：</p><ul><li><code>cudaStream_t stream</code>：表示回调函数与哪个CUDA流相关联。当流中的所有操作都完成时，CUDA运行时将调用此回调函数</li><li><code>cudaError_t status</code>：表示流中最后一个操作的状态。如果状态是<code>cudaSuccess</code>，则表示所有操作已成功完成</li><li><code>void *data</code>：表示传递给回调函数的数据指针。在此示例中，该指针指向一个整数，其中包含与流相关的自定义数据</li></ul><p>并使用下面的函数加入流中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamAddCallback</span><span class="params">(cudaStream_t stream,cudaStreamCallback_t callback, <span class="type">void</span> *userData, <span class="type">unsigned</span> <span class="type">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>stream</code>：CUDA 流，表示将要添加回调函数的流</li><li><code>callback</code>：回调函数，该函数会在指定的流上的所有操作都已经完成时被调用。回调函数的原型为 <code>void (*)(cudaStream_t, cudaError_t, void*)</code>，其中第一个参数表示回调函数所在的流，第二个参数表示流上的最后一个 CUDA 操作的状态，第三个参数为用户自定义数据</li><li><code>userData</code>：用户自定义数据指针，会在回调函数被调用时传递给回调函数</li><li><code>flags</code>：标志位，用于控制回调函数的行为。目前只支持 <code>cudaStreamCallbackBlocking</code> 和 <code>cudaStreamCallbackNonblocking</code> 两种标志位，分别表示回调函数是阻塞还是非阻塞的。如果使用阻塞回调函数，则该回调函数必须在流上的所有操作完成后才能被调用。如果使用非阻塞回调函数，则该回调函数可能会在流上的操作尚未全部完成时被调用</li></ul><p>下面是流回调的一个例子</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="comment">// 定义一个回调函数 my_callback()，监控每个流的完成情况，并在每个流完成后输出相应流的 ID 号，用于在 CUDA 异步操作完成后执行</span></span><br><span class="line"><span class="function"><span class="type">void</span> CUDART_CB <span class="title">my_callback</span><span class="params">(cudaStream_t stream,cudaError_t status,<span class="type">void</span> * data)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;call back from stream:%d\n&quot;</span>,*((<span class="type">int</span> *)data));</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;N_SEGMENT;i++) &#123;</span><br><span class="line">        <span class="type">int</span> ioffset=i*iElem;</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;a_d[ioffset],&amp;a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]);</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;b_d[ioffset],&amp;b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]);</span><br><span class="line">        ArraysSum&lt;&lt;&lt;grid,block,<span class="number">0</span>,stream[i]&gt;&gt;&gt;(&amp;a_d[ioffset],&amp;b_d[ioffset],&amp;res_d[ioffset],iElem);</span><br><span class="line">        <span class="built_in">cudaMemcpyAsync</span>(&amp;res_from_gpu_h[ioffset],&amp;res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]);</span><br><span class="line">        <span class="comment">// 使用 cudaStreamAddCallback() 将回调函数 my_callback() 添加到每个流中，以便跟踪每个流的完成情况</span></span><br><span class="line">        <span class="built_in">cudaStreamAddCallback</span>(stream[i],my_callback,(<span class="type">void</span> *)(stream+i),<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">/* 输出</span></span><br><span class="line"><span class="comment">Vector size:1048576</span></span><br><span class="line"><span class="comment">call back from stream:1947823616</span></span><br><span class="line"><span class="comment">call back from stream:1946969680</span></span><br><span class="line"><span class="comment">call back from stream:1947835744</span></span><br><span class="line"><span class="comment">call back from stream:1947835776</span></span><br><span class="line"><span class="comment">Counter: 5126</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="CUDA-Context-上下文"><a href="#CUDA-Context-上下文" class="headerlink" title="CUDA Context 上下文"></a>CUDA Context 上下文</h2><p>CUDA Context 是一个由特定进程与设备相关联的状态集合，包括：</p><ul><li><p>所有分配内存</p></li><li><p>Modules，类似于动态链接库，以.cubin和.ptx结尾 【在jcuda中要使用】</p></li><li>CUDA streams，管理执行单元的并发性</li><li>CUDA events</li><li>texture和surface引用</li><li>kernel里面使用到的本地内存（设备内存）</li><li>用于调试、分析和同步的内部资源</li><li>用于分页复制的固定缓冲区</li></ul><p>CUDA 程序通过使用 CUDA Context 来管理设备资源和执行 CUDA 指令。每个进程可以有多个 CUDA Context，每个 CUDA Context 只能与一个设备相关联。CUDA 程序通过使用 CUDA Context 来管理设备资源和执行 CUDA 指令</p><p>每个进程或 GPU 可以有多个 CUDA Context，而每个 CUDA Context 只能与一个 GPU 相关联</p><p>在 CUDA 中，每个任务都有一个独立的设备 ID，每个设备 ID 对应一个唯一的 CUDA Context。所以 Context 类似于 CPU 上的进程，由 Driver 层管理分配资源的生命周期</p><p>与 CPU 进程的管理类似，每个 Context 有自己的地址空间，且之间是隔离的，在一个 Context 中所有指针只能在这一个 Context 中使用，但一个 CUDA Context 中的任何一个 kernel 被挂掉后，则此时处于同一个 GPU 上的 所有 Context 的所有都会失效</p><h3 id="隐式创建"><a href="#隐式创建" class="headerlink" title="隐式创建"></a>隐式创建</h3><p>CUDA Runtime 软件层的库是隐式创建 context，且不提供 API 直接创建 CUDA context，而是通过延迟初始化（deferred initialization）来创建 context，也就是 lazy initialization</p><p>在 Linux 中通过导入环境变量延迟初始化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export CUDA_MODULE_LOADING=LAZY</span><br></pre></td></tr></table></figure><blockquote><p>CUDA_MODULE_LOADING 默认为 <code>EAGER</code>，会最大限度地减少模块加载时的延迟，但会增加程序启动时间和内存占用</p></blockquote><p>具体意思是在调用每一个 CUDART 库函数时，它会检查当前是否有 context 存在，假如需要 context，那么才自动创建。也就是说需要创建上面这些对象的时候就会创建context。可以显式的控制初始化，即调用 cudaFree(0)，强制的初始化</p><p>CUDA Runtime 将 context 和 device 的概念合并了，即在一个 GPU 上操作可看成在一个 context 下</p><h3 id="显示创建"><a href="#显示创建" class="headerlink" title="显示创建"></a>显示创建</h3><p>可以使用 CUDA Driver API 显示创建 context，CUDA Driver API 是一种更偏向底层的 API，提供了对硬件的更细粒度的控制，直接控制 GPU 的所有硬件资源。这些函数被实现在 <a href="https://docs.nvidia.com/cuda/cuda-driver-api/index.html">CUDA Driver 库 </a>中，需要手动链接这个库并直接调用这些函数，下面几个函数用于管理 CUDA 上下文</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CUresult <span class="title">cuCreateContext</span><span class="params">(CUcontext* pctx, <span class="type">unsigned</span> <span class="type">int</span> flags, CUdevice dev)</span></span>;</span><br></pre></td></tr></table></figure><p>创建 CUDA 上下文</p><ul><li><p>pctx：输出参数，指向新创建的CUDA上下文句柄</p></li><li><p>flags：用于设置上下文属性的标志位，可以为 0</p></li><li><p>dev：用于创建上下文的设备句柄</p></li><li><p>返回 CUDA_SUCCESS 表示函数调用成功，否则返回错误码</p></li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CUresult <span class="title">cuPushCurrent</span><span class="params">(CUcontext ctx)</span></span>;</span><br></pre></td></tr></table></figure><p>将当前线程的CUDA上下文压入上下文栈中，并将给定上下文设置为当前上下文</p><ul><li>ctx：要设置为当前上下文的CUDA上下文句柄</li><li>返回 CUDA_SUCCESS 表示函数调用成功，否则返回错误码</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">CUresult <span class="title">cuPopCurrent</span><span class="params">(CUcontext *pctx)</span></span>;</span><br></pre></td></tr></table></figure><p>将当前线程的CUDA上下文从上下文栈中弹出，并将上下文栈顶的上下文设置为当前上下文</p><ul><li>pctx：输出参数，指向弹出的CUDA上下文句柄</li><li>返回 CUDA_SUCCESS 表示函数调用成功，否则返回错误码</li></ul><p>其中，隐式调用的context是 primary context，由 CUDA 驱动程序自动创建和管理； 显示调用的 context 是standard context，需要手动管理其生命周期和状态，并且可以同时存在多个 standard context。每次 CUDA 初始化比较费时间，可能是 Runtime 进行了隐式调用 context，可以使用 <code>cudaError_t cudaSetDevice(int device)</code>  提前创建 context 节省这部分时间</p><p>使用 CUDA Driver API 编写的 CUDA 程序通常具有更高的性能，因为它们可以更充分地利用 GPU 的硬件资源。但是，由于这种API需要我们对硬件有更深入的了解，并且需要编写更多的底层代码，所以这种编程方式会更加困难和容易出错，目前阶段我们暂不深入了解这个库，下面只给出简单示例</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// This code is modified from https://blog.csdn.net/weicao1990/article/details/123959648</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span><span class="comment">// 包含 cuda driver api</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span> </span></span><br><span class="line"> </span><br><span class="line"><span class="meta">#<span class="keyword">define</span> checkDriver(op)  __check_cuda_driver((op), #op, __FILE__, __LINE__)</span></span><br><span class="line"> </span><br><span class="line"><span class="type">bool</span> __check_cuda_driver(CUresult code, <span class="type">const</span> <span class="type">char</span>* op, <span class="type">const</span> <span class="type">char</span>* file, <span class="type">int</span> line)&#123;</span><br><span class="line">    <span class="keyword">if</span>(code != CUresult::CUDA_SUCCESS)&#123;    <span class="comment">// 如果 成功获取CUDA情况下的返回值 与我们给定的值(0)不相等， 即条件成立， 返回值为flase</span></span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* err_name = <span class="literal">nullptr</span>;    <span class="comment">// 定义了一个字符串常量的空指针</span></span><br><span class="line">        <span class="type">const</span> <span class="type">char</span>* err_message = <span class="literal">nullptr</span>;  </span><br><span class="line">        <span class="built_in">cuGetErrorName</span>(code, &amp;err_name);    </span><br><span class="line">        <span class="built_in">cuGetErrorString</span>(code, &amp;err_message);   </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%s:%d  %s failed. \n  code = %s, message = %s\n&quot;</span>, file, line, op, err_name, err_message); <span class="comment">//打印错误信息</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// 检查cuda driver的初始化</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuInit</span>(<span class="number">0</span>));</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 为设备创建上下文</span></span><br><span class="line">    CUcontext ctxA = <span class="literal">nullptr</span>;                                   <span class="comment">// CUcontext 其实是 struct CUctx_st*（是一个指向结构体CUctx_st的指针）</span></span><br><span class="line">    CUcontext ctxB = <span class="literal">nullptr</span>;</span><br><span class="line">    CUdevice device = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxCreate</span>(&amp;ctxA, CU_CTX_SCHED_AUTO, device)); <span class="comment">// 这一步相当于告知要某一块设备上的某块地方创建 ctxA 管理数据。输入参数 参考 https://www.cs.cmu.edu/afs/cs/academic/class/15668-s11/www/cuda-doc/html/group__CUDA__CTX_g65dc0012348bc84810e2103a40d8e2cf.html</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxCreate</span>(&amp;ctxB, CU_CTX_SCHED_AUTO, device)); </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;ctxA = %p\n&quot;</span>, ctxA);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;ctxB = %p\n&quot;</span>, ctxB);</span><br><span class="line">    <span class="comment">/* </span></span><br><span class="line"><span class="comment">        contexts 栈：</span></span><br><span class="line"><span class="comment">            ctxB -- top &lt;--- current_context</span></span><br><span class="line"><span class="comment">            ctxA </span></span><br><span class="line"><span class="comment">            ...</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">// 获取当前上下文信息</span></span><br><span class="line">    CUcontext current_context = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxGetCurrent</span>(&amp;current_context));             <span class="comment">// 这个时候current_context 就是上面创建的context</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;current_context = %p\n&quot;</span>, current_context);</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 可以使用上下文堆栈对设备管理多个上下文</span></span><br><span class="line">    <span class="comment">// 压入当前context</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxPushCurrent</span>(ctxA));                        <span class="comment">// 将这个 ctxA 压入CPU调用的thread上。专门用一个thread以栈的方式来管理多个contexts的切换</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxGetCurrent</span>(&amp;current_context));             <span class="comment">// 获取current_context (即栈顶的context)</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;after pushing, current_context = %p\n&quot;</span>, current_context);</span><br><span class="line">    <span class="comment">/* </span></span><br><span class="line"><span class="comment">        contexts 栈：</span></span><br><span class="line"><span class="comment">            ctxA -- top &lt;--- current_context</span></span><br><span class="line"><span class="comment">            ctxB</span></span><br><span class="line"><span class="comment">            ...</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 弹出当前context</span></span><br><span class="line">    CUcontext popped_ctx = <span class="literal">nullptr</span>;</span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxPopCurrent</span>(&amp;popped_ctx));                   <span class="comment">// 将当前的context pop掉，并用popped_ctx承接它pop出来的context</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxGetCurrent</span>(&amp;current_context));              <span class="comment">// 获取current_context(栈顶的)</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;after poping, popped_ctx = %p\n&quot;</span>, popped_ctx);       <span class="comment">// 弹出的是ctxA</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;after poping, current_context = %p\n&quot;</span>, current_context); <span class="comment">// current_context是ctxB</span></span><br><span class="line"> </span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxDestroy</span>(ctxA));</span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuCtxDestroy</span>(ctxB));</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 更推荐使用cuDevicePrimaryCtxRetain获取与设备关联的context</span></span><br><span class="line">    <span class="comment">// 注意这个重点，以后的runtime也是基于此, 自动为设备只关联一个context</span></span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuDevicePrimaryCtxRetain</span>(&amp;ctxA, device));       <span class="comment">// 在 device 上指定一个新地址对ctxA进行管理</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;ctxA = %p\n&quot;</span>, ctxA);</span><br><span class="line">    <span class="built_in">checkDriver</span>(<span class="built_in">cuDevicePrimaryCtxRelease</span>(device));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 输出</span></span><br><span class="line"><span class="comment">ctxA = 0x560a174cce50</span></span><br><span class="line"><span class="comment">ctxB = 0x560a179c5810</span></span><br><span class="line"><span class="comment">current_context = 0x560a179c5810</span></span><br><span class="line"><span class="comment">after pushing, current_context = 0x560a174cce50</span></span><br><span class="line"><span class="comment">after poping, popped_ctx = 0x560a174cce50</span></span><br><span class="line"><span class="comment">after poping, current_context = 0x560a179c5810</span></span><br><span class="line"><span class="comment">ctxA = 0x560a174edbd0</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><p>编译</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc &#123;&#125;.cu -lcuda</span><br></pre></td></tr></table></figure><h2 id="MPS-多进程服务"><a href="#MPS-多进程服务" class="headerlink" title="MPS 多进程服务"></a>MPS 多进程服务</h2><p>CUDA MPS（Multi-Process Service）是一种允许多个进程共享单个 GPU 的技术。它允许在同一时间多个进程使用相同的GPU，从而提高GPU的利用率。通过在 GPU 上创建多个 CUDA 上下文来实现的。每个进程都可以创建自己的 CUDA 上下文，并且在这些上下文之间共享 GPU 资源。在 MPS 模式下，多个进程可以并发地使用 GPU，而不会互相干扰</p><p>在使用MPS时，需要在每个进程中创建一个CUDA上下文，并且这些上下文需要使用相同的 GPU 设备</p><p>开启 MPS 服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-cuda-mps-control -d</span><br></pre></td></tr></table></figure><p>关闭 MPS 服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo nvidia-cuda-mps-control quit</span><br></pre></td></tr></table></figure><p>在使用 MPS 时，需要避免使用所有 GPU 资源，因为MPS需要一些 GPU 资源来管理多个CUDA上下文。可以使用 nvidia-smi 工具来检查 MPS 所使用的 GPU 资源</p>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA流,事件与同步</title>
      <link href="/2024/01/30/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%B5%81-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E5%90%8C%E6%AD%A5/"/>
      <url>/2024/01/30/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%B5%81-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E5%90%8C%E6%AD%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>详细讲解 CUDA 流，事件的概念以及声明，以此为基础，深入了解 CUDA 流操作之间的依赖关系，流的同步与异步，以及如何优化事件的创建与管理等等。</p><span id="more"></span><h2 id="CUDA-流"><a href="#CUDA-流" class="headerlink" title="CUDA 流"></a>CUDA 流</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>CUDA 流是一系列异步的 CUDA 操作，这些操作按照主机代码所定义的顺序在设备上执行。流会封装这些操作，保持操作的顺序，允许操作在流中排队，并使它们在先前的所有操作之后执行。CUDA 流中的操作可以是主机与设备的内存数据传输，设备内核启动，主机和设备之间的同步等由主机发起但由设备处理的命令</p><p>在 CUDA 编程中，一般的执行模式如下</p><ul><li>将数据从主机传输到设备</li><li>在设备上执行内核</li><li>将结果从设备传输回主机</li></ul><p>因为不同的 CUDA 流中的操作是异步执行的，这使得它们可以并行运行，不会受到其它流中操作的影响，所以可以将内核执行和数据传输调度到不同的流中，完全隐藏CPU和GPU之间的通信延迟，提高程序的效率</p><p>流在 CUDA 的 API 调用粒度上可实现流水线或双缓冲技术。CUDA 的 API函数一般可以分为同步或异步。具有同步行为的函数会阻塞主机端线程，直到它们完成。具有异步行为的函数被调用后，会立即将控制权归还给主机</p><p>异步函数和流是在 CUDA 中构建网格级并发的基础。从软件上看，CUDA 操作在不同的流中并发运行，但从硬件上看，不总是如此。根据 PCIe 总线争用或每个SM资源的可用性，完成不同的 CUDA 流可能仍然需要互相等待。下面将详细了解在有多种计算能力的设备上，流是如何运行的</p><h3 id="声明"><a href="#声明" class="headerlink" title="声明"></a>声明</h3><p>所有的 CUDA 操作都是在流中进行的，流分为</p><ul><li>隐式声明（空流）</li><li>显式声明（非空流）</li></ul><p>如果没有显式地指定一个流，那么内核启动和数据传输将默认使用空流。在本章之前所有例子都是空流</p><p>基于流的异步内核启动和数据传输支持以下类型的粗粒度并发</p><ul><li>主机计算 - 设备计算</li><li>主机计算 - 主机与设备间的数据传输</li><li>设备计算 - 主机与设备间的数据传输</li><li>并发执行多个设备的计算</li></ul><p>我们首先要有一个概念，设备与主机是两个运算节点，以一般的 CUDA 程序举例，下面的 3 个操作会被发布到默认的流中，设备只需要按发布顺序执行，而其他主机上的操作设备一概不知</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="built_in">cudaMemcpy</span>(..., cudaMemcpyHostToDevice);</span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(...);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(..., cudaMemcpyDeviceToHost);</span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure><p>但是主机要等待设备运算，数据向主机传输完成后才能执行后面的操作，也就是之前我们接触到的数据传输都是同步的。不同的是，内核启动是异步的，无论内核是否完成，主机的应用程序都立即恢复执行。</p><p>现在介绍一下异步的数据传输，下面是 <code>cudaMemcpy</code> 函数的异步版本</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaMemcpyAsync</span> <span class="params">( <span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count, cudaMemcpyKind kind, cudaStream_t stream = <span class="number">0</span> )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>dst</code>：目标地址，指向要复制数据的位置</li><li><code>src</code>：源地址，指向要复制的数据的位置</li><li><code>count</code>：要复制的字节数。</li><li><code>kind</code>：指定复制的方向，可选值为<code>cudaMemcpyHostToDevice</code>、<code>cudaMemcpyDeviceToHost</code>、<code>cudaMemcpyDeviceToDevice</code>和<code>cudaMemcpyDefault</code></li><li><code>stream</code>：可选参数，指定将异步操作添加到的流</li></ul><p>其中<code>stream</code>默认被设置为空流。这个函数与主机是异步的，在调用发布后，控制权将立即返回到主机。</p><p>如果我们希望数据传输与非空流关联，可以使用下面的函数显式创建一个非空流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaStreamCreate</span> <span class="params">( cudaStream_t* pStream )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>pStream</code>：指向新流标识符的指针</li></ul><p>返回到<code>pStream</code>中的流就可以被当作参数给其它异步 CUDA 的 API 函数使用。需要注意的是，当执行异步数据传输时，必须使用固定的主机内存，我们可以使用前面章节提到的两个函数分配固定内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMallocHost</span><span class="params">(<span class="type">void</span> **ptr, <span class="type">size_t</span> size)</span></span>;</span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaHostAlloc</span><span class="params">(<span class="type">void</span> **pHost, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span> flags)</span></span>;</span><br></pre></td></tr></table></figure><p>在非空流中启动内核，需要注意提供流标识符作为第四个参数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel_name&lt;&lt;&lt;grid, block, sharedMemSize, stream&gt;&gt;&gt;(argument list);</span><br></pre></td></tr></table></figure><p>一个非空流声明与创建如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cudaStream_t stream;</span><br><span class="line"><span class="built_in">cudaStreamCreate</span>(&amp;stream);</span><br></pre></td></tr></table></figure><p>可以用如下代码释放流中的资源</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamDestroy</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure><p>在一个流中，当 <code>cudaStreamDestroy</code> 函数被调用时，如果该流中仍有未完成的工作，<code>cudaStreamDestroy</code> 函数将立即返回，当流中所有工作都已完成时，与流相关的资源将被自动释放</p><p>因为所有流都是异步的，有两个专用的函数来检查流中的所有操作是否都已经完成</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamSynchronize</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure><p><code>cudaStreamSynchronize</code>函数用于强制阻塞主机，直到给定流中的所有操作都完成</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaStreamQuery</span><span class="params">(cudaStream_t stream)</span></span>;</span><br></pre></td></tr></table></figure><p><code>cudaStreamQuery</code>函数用于检查流中的所有操作是否都已经完成，但不会阻塞主机。当所有操作都完成时函数会返回<code>cudaSuccess</code>。否则返回<code>cudaErrorNotReady</code></p><p>下面这段代码是使用流的一个例子，在多个流中执行 CUDA 核函数和数据传输操作</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * bytesPerStream;</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(&amp;d_a[offset], &amp;a[offset], bytePerStream, streams[i]);</span><br><span class="line">    kernel&lt;&lt;grid, block, <span class="number">0</span>, streams[i]&gt;&gt;(&amp;d_a[offset]);</span><br><span class="line">    <span class="built_in">cudaMemcpyAsync</span>(&amp;a[offset], &amp;d_a[offset], bytesPerStream, streams[i]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; i++) &#123;</span><br><span class="line">    <span class="built_in">cudaStreamSynchronize</span>(streams[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>流在 CUDA 中执行的时间轴如下图所示，H2D 是主机到设备的内存传输，D2H 是设备到主机的内存传输。数据传输和内核执行分布在 3 个并发流中，但是在执行时，数据传输并没有并发执行，这是因为 PCIe 总线是共享的，当第一个流占据了总线，后面的流就要等待总线空闲，但是如果 H2D 和 D2H 同时发生，就不会产生等待，而是同时进行</p><p><img src="/image/CUDA编程-CUDA流-事件与同步/1.png" alt=""></p><h2 id="流调度"><a href="#流调度" class="headerlink" title="流调度"></a>流调度</h2><p>前面我们说到，所有流都可以同时运行，但在硬件中没有流的概念，而是包含一个或多个执行内存拷贝操作的引擎和执行核函数的引擎。这些引擎彼此独立地对操作进行排队，这将导致如下图所示的任务调度情形</p><p><img src="/image/CUDA编程-CUDA流-事件与同步/5.png" alt=""></p><p>在不同流的操作中，存在相互的依赖，比如 memcpy A B C 为从主机拷贝数据到设备内存，kernel 需要等待 memcpy A B C 操作完成后再执行，Stream 1 需要等待 Stream 0 完成操作后再执行，如下图所示</p><p><img src="/image/CUDA编程-CUDA流-事件与同步/6.png" alt=""></p><p>为了避免这个问题，我们可以交错地执行不同流的拷贝内存操作和核函数运算操作，如下图所示</p><p><img src="/image/CUDA编程-CUDA流-事件与同步/7.png" alt=""></p><p>为了解决多个 Kernel 函数同时在 GPU 中运行的问题，节省代码编写成本，从 Kepler 架构开始，Nvidia 推出了 Hyper-Q 硬件技术，主机与设备之间最多可以建立 32 个工作队列，每个流分配一个工作队列，如果创建的流超过32个，则多个流共用一个工作队列</p><p>同时 Hyper-Q 技术还可以使不同流中的计算和使用带宽能够重叠，最大化 GPU 的资源利用率。例如 Stream1 中的计算要占用 60% 的核心和 60% 的显存带宽，而 Stream2 中的计算要占用 70% 的核心和 50% 的显存带宽，二者同时运行时会按一定的比率争用 GPU 资源</p><h2 id="流优先级"><a href="#流优先级" class="headerlink" title="流优先级"></a>流优先级</h2><p>对于计算能力在 3.5 以上的设备可以分配流的优先级，下面函数创建一个有指定优先级的流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaStreamCreateWithPriority</span> <span class="params">( cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span>  flags, <span class="type">int</span>  priority )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>pStream</code>：一个指向 <code>cudaStream_t</code> 类型的指针，用于存储创建的流的句柄</li><li><code>flags</code>：流的行为标志，可选参数，默认为0。当前支持的标志只有 <code>cudaStreamNonBlocking</code>，指定在创建的流中运行的工作可以与 Stream0（空流）中的工作同时运行，并且创建的流不应该与 Stream0 执行隐式同步</li><li><code>priority</code>：流的优先级，较低的数字代表较高的优先级。0 表示默认优先级。 可以使用 <code>cudaDeviceGetStreamPriorityRange</code> 查询有意义的数值优先级范围。 如果指定的优先级超出了 <code>cudaDeviceGetStreamPriorityRange</code> 返回的数值范围，它将自动被限制在范围内的最低或最高数字</li></ul><p>不同的设备有不同的优先级等级，下面函数可以查询当前设备的优先级分布情况</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaDeviceGetStreamPriorityRange</span> <span class="params">( <span class="type">int</span>* leastPriority, <span class="type">int</span>* greatestPriority )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>leastPriority</code>：指向整数的指针，返回设备支持的最低优先级</li><li><code>greatestPriority</code>：指向整数的指针，返回设备支持的最高优先级</li></ul><p>笔主的显卡优先级等级范围为 [0, -5]</p><h2 id="CUDA-事件"><a href="#CUDA-事件" class="headerlink" title="CUDA 事件"></a>CUDA 事件</h2><h3 id="概念-1"><a href="#概念-1" class="headerlink" title="概念"></a>概念</h3><p>CUDA 事件是 CUDA 流中的一个标记点，检查正在执行的流操作是否已经到达了该点。使用事件可以用来执行以下两个基本任务</p><ul><li>同步流的执行</li><li>监控设备的进展</li></ul><p>CUDA API 提供了在流中任意点插入事件以及查询事件完成的函数。只有当一个 CUDA 流中，事件标注点之前的所有操作都执行完成后，该事件才会完成，在默认流中的指定事件，适用于 CUDA 流中先前的所有操作</p><h3 id="声明-1"><a href="#声明-1" class="headerlink" title="声明"></a>声明</h3><p>一个 CUDA 事件声明如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaEvent_t event;</span><br></pre></td></tr></table></figure><p>创建事件</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaEventCreate</span> <span class="params">( cudaEvent_t* event )</span></span></span><br></pre></td></tr></table></figure><p>销毁事件</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaEventDestroy</span> <span class="params">( cudaEvent_t event )</span></span></span><br></pre></td></tr></table></figure><p>如果回收指令执行的时候事件还没有完成，那么回收指令立即完成，当事件完成后，资源被回收</p><p>事件也可以看作是流的一次操作，通过下面函数排队添加到 CUDA 流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaEventRecord</span> <span class="params">( cudaEvent_t event, cudaStream_t stream = <span class="number">0</span> )</span></span></span><br></pre></td></tr></table></figure><p>在流中的事件用于等待前面的操作完成，或测试流中操作的完成情况，可以使用下面的函数阻塞主机线程直到事件被完成，类似于<code>cudaStreamSynchronize</code>函数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaEventSynchronize</span> <span class="params">( cudaEvent_t event )</span></span></span><br></pre></td></tr></table></figure><p><code>cudaEventQuery</code>函数用于检查事件之前的所有操作是否都已经完成，但不会阻塞主机。当所有操作都完成时函数会返回<code>cudaSuccess</code>。否则返回<code>cudaErrorNotReady</code>。类似于<code>cudaStreamQuery</code></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaEventQuery</span> <span class="params">( cudaEvent_t event )</span></span></span><br></pre></td></tr></table></figure><h3 id="记录事件和计算运行时间"><a href="#记录事件和计算运行时间" class="headerlink" title="记录事件和计算运行时间"></a>记录事件和计算运行时间</h3><p>下面函数记录两个事件 start 和 stop 之间的时间间隔，毫秒单位。此外，这两个事件可以在不同流中</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaEventElapsedTime</span> <span class="params">( <span class="type">float</span>* ms, cudaEvent_t start, cudaEvent_t end )</span></span></span><br></pre></td></tr></table></figure><p>下面是一段记录事件时间间隔的示例代码，两个事件被插入到空流中，作为标记，然后记录他们之间的时间间隔。但是这里时间间隔可能会比实际大一些，因为这里用到 <code>cudaEventRecord</code> 函数是异步的，计算会有延时</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// create two events</span></span><br><span class="line">cudaEvent_t start, stop;</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line"><span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// record start event on the default stream</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(start);</span><br><span class="line"></span><br><span class="line"><span class="comment">// execute kernel</span></span><br><span class="line">kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(arguments);</span><br><span class="line"></span><br><span class="line"><span class="comment">// record stop event on the default stream</span></span><br><span class="line"><span class="built_in">cudaEventRecord</span>(stop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// wait until the stop event completes</span></span><br><span class="line"><span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// calculate the elapsed time between two events</span></span><br><span class="line"><span class="type">float</span> time;</span><br><span class="line"><span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop);</span><br><span class="line"></span><br><span class="line"><span class="comment">// clean up the two events</span></span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line"><span class="built_in">cudaEventDestroy</span>(stop);</span><br></pre></td></tr></table></figure><h2 id="流同步"><a href="#流同步" class="headerlink" title="流同步"></a>流同步</h2><p>在非空流中，所有操作对于主机来说都是并行的，如果我们想在某一刻等待，执行当前时刻所有操作同步，就会导致等待时资源的闲置，浪费性能</p><p>从主机的角度，CUDA 操作可以分为两类</p><ul><li>内核启动</li><li>内存操作</li></ul><p>其中内核启动总是异步的，内存操作可以是同步或异步</p><p>前面我们说到有两种类型的流，按同步异步分，又可分为</p><ul><li>同步流（空流）</li><li>异步流（非空流）</li></ul><p>显式声明的都是异步流，异步流通常不会阻塞主机。而在隐式声明的同步流中，部分操作会造成阻塞，让主机等待</p><p>异步流并不都是非阻塞的，可进一步分为如下两种类型</p><ul><li>阻塞流</li><li>非阻塞流</li></ul><p>如果一个异步流被声明为非阻塞的，就不会被空流阻塞，如果声明为阻塞流，则会被空流阻塞</p><h3 id="阻塞流与非阻塞流"><a href="#阻塞流与非阻塞流" class="headerlink" title="阻塞流与非阻塞流"></a>阻塞流与非阻塞流</h3><p><code>cudaStreamCreate</code>创建的是阻塞流，意味着流中的操作可以被阻塞，直到空流中某些操作完成。任何发布到阻塞流中的操作，都要等待空流中先前的操作执行结束才开始执行</p><p>举例代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Kernel1&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, Stream1&gt;&gt;&gt;();</span><br><span class="line">Kernel2&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">Kernel3&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, Stream2&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure><p><code>Kernel1</code>在执行结束后才执行 <code>Kernel2</code>，<code>Kernel2</code> 执行结束后才执行 <code>Kernel3</code></p><p>下面的函数用于创建一个非阻塞流</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaStreamCreateWithFlags</span> <span class="params">( cudaStream_t* pStream, <span class="type">unsigned</span> <span class="type">int</span>  flags )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>pStream</code>：一个指向 <code>cudaStream_t</code> 类型的指针，用于存储创建的流的句柄</li><li><code>flags</code>：流的行为标志，可选参数，默认为0。当前支持的标志只有 <code>cudaStreamNonBlocking</code>，指定在创建的流中运行的工作可以与 Stream0（空流）中的工作同时运行，并且创建的流不应该与 Stream0 执行隐式同步</li></ul><h3 id="隐式同步"><a href="#隐式同步" class="headerlink" title="隐式同步"></a>隐式同步</h3><p>这里的同步也可以说是阻塞，例如在调用 <code>cudaMemcpy</code> 函数时，会隐式同步设备和主机，也可以说其它操作在数据传输完成前都会被阻塞。运行带有隐式同步行为的操作时会导致不必要的阻塞，造成性能下降。此外，如下与内存有关的操作都会有隐式同步，需要格外注意</p><ul><li>锁页主机内存分布</li><li>设备内存分配</li><li>设备内存初始化</li><li>同一设备上两地址之间的内存复制</li><li>一级缓存/共享内存配置修改</li></ul><h3 id="显式同步"><a href="#显式同步" class="headerlink" title="显式同步"></a>显式同步</h3><p>常见的显式同步有</p><ul><li>同步设备：<code>cudaDeviceSynchronize</code></li><li>同步流：<code>cudaStreamSynchronize</code>，<code>cudaStreamQuery</code></li><li>同步流中的事件：<code>cudaEventSynchronize</code>，<code>cudaEventQuery</code></li><li>使用事件跨流同步：<code>cudaEventRecord</code>，<code>cudaStreamWaitEvent</code></li></ul><p>其中，除了最后一个函数，其他我们都有所介绍</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaStreamWaitEvent</span> <span class="params">( cudaStream_t stream, cudaEvent_t event, <span class="type">unsigned</span> <span class="type">int</span>  flags = <span class="number">0</span> )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>stream</code>：要等待事件的 CUDA 流</li><li><code>event</code>：等待的 CUDA 事件</li><li><code>flags</code>：控制等待事件时的行为。可选参数，默认为 0。可以使用 <code>cudaEventBlockingSync</code>（阻塞同步）或<code>cudaEventDisableTiming</code>（禁用事件记录）等标志</li></ul><p>这个函数的作用是指定的流等待指定的事件，事件完成后流才能继续，其中的事件可以在任意流中，当在不同的流的时候，就实现了事件跨流同步</p><p>如下图所示，Stream2 在调用 <code>cudaStreamWaitEvent</code> 函数后执行跨流同步，确保 Stream1 创建的事件是满足依赖关系的</p><p><img src="/image/CUDA编程-CUDA流-事件与同步/8.png" alt=""></p><h3 id="可配置事件"><a href="#可配置事件" class="headerlink" title="可配置事件"></a>可配置事件</h3><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaEventCreateWithFlags</span> <span class="params">( cudaEvent_t* event, <span class="type">unsigned</span> <span class="type">int</span>  flags )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>event</code>：指向<code>cudaEvent_t</code>类型的指针，用来存储创建的CUDA事件对象</li><li><code>flags</code>：用来指定事件对象的创建标志</li></ul><p>其中 flag 可选参数如下</p><ul><li><code>cudaEventDefault</code>：默认事件创建标志</li><li><code>cudaEventBlockingSync</code>：指定事件应该使用阻塞同步。 使用 <code>cudaEventSynchronize()</code> 等待使用此标志创建的事件的主机线程将阻塞，直到事件实际完成</li><li><code>cudaEventDisableTiming</code>：指定创建的事件不需要记录计时数据。 当与 <code>cudaStreamWaitEvent()</code> 和 <code>cudaEventQuery()</code> 一起使用时，使用指定此标志创建的事件和未指定 <code>cudaEventBlockingSync</code> 标志将提供最佳性能</li><li><code>cudaEventInterprocess</code>：指定创建的事件可以用作进程间事件，<code>cudaEventInterprocess</code> 必须与 <code>cudaEventDisableTiming</code> 一起指定</li></ul>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA内存管理（三）</title>
      <link href="/2024/01/23/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2024/01/23/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>CUDA C 编程中的简单的内存管理，线程操作，如何编写核函数，使用 Thrust 库，并行计算，性能分析工具。</p><span id="more"></span><h2 id="获取-GPU-信息"><a href="#获取-GPU-信息" class="headerlink" title="获取 GPU 信息"></a>获取 GPU 信息</h2><p>CUDA  提供了几种获取 GPU 信息的方法，这里介绍一下通过调用 <code>cuda_runtime.h</code>中的 API 得到 GPU 的一些属性。</p><blockquote><p>在编写 CUDA C 程序时， 要将文件命名为 <code>*.cu</code>，一般使用 nvcc 命令编译运行，为 CUDA程序文件，支持 C/C++ 语法。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp devProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;devProp, dev);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;GPU Device Name&quot;</span> &lt;&lt; dev &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; devProp.name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;SM Count: &quot;</span> &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Shared Memory Size per Thread Block: &quot;</span> &lt;&lt; devProp.sharedMemPerBlock / <span class="number">1024.0</span> &lt;&lt; <span class="string">&quot; KB&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per Thread Block: &quot;</span> &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Warps per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor / <span class="number">32</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc checkDeviceInfor.cu -o checkDeviceInfor</span><br></pre></td></tr></table></figure><p>输出如下，SM 数量为 30，每个线程块的共享内存为 48KB，每个线程块有 1024 个线程，每个 SM 有 1536 个线程，每个 SM 有 48 个线程束</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU Device Name0: NVIDIA GeForce RTX 3060 Laptop GPU</span><br><span class="line">SM Count: 30</span><br><span class="line">Shared Memory Size per Thread Block: 48 KB</span><br><span class="line">Threads per Thread Block: 1024</span><br><span class="line">Threads per SM: 1536</span><br><span class="line">Warps per SM: 48</span><br></pre></td></tr></table></figure><h2 id="初步内存管理"><a href="#初步内存管理" class="headerlink" title="初步内存管理"></a>初步内存管理</h2><p>主机和设备各自拥有独立的内存，C 拥有标准库可以管理主机的内存，CUDA 提供的 API 管理设备的内存，下面是 C 和 CUDA 的部分内存管理函数</p><div class="table-container"><table><thead><tr><th>C</th><th>CUDA</th><th>功能</th></tr></thead><tbody><tr><td>malloc</td><td>cudaMalloc</td><td>分配内存</td></tr><tr><td>memcpy</td><td>cudaMemcpy</td><td>复制内存</td></tr><tr><td>memset</td><td>cudaMemset</td><td>设置内存</td></tr><tr><td>free</td><td>cudaFree</td><td>释放内存</td></tr></tbody></table></div><h3 id="主机与设备的数据拷贝"><a href="#主机与设备的数据拷贝" class="headerlink" title="主机与设备的数据拷贝"></a>主机与设备的数据拷贝</h3><p>下面的程序举例了如何使用进行主机与设备的数据拷贝，使用了 <code>cudaMalloc</code>，<code>cudaMemcpy</code> 和 <code>cudaFree</code> 函数，函数形参如下</p><ul><li><p><code>__host__ cudaError_t cudaMalloc (void** devPtr, size_t size)</code></p><ul><li><code>devPtr</code>: 开辟数据的首指针</li><li><code>size</code>: 开辟的设备内存空间长度</li></ul></li><li><p><code>__host__ cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)</code></p><ul><li><code>dst</code>: 目的数据内存首指针</li><li><code>src</code>: 源数据首指针</li><li><code>count</code>: 数据长度</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li><li><p><code>__host__ cudaError_t cudaFree (void* devPtr)</code></p></li><li><code>devPtr</code>: 设备变量指针</li></ul><p>上述函数的返回值类型都是 <code>cudaError_t</code>，以枚举形式保存各种错误类型</p><p>更多运行时函数详解见<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;dev_dets, <span class="built_in">sizeof</span>(dets));</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(dev_dets, dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(&amp;host_dets, dev_dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>);i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>上面的程序使用<code>cudaMalloc</code>来申请设备内存，但二维数组不推荐这么做，在 kernel 运算时较高的性能损失，CUDA 给出了二维数组专用的内存申请函数<code>cudaMallocPitch</code>，在设备间内存拷贝时，也要使用<code>cudaMemcpy2D</code>函数，形参如下</p><ul><li><code>__host__cudaError_t cudaMallocPitch ( void** devPtr, size_t* pitch, size_t width, size_t height )</code><ul><li><code>devPtr</code>: 开辟矩阵的数据的首指针</li><li><code>pitch</code>: 分配存储器的宽度</li><li><code>width</code>: 二维数组列数</li><li><code>height</code>: 二维数组行数</li></ul></li><li><code>__host__ cudaError_t cudaMemcpy2D ( void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind )</code><ul><li><code>dst</code>: 目的矩阵内存首指针</li><li><code>dpitch</code>:  dst指向的 2D 数组中的内存宽度，以字节为单位，是cuda为了读取方便，对齐过的内存宽度，可能大于一行元素占据的实际内存</li><li><code>src</code>: 源矩阵内存首指针</li><li><code>spitch</code>: src 指向的 2D 数组中的内存宽度</li><li><code>width</code>: src指向的2D数组中一行元素占据的实际宽度，为 <code>width*sizeof(type)</code></li><li><code>height</code>: src指向的2D数组的行数</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="type">size_t</span> width = <span class="number">4</span>;</span><br><span class="line">    <span class="type">size_t</span> height = <span class="number">6</span>;</span><br><span class="line">    <span class="type">size_t</span> pitch;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMallocPitch</span>((<span class="type">void</span> **)&amp;dev_dets, &amp;pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height);</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(dev_dets, pitch, dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(&amp;host_dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, dev_dets, pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;width*height;i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>这两个函数应该会使 kernel 的运行时间变短，因为 pitch 对齐后可实现 global 内存联合访问，但<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>会变慢，因为比一维的操作多了对齐的考虑</p><h2 id="Kernel-函数"><a href="#Kernel-函数" class="headerlink" title="Kernel 函数"></a>Kernel 函数</h2><h3 id="kernel-限定词"><a href="#kernel-限定词" class="headerlink" title="kernel 限定词"></a>kernel 限定词</h3><ul><li><code>__device__</code>: 在设备上执行，只能在设备上调用；</li><li><code>__global__</code>: 在设备上执行，只能在主机上调用；</li><li><code>__host__</code>: 在主机上执行，只能在主机上调用。</li></ul><p><code>__device__</code>和<code>__global__</code>代表函数在设备上执行，不支持递归，不能在函数体内声明静态变量，静态变量对应于CPU的整个程序生命过程，不能有可变长参数；</p><p><code>__global__</code>和<code>__host__</code>不能一起使用，而<code>__device__</code>和<code>__host__</code>可以一起使用，编译器会在 CPU 和 GPU 各复制一份函数。</p><p>不添加限定词时，函数默认为<code>__host__</code>，也就是在主机上执行。</p><p>所有的 kernel 函数返回类型都是 void，且 kernel 函数都是异步执行。</p><h3 id="kernel-调用方式"><a href="#kernel-调用方式" class="headerlink" title="kernel 调用方式"></a>kernel 调用方式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_func</span> <span class="params">(param list)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line">kernel_func &lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt; (param list);</span><br></pre></td></tr></table></figure><ul><li><code>&lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt;</code>: 是运算符内是核函数的执行参数，告诉编译器运行时如何启动核函数</li><li><code>Dg</code>: grid 的维度和尺寸，dim3 类型，意为一个 grid 有多少个 block</li><li><code>Db</code>: block 的维度和尺寸， dim3 类型，意为一个 block 有多少个 thread</li><li><code>Ns</code>: （可选）用于设置每个block除了静态分配的 shared Memory 以外，最多能动态分配的 shared Memory 大小，单位为 byte 不需要动态分配时该值为0或省略不写</li><li><code>S</code>: （可选） cudastream 类型的参数，表示该核函数处在哪个流之中</li></ul><p>这里我们实现一下第二章最后的例子，下面的程序使用了<code>cudaDeviceSynchronize</code>和<code>cudaDeviceReset</code>函数，解释如下</p><ul><li><code>__host__ __device__ cudaDeviceSynchronize</code>: 使设备阻塞到完成所有前面请求的任务，CUDA 11.6 后已弃用</li><li><code>__host__ cudaDeviceReset</code>: 显式销毁并清除当前进程中与设备关联的所有资源，资源不能再被访问，可能导致未定义的行为</li></ul><p>由于 CUDA printf 的输出存储在缓冲中，后台同步机制会有延时，需要使用上面两个同步函数中任意一个使 printf 函数的内容与主机同步，即可输出</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">printThreadIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy*blockDim.x * gridDim.x + ix;</span><br><span class="line">    <span class="keyword">if</span>(threadIdx.x == <span class="number">3</span> &amp;&amp; threadIdx.y == <span class="number">1</span> &amp;&amp; blockIdx.x == <span class="number">0</span> &amp;&amp; blockIdx.y == <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;thread_id (%d,%d) block_id (%d,%d) coordinate (%d, %d), global index %2d \n&quot;</span>, threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, ix, iy, idx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    printThreadIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    <span class="comment">// cudaDeviceSynchronize(); </span></span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="built_in">thread_id</span> (<span class="number">3</span>,<span class="number">1</span>) <span class="built_in">block_id</span> (<span class="number">0</span>,<span class="number">1</span>) <span class="built_in">coordinate</span> (<span class="number">3</span>, <span class="number">3</span>), global index <span class="number">27</span></span><br></pre></td></tr></table></figure><blockquote><p>在输出时不能使用 std::cout, std 命名空间不能使用到 GPU 上</p></blockquote><h2 id="CUDA-的-Thrust-库"><a href="#CUDA-的-Thrust-库" class="headerlink" title="CUDA 的 Thrust 库"></a>CUDA 的 Thrust 库</h2><p>CUDA 的 Thrust 库是基于标准模板库 STL 的 CUDA 的 C++ 模板库， 通过与 CUDA C 配合使用，节省了大量优化算法的时间，保证了性能与开发效率，在 CUDA Toolkit 中包含 Thrust，无需额外安装，只需导入相应头文件，在调用时使用 <code>thrust</code> 命名空间，并尽量不要使用 <code>using namespace std;</code> 语句，因为 thrust 库和 STL 库非常多的重名</p><h3 id="Vector-容器"><a href="#Vector-容器" class="headerlink" title="Vector 容器"></a>Vector 容器</h3><p>Thrust 中定义了主机端和设备端的两种 vector，分别定义在 host_vector.h 和 device_vector.h 中，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// H has storage for 4 integers</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(<span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize individual elements</span></span><br><span class="line">    H[<span class="number">0</span>] = <span class="number">14</span>;</span><br><span class="line">    H[<span class="number">1</span>] = <span class="number">20</span>;</span><br><span class="line">    H[<span class="number">2</span>] = <span class="number">38</span>;</span><br><span class="line">    H[<span class="number">3</span>] = <span class="number">46</span>;</span><br><span class="line">    H.<span class="built_in">push_back</span>(<span class="number">52</span>);</span><br><span class="line">    <span class="comment">// H.size() returns the size of vector H</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// print contents of H</span></span><br><span class="line">    <span class="comment">// for(int i = 0; i &lt; H.size(); i++)</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:H)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// resize H</span></span><br><span class="line">    H.<span class="built_in">resize</span>(<span class="number">2</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H now has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// Copy host_vector H to device_vector D</span></span><br><span class="line">    thrust::device_vector&lt;<span class="type">int</span>&gt; D = H; </span><br><span class="line">    <span class="comment">// elements of D can be modified</span></span><br><span class="line">    D[<span class="number">0</span>] = <span class="number">99</span>;</span><br><span class="line">    D[<span class="number">1</span>] = <span class="number">88</span>;</span><br><span class="line">    <span class="comment">// print contents of D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// H and D are automatically deleted when the function returns</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">H has size <span class="number">5</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">38</span></span><br><span class="line"><span class="number">46</span></span><br><span class="line"><span class="number">52</span></span><br><span class="line">H now has size <span class="number">2</span></span><br><span class="line"><span class="number">99</span></span><br><span class="line"><span class="number">88</span></span><br></pre></td></tr></table></figure><p>Thrust 允许使用 <code>=</code> 运算符对 <code>host_vector</code> 和 <code>device_vector</code> 的相互拷贝，也允许使用 <code>[i]</code> 下标访问 <code>device_vector</code> 的各个元素，但是用这种方法访问每一次都需要调用 <code>cudaMemcpy</code>，性能损失较大，应谨慎使用。 下面我们将介绍一些更有效的技术</p><p>下面展示 Thrust 提供的几种对 vector 操作的方法，包括初始化，赋值，<code>iterator</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize all ten integers of a device_vector to 1</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">D</span><span class="params">(<span class="number">10</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the first seven elements of a vector to 9</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">begin</span>() + <span class="number">7</span>, <span class="number">9</span>);</span><br><span class="line">    <span class="comment">// initialize a host_vector with the first five elements of D</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(D.begin(), D.begin() + <span class="number">5</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the elements of H to 0, 1, 2, 3, ...</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// copy all of H back to the beginning of D</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>(), D.<span class="built_in">begin</span>());</span><br><span class="line">    <span class="comment">// print D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)dd</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">9</span> <span class="number">9</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>上面的程序使用了<code>thrust::fill</code>，当它对 <code>device_vector iterator</code> 操作时，会在编译时检查 <code>iterator</code> 在主机上还是在设备上，这个过程被称为静态调度，意味着调度过程没有运行时开销</p><h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><p>thrust 中定义了 <code>device_ptr</code> 数据类型，当传入函数的指针指向设备端内存时，需要用<code>device_ptr</code>进行封装</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> N = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// raw pointer to device memory</span></span><br><span class="line"><span class="type">int</span> * raw_ptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;raw_ptr, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"><span class="comment">// wrap raw pointer with a device_ptr </span></span><br><span class="line"><span class="function">thrust::device_ptr&lt;<span class="type">int</span>&gt; <span class="title">dev_ptr</span><span class="params">(raw_ptr)</span></span>;</span><br><span class="line"><span class="comment">// use device_ptr in thrust algorithms</span></span><br><span class="line">thrust::<span class="built_in">fill</span>(dev_ptr, dev_ptr + N, (<span class="type">int</span>) <span class="number">0</span>);</span><br></pre></td></tr></table></figure><h3 id="数值操作"><a href="#数值操作" class="headerlink" title="数值操作"></a>数值操作</h3><h4 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h4><p>Transformations 是对一个输入范围中的每一个元素应用操作，将结果存储在给定范围中的方法，上面程序中已经 <code>thrust::fill</code> 就是一个 Transformations，它将范围内的所有元素设置为指定值。下面的程序用到了 <code>thrust::sequence</code>，<code>thrust::replace</code>，<code>thrust::transform</code>，更多 Transformations 请查看<a href="https://thrust.github.io/doc/group__transformations.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/replace.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// allocate three device_vectors with 10 elements</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">X</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Y</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Z</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize X to 0,1,2,3, ....</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// compute Y = -X</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">negate</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// fill Z with twos</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(Z.<span class="built_in">begin</span>(), Z.<span class="built_in">end</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="comment">// compute Y = X mod 2</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Z.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">modulus</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// replace all the ones in Y with tens</span></span><br><span class="line">    thrust::<span class="built_in">replace</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), <span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="comment">// print Y</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), std::<span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(std::cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> </span><br></pre></td></tr></table></figure><h4 id="SAXPY"><a href="#SAXPY" class="headerlink" title="SAXPY"></a>SAXPY</h4><p>SAXPY（Scalar Alpha X Plus Y）是一个在 BLAS（Basic Linear Algebra Subprograms）函数库提供中的函数，并且是一个并行向量处理机（vector processor）中常用的计算操作指令，为标量乘法和向量加法的组合，如 $y = a*x + y$，其中 $x$ 和 $y$ 为向量，$a$ 为标量常数。下面的程序定义了一个 functor 实现 SAXPY</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">saxpy_functor</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> a;</span><br><span class="line">    <span class="built_in">saxpy_functor</span>(<span class="type">float</span> _a) : <span class="built_in">a</span>(_a) &#123;&#125;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        <span class="type">float</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> <span class="type">float</span>&amp; x, <span class="type">const</span> <span class="type">float</span>&amp; y)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> a * x + y;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">saxpy</span><span class="params">(<span class="type">float</span> A, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; X, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; Y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// y = a * x + y</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), <span class="built_in">saxpy_functor</span>(A));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Reductions"><a href="#Reductions" class="headerlink" title="Reductions"></a>Reductions</h4><p>使用 <code>thrust::reduce</code> 函数对一组数据进行操作，返回值为一个具体数值，下例就是对一组数据求和</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>, thrust::<span class="built_in">plus</span>&lt;<span class="type">int</span>&gt;());</span><br></pre></td></tr></table></figure><p>上列中<code>(int) 0</code>为计算的初始值，<code>thrust::plus&lt;int&gt;()</code>为操作符，当没有定义初始值和操作符时，它们是默认值，因此下面的两条语句和上面的等价，更多操作符请查看<a href="https://nvidia.github.io/thrust/api/groups/group__reductions.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>);</span><br><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p><code>thrust::transform_reduce</code>允许接受多个操作符来对一组数据求值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform_reduce.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// square&lt;T&gt; computes the square of a number f(x) -&gt; x*x</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">square</span> &#123;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        T <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> T&amp; x)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> x * x;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize host array</span></span><br><span class="line">    <span class="type">float</span> x[<span class="number">4</span>] = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;;</span><br><span class="line">    <span class="comment">// transfer to device</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">float</span>&gt; <span class="title">d_x</span><span class="params">(x, x + <span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// setup arguments</span></span><br><span class="line">    square&lt;<span class="type">float</span>&gt; unary_op;</span><br><span class="line">    thrust::plus&lt;<span class="type">float</span>&gt; binary_op;</span><br><span class="line">    <span class="type">float</span> init = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// compute norm</span></span><br><span class="line">    <span class="type">float</span> norm = std::<span class="built_in">sqrt</span>( thrust::<span class="built_in">transform_reduce</span>(d_x.<span class="built_in">begin</span>(), d_x.<span class="built_in">end</span>(), unary_op, init, binary_op));</span><br><span class="line">    std::cout &lt;&lt; norm &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">5.47723</span></span><br></pre></td></tr></table></figure><p>上面的程序对一组数据计算平方和再开方，这种写法会大大优化性能。</p><h4 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h4><p>对数据进行排序，很常用的排序功能，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sort.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort</span>(A, A + N);</span><br><span class="line"><span class="comment">// A is now &#123;1, 2, 4, 5, 7, 8&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span>    keys[N] = &#123;  <span class="number">1</span>,   <span class="number">4</span>,   <span class="number">2</span>,   <span class="number">8</span>,   <span class="number">5</span>,   <span class="number">7</span>&#125;;</span><br><span class="line"><span class="type">char</span> values[N] = &#123;<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort_by_key</span>(keys, keys + N, values);</span><br><span class="line"><span class="comment">// keys is now   &#123;  1,   2,   4,   5,   7,   8&#125;</span></span><br><span class="line"><span class="comment">// values is now &#123;&#x27;a&#x27;, &#x27;c&#x27;, &#x27;b&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;d&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">stable_sort</span>(A, A + N, thrust::<span class="built_in">greater</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line"><span class="comment">// A is now &#123;8, 7, 5, 4, 2, 1&#125;</span></span><br></pre></td></tr></table></figure><p>上例中的 <code>thrust::stable_sort</code>接受用户自定义比较运算符</p><h4 id="max-element-min-element"><a href="#max-element-min-element" class="headerlink" title="max_element(min_element)"></a>max_element(min_element)</h4><p>求最大（小）值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/extrema.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line">thrust::device_vector&lt;type&gt;::iterator iter = thrust::<span class="built_in">max_element</span>(dvec.<span class="built_in">begin</span>()，dvec.<span class="built_in">end</span>());</span><br><span class="line"><span class="type">int</span> position = iter - dvec.<span class="built_in">begin</span>();</span><br><span class="line">type max_val = *iter;</span><br></pre></td></tr></table></figure><p>其返回值是一个迭代器，需要获取最大（小）值所在位置，再得到结果</p><h4 id="unique"><a href="#unique" class="headerlink" title="unique"></a>unique</h4><p>将一组数据中满足条件的数据筛选出来，可自定义筛选条件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/unique.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_same</span> &#123;</span><br><span class="line"><span class="function">__host__ __device__</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> float3 &amp;p1, <span class="type">const</span> float3 &amp;p2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (p1.x==p2.x) &amp;&amp; (p1.y==p2.y) &amp;&amp; (p1.z==p2.z);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_same</span>()),p.<span class="built_in">end</span>();</span><br><span class="line">p.<span class="built_in">erase</span>(thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_sam</span>()),p.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p>unique 函数的功能只是将满足条件的数据筛选出来，无法直接删除，需要结合 vector 的 erase 函数进行删除</p><h2 id="建立-CUDA-的并行线程计算"><a href="#建立-CUDA-的并行线程计算" class="headerlink" title="建立 CUDA 的并行线程计算"></a>建立 CUDA 的并行线程计算</h2><p>下面的程序为大家演示以结构体类型存储的矩阵计算，后续章节会教大家使用 cuBLAS 库进行并行计算</p><h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p>下面的程序进行了 $C = A + B$ 矩阵加法运算，下面的程序中使用了<code>cudaMallocManaged</code>函数，简单来说，就是结合了之前讲到的<code>cudaMalloc</code>和<code>cudaMemcpy</code>等内存迁移拷贝的操作，自动内存管理，方便代码编写，弊端是在 kernel 执行时会降低 kernel 的执行效率，在后续章节，我们会详细讲解有关 CUDA 的内存管理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="type">int</span> w;</span><br><span class="line">    <span class="type">int</span> h;</span><br><span class="line">    <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">        A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>下面的程序进行了 $C = A * B$ 矩阵乘法运算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMul</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">float</span> k = <span class="number">0.0</span>;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;A-&gt;w; i++)</span><br><span class="line">                k += <span class="built_in">getValue</span>(A, row, i) * <span class="built_in">getValue</span>(B, i, col);</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="为运行程序计时"><a href="#为运行程序计时" class="headerlink" title="为运行程序计时"></a>为运行程序计时</h2><h3 id="nvprof"><a href="#nvprof" class="headerlink" title="nvprof"></a>nvprof</h3><p>nvprof 是过去比较常用的命令行工具，但在终端直接输入<code>nvprof ./*.o</code>会得到以下 Warning</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.</span><br><span class="line">                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.</span><br><span class="line">                  Refer https://developer.nvidia.com/tools-overview for more details.</span><br></pre></td></tr></table></figure><p>目前主流的 CUDA 驱动不再支持<code>nvprof</code>命令，但我们仍可以在 NVIDIA Nsight Systems 中使用，在终端输入 <code>nsys nvprof ./*.o</code>就可以看到CUDA 程序执行的具体内容</p><p>这里我们以主机与设备的数据拷贝的两个程序为例</p><p>使用<code>cudaMalloc</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 1d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-01f4.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report5.nsys-rep</span><br><span class="line">[2/7] [========================100%] report5.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)     Name   </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------</span><br><span class="line">     99.9        137542088          1  137542088.0  137542088.0  137542088  137542088          0.0  cudaMalloc</span><br><span class="line">      0.1           163239          1     163239.0     163239.0     163239     163239          0.0  cudaFree  </span><br><span class="line">      0.0            36460          2      18230.0      18230.0      18070      18390        226.3  cudaMemcpy</span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     51.6             1504      1    1504.0    1504.0      1504      1504          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     48.4             1408      1    1408.0    1408.0      1408      1408          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report5.nsys-rep</span><br><span class="line">    /root/report5.sqlite</span><br></pre></td></tr></table></figure><p>使用<code>cudaMallocPitch</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 2d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-6614.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report6.nsys-rep</span><br><span class="line">[2/7] [========================100%] report6.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)       Name      </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------</span><br><span class="line">    100.0        745692893          1  745692893.0  745692893.0  745692893  745692893          0.0  cudaMallocPitch</span><br><span class="line">      0.0           161820          1     161820.0     161820.0     161820     161820          0.0  cudaFree       </span><br><span class="line">      0.0            39090          2      19545.0      19545.0      16590      22500       4179.0  cudaMemcpy2D   </span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     64.8             2880      1    2880.0    2880.0      2880      2880          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     35.2             1567      1    1567.0    1567.0      1567      1567          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report6.nsys-rep</span><br><span class="line">    /root/report6.sqlite</span><br></pre></td></tr></table></figure><p>这里我们可以看到<code>Total Time</code>中<code>cudaMallocPitch</code>函数用时几乎是<code>cudaMalloc</code>的 5 倍，更加肯定了我们的说法，<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>需要额外对二维数据进行对齐操作</p><h3 id="cudaEvent-计时函数"><a href="#cudaEvent-计时函数" class="headerlink" title="cudaEvent 计时函数"></a>cudaEvent 计时函数</h3><p>以前面的矩阵乘法为例，分别计算 CUDA 开辟内存时间和矩阵乘法运算时间</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;cudaMalloc cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Matrix multiplication cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">cudaMalloc cost: <span class="number">0.161696</span>s</span><br><span class="line">Matrix multiplication cost: <span class="number">0</span>s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA内存管理（二）</title>
      <link href="/2024/01/16/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024/01/16/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>详细讲解共享内存存储体、存储体冲突、访问模式以及内存填充等知识点。</p><span id="more"></span><h2 id="CUDA-共享内存"><a href="#CUDA-共享内存" class="headerlink" title="CUDA 共享内存"></a>CUDA 共享内存</h2><p>GPU 的物理内存可以分为</p><ul><li>板载内存</li><li>片上内存</li></ul><p>全局内存就是板载内存，有较高的延时；共享内存就是较小的片上内存 ，有较低的延时。共享有比全局内存更高的带宽，可以把它当作一个可编程的缓存。共享内存通常的用途有</p><ul><li>块内线程通信的通道</li><li>用于全局内存数据的可编程管理的缓存</li><li>高速暂存存储器，用于转换数据，优化全局内存访问模式</li></ul><p>共享内存（Shared Memory, SMEM），在 GPU 中的位置如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/1.png" alt=""></p><p>每个 SM 都有一个小的内存池，这个内存池被当前正在该 SM 上执行的线程块中的所有线程所共享。SMEM 使同一个线程块中的线程能够互相协作，以重用片上数据，并可以大大降低核函数所需的全局内存带宽。由于 SMEM 中的内容是由应用程序显式管理的，所以是可编程管理的缓存</p><p>上图可以看到 SMEM 不需要经过 L1，相比 DRAM，延迟低 20~30 倍，带宽为 DRAM 的 10倍</p><p>在每个线程块被执行时会分配给它一些 SMEM，线程块执行完毕后 SMEM 释放，线程块和它的 SMEM 有相同的生命周期。每个线程束对 SMEM 的访问请求分为以下几种情况</p><ul><li><p>最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存，一个事务完成整个线程束的访问</p></li><li><p>最坏的情况是有冲突访问，每个线程束的 32 个线程需要不同的 32 个事务来完成</p></li><li><p>如果线程束内 32 个线程访问 SMEM 中的同一个地址，那么一个线程访问完后以广播的形式告诉其它线程</p></li></ul><p>一个 SM 上的所有的正在执行的线程块共会划分有限的 SMEM 资源，所以核函数使用的共享内存越多，那么处于并发活跃状态的线程块就越少</p><p>下面将围绕避免 SMEM 中多个事务访问冲突的问题展开讨论</p><h3 id="共享内存分配"><a href="#共享内存分配" class="headerlink" title="共享内存分配"></a>共享内存分配</h3><p>可以动态的或静态的声明使用共享内存的变量。共享内存变量在核函数中声明，作用域就只在核函数中，在核函数外声明，对所有核函数来说作用域都是全局的，我们可以声明一维，二维和三维的共享内存数组</p><p>使用<code>__shared__</code>修饰符来声明共享内存变量，下面声明了使用共享内存的一维，二维和三维浮点数组</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">float</span> a[*];</span><br><span class="line">__shared__ <span class="type">float</span> b[*][*];</span><br><span class="line">__shared__ <span class="type">float</span> c[*][*][*];</span><br></pre></td></tr></table></figure><p>这里的 <code>*</code> 必须是一个编译时确定的数字，不能是变量，如果共享内存的大小在编译时是未知的，也就是动态声明一个共享内存数组，使用<code>extern</code>关键字<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> __shared__ <span class="type">int</span> d[];</span><br></pre></td></tr></table></figure></p><p>并将所需的大小按字节数作为三重括号内的第三个参数，<code>isize</code>为数组的中的元素个数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel&lt;&lt;&lt;grid, block, <span class="function">isize * <span class="title">sizeof</span><span class="params">(<span class="type">float</span>)</span>&gt;&gt;&gt;<span class="params">(...)</span></span>;</span><br></pre></td></tr></table></figure><p>注意这里的动态声明只支持一维数组</p><h3 id="共享内存存储体和访问模式"><a href="#共享内存存储体和访问模式" class="headerlink" title="共享内存存储体和访问模式"></a>共享内存存储体和访问模式</h3><p>优化内存性能的着重指标就是</p><ul><li>延时</li><li>带宽</li></ul><p>上面提到，共享内存隐藏了全局内存延迟，并且大大提高了带宽，所以了解共享内存的原理和特性会让我们更为清晰地使用共享内存</p><h4 id="共享内存存储体-bank"><a href="#共享内存存储体-bank" class="headerlink" title="共享内存存储体 (bank)"></a>共享内存存储体 (bank)</h4><p>为了获得高内存带宽，共享内存被分为 32 个同样大小的内存模型，称为存储体（bank），对应一个线程束中的 32 个线程，存储体可以同时被访问。并且共享内存是一个一维地址空间。根据 GPU 的计算能力，共享内存的地址在不同模式下会映射到不同的存储体中。</p><h4 id="存储体冲突（bank-conflict）"><a href="#存储体冲突（bank-conflict）" class="headerlink" title="存储体冲突（bank conflict）"></a>存储体冲突（bank conflict）</h4><p>如果线程束对共享内存有操作，且在个存储体上只请求访问最多一次，那么就由一个内存事务来完成，如果线程束在任意一个存储体上请求访问大于一次，就会由多个内存事务来完成，称为存储体冲突（bank conflict）</p><p>bank conflict 会导致请求被重复执行，GPU 会将存储体冲突的请求访问分割到尽可能多的独立的无冲突事务中，而独立内存事务的数量会直接影响内存带宽</p><p>线程束访问共享内存时有以下三种模式</p><ul><li><p>并行访问，多地址访问多存储体，带宽利用率最高</p><p>如下图所示是最完美的情况，线程束中每个线程对应一个存储体</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/2.png" alt=""></p><p>如下图所示为不规则的访问模式，并行却不冲突，带宽利用率也是最高</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/3.png" alt=""></p><p>如下图所示同样为不规则的访问模式，但如果线程访问的是同一个存储体中相同的地址，广播访问就不会冲突，如果线程访问的是同一个存储体中不同的地址，就会产生冲突</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/4.png" alt=""></p></li><li><p>串行访问，多地址访问同一存储体，就会有对应 32 个线程的 32 个事务，带宽利用率最差</p></li><li><p>广播访问，单一地址读取单一存储体，线程束中所有的线程都读取同一存储体中相同的地址。一个内存事务执行后，那么被访问的字就会被广播到所有请求的线程中。虽然只有一个内存事务，但只有一小部分字节被读取，所以带宽利用率很差</p></li></ul><h4 id="访问模式"><a href="#访问模式" class="headerlink" title="访问模式"></a>访问模式</h4><p>共享内存存储体宽度（bank widths）直接影响访问模式，也就是每个存储体（bank）在一个时钟周期内的带宽，在计算能力 1.x 的设备中 bank widths 为 2 字节（16 位），计算能力 2.x 的设备中为  4  字节（32 位wjg），计算能力 3.x 以上的设备中为 8 字节（64 位）</p><p>对于计算能力为 2.0 的设备来说，bank widths 为 32 位，如下图所示就是共享内存的存储体的访问模式，字节地址除以 4 转换为 4 字节索引，再模 32，将 4 字节索引转换为存储体索引</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/5.png" alt=""></p><p>上面的操作对应如下公式</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bank index = (Byte address ÷ <span class="number">4</span> byte/bank) % <span class="number">32</span> banks</span><br></pre></td></tr></table></figure><p>现今的 GPU 同时支持 64 位模式和 32 位模式。如果为 64 位模式，由于 SMEM 只有 32 个 bank，所以每个 bank 中的地址会被逻辑分成两侧，每个时钟周期内的每个 bank 都有 64 位的带宽，公式中的<code>byte/bank</code>就是 8。如下图所示是 64 位模式的几种情况，这也解释了为什么相比 32 位模式，其更不容易引起冲突</p><ul><li><p>每个线程访问不同的 bank，无 bank conflict</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/7.png" alt=""></p></li><li><p>多个线程访问一个 bank 中同一侧的同一个地址，地址会广播到所有线程；两个线程访问同一个 bank，所以会无 bank conflict</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/8.png" alt=""></p></li><li><p>两个线程访问同一个存储体的同一侧，为 bank conflict</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/9.png" alt=""></p></li><li><p>同一个 bank 的左侧被两个线程同时访问了不同的地址，会导致三向的 bank conflict</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/10.png" alt=""></p></li></ul><p>在现今 GPU 的 32 位模式下，由于 GPU 的每个时钟周期都是 64 带宽，所以 bank 中 32 位的数据需要 2 个时钟周期才能凑够 64 位，这也就使得两个线程读同一个 bank 时，如果读取的两个地址索引分别在两个不同的时钟周期被传输，就不会产生冲突，例如两个线程可以读<code>4-byte word index</code>为 0 和 32 的两个地址。如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/6.png" alt=""></p><p>cuda_runtime.h 提供了如下函数设置当前共享内存访问模式</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaDeviceSetSharedMemConfig</span> <span class="params">( cudaSharedMemConfig config )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>config</code>: 请求的缓存配置，枚举类型，有如下值<ul><li><code>cudaSharedMemBankSizeDefault = 0</code>: 设置 bank widths 为设备默认值</li><li><code>cudaSharedMemBankSizeFourByte = 1</code>: 设置 bank widths 为 4 字节（32 位）</li><li><code>cudaSharedMemBankSizeEightByte = 2</code>: 设置 bank widths 为 8 字节（64 位）</li></ul></li></ul><blockquote><p>注意<code>cudaDeviceSetSharedMemConfig</code>函数在固定共享内存大小的设备上无作用</p></blockquote><p>如下函数查询当前共享内存访问模式</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaDeviceGetSharedMemConfig</span> <span class="params">( cudaSharedMemConfig ** pConfig )</span></span></span><br></pre></td></tr></table></figure><p><code>pConfig</code>: 返回的缓存配置，枚举类型，有如下值</p><ul><li><code>cudaSharedMemBankSizeFourByte = 1</code>: bank widths 为 4 字节（32 位）</li><li><code>cudaSharedMemBankSizeEightByte = 2</code>: bank widths 为 8 字节（64 位）</li></ul><p>在不同的核函数启动之间更改共享内存的配置，可能需要一个隐式的设备同步点，更改共享内存存储体的大小对性能有重大的影响。更大的 bank widths 可能有更高的带宽，也可能导致更多的 bank conflict，需要实验得出</p><p>下面的代码简单地使用了上面两个函数，仅供参考，这里笔主的显卡固定了共享内存大小，无法演示</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">printSharedMemConfig</span><span class="params">(<span class="keyword">enum</span> cudaSharedMemConfig &amp;sharedMemConfig)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">switch</span> (sharedMemConfig) &#123;</span><br><span class="line">        <span class="keyword">case</span> cudaSharedMemBankSizeDefault:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Shared memory config: Default\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> cudaSharedMemBankSizeFourByte:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Shared memory config: Bank size of 4 bytes\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> cudaSharedMemBankSizeEightByte:</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;Shared memory config: Bank size of 8 bytes\n&quot;</span>);</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaSharedMemConfig</span> sharedMemConfig;</span><br><span class="line">    <span class="built_in">cudaDeviceGetSharedMemConfig</span>(&amp;sharedMemConfig);</span><br><span class="line">    <span class="built_in">printSharedMemConfig</span>(sharedMemConfig);</span><br><span class="line">    <span class="built_in">cudaDeviceSetSharedMemConfig</span>(cudaSharedMemBankSizeEightByte);</span><br><span class="line">    <span class="built_in">cudaDeviceGetSharedMemConfig</span>(&amp;sharedMemConfig);</span><br><span class="line">    <span class="built_in">printSharedMemConfig</span>(sharedMemConfig);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="内存填充（memory-padding）"><a href="#内存填充（memory-padding）" class="headerlink" title="内存填充（memory padding）"></a>内存填充（memory padding）</h4><p>内存填充是避免存储单元冲突的一种方法。假设 5 个共享内存存储单元。如果所有线程访问 bank 0 的不同地址，那么会发生一个五向的存储单元冲突。解决这种存储单元冲突的一个方法是在每 5 个元素之后添加一个填充，改变从字到存储单元的映射，以错开访问每行数据</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/11.png" alt=""></p><p>如上图所示，由于填充，之前所有属于 bank 0 的字，现在被传播到了不同的存储单元中。 填充的内存不能用于数据存储，其唯一的作用就是移动数据元素，以便将原来属于同一个存储单元中的数据分散到不同存储单元中。这样可以使得线程块可用的总共享内存的数量减少。 填充之后还需要根据前面的公式重新计算数组索引以确保能访问到正确的数据元素。例如下面的共享内存数组</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">4</span>];</span><br></pre></td></tr></table></figure><p>我们可以更改声明以还原图例中的情况</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__shared__ <span class="type">int</span> a[<span class="number">5</span>][<span class="number">5</span>];</span><br></pre></td></tr></table></figure><h3 id="配置共享内存"><a href="#配置共享内存" class="headerlink" title="配置共享内存"></a>配置共享内存</h3><p>每个 SM 上有 64KB 的片上内存，SMEM 和 L1 共享这 64KB，并且可以配置，CUDA 为配置 L1 和 SMEM 提供以下两种方法</p><ul><li>按设备进行配置</li><li>按核函数进行配置</li></ul><p>为当前设备设置首选缓存配置</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaDeviceSetCacheConfig</span> <span class="params">( cudaFuncCache cacheConfig )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>cacheConfig</code>：请求的缓存配置，枚举类型</li><li><p><code>cudaFuncCachePreferNone = 0</code>: 默认函数缓存配置，无优先级</p></li><li><p><code>cudaFuncCachePreferShared = 1</code>: 首选更大的 SMEM 和更小的 L1 缓存</p></li><li><p><code>cudaFuncCachePreferL1 = 2</code>: 首选较大的 L1 缓存和较小的 SMEM</p></li><li><p><code>cudaFuncCachePreferEqual = 3</code>: 首选大小相同的 L1 缓存和 SMEM</p></li></ul><p>使用上面哪种更好要根据核函数使用了多少 SMEM</p><ul><li>SMEM 使用较多，那么首选更大的 SMEM</li><li>更多的寄存器使用，那么首选较大的 L1</li></ul><p>另一个函数是为当前核函数设置首选缓存配置</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaFuncSetCacheConfig</span> <span class="params">( <span class="type">const</span> <span class="type">void</span>* func, cudaFuncCache cacheConfig )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>func</code>: 设备函数指针</li><li><code>cacheConfig</code>: 请求的缓存配置</li></ul><p>L1 和 SMEM 虽然都在同一个片上，但是与 SMEM 的 bank 不同，L1 通过缓存行进行访问。我们可以完全控制 SMEM，但 L1 的删除工作是硬件完成的</p><p>GPU使用不同的启发式算法来处理数据。在GPU上，数百个线程共享相同的 L1，数千个线程共享有网的 L2。因此，数据处理在 GPU上可能会发生的更频繁而且更不可预知，所以使用 SMEM 不仅可以显式管理数据，还可以保证 SM 的局部性</p><h3 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h3><p>同步是并行的重要机制，其主要目的就是防止冲突。同步基本方法有</p><ul><li>障碍，是所有调用线程等待其余调用线程达到的障碍点</li><li>内存栅栏，所有调用线程必须等到全部内存修改对其余线程可见时才继续进行</li></ul><p>首先需要理解 CUDA 采用的弱排序内存模型</p><h4 id="弱排序内存模型"><a href="#弱排序内存模型" class="headerlink" title="弱排序内存模型"></a>弱排序内存模型</h4><p>CUDA 允许编译器大幅优化源代码以加速程序运行效率，这就会导致内存访问的顺序被改变，也就是说 GPU 线程在不同的内存，比如 SMEM，全局内存，锁页内存或对等设备内存中，写入数据的顺序是不一定和这些数据在源代码中访问的顺序相同。当线程的写入顺序对其他线程可见的时候，它可能和写操作被执行的实际顺序不一致。如果指令之间相互独立，线程从不同内存中读取数据和指令的顺序也不一定相同。在这种不正确情况下，为了保持内存管理的可控，必须在代码中使用障碍和内存栅栏以防止冲突</p><h4 id="显示障碍"><a href="#显示障碍" class="headerlink" title="显示障碍"></a>显示障碍</h4><p>CUDA 中，障碍点只对同一线程块内的线程执行，且只能设置在核函数中，使用如下函数设置一个障碍点</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure><p><code>__syncthreads()</code>作为一个障碍点，保证在同一线程块内所有线程没到达此障碍点时，不能继续向下执行，也就是阻塞 block 直至 block 内的线程全都执行到这一行。且在同一线程块内，此障碍点之前的所有全局内存，共享内存操作，对后面的线程都是可见的。</p><p><code>__syncthreads()</code>也可以解决同一线程块内，内存竞争的问题，保证执行的先后顺序</p><p>此外在条件语句中使用<code>__syncthreads()</code>，会导致无法预料的严重情况。如下面的代码，因块中的所有线程都没有达到相同的障碍点，会直接导致内核死锁</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (threadID % <span class="number">2</span> == <span class="number">0</span>) &#123;</span><br><span class="line">__syncthreads();</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">__syncthreads();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但是<code>__syncthreads()</code>的局限就在于只能解决一个块内的线程同步，不能跨线程同步，线程块会以任何顺序，并行或串行地在任何 SM 上执行，线程块这种独立的特性使得 CUDA 在任意数量的核心中都是可扩展的，如果一个 CUDA 核函数要求线程块全局同步，那么只能结束核函数的运行来隐式的同步线程块</p><h4 id="内存栅栏"><a href="#内存栅栏" class="headerlink" title="内存栅栏"></a>内存栅栏</h4><p>内存栅栏能保证栅栏前的内核内存写操作对栅栏后的其他线程都是可见的，根据所需范围有以下三种栅栏</p><ul><li><p>线程块，在 block 内创建内存栅栏，阻塞线程束直至阻塞线程束发出的写操作完成，但由于阻塞线程束本身就是单指令多线程，该指令就没什么用</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  <span class="type">void</span> __threadfence_block();</span><br><span class="line"></span><br><span class="line">* 网格，在 grid 内创建内存栅栏，阻塞 grid 直至 grid 内的线程发出的读写操作完成，可以实现块间同步</span><br><span class="line"></span><br><span class="line">  ```cpp</span><br><span class="line">  <span class="type">void</span> __threadfence();</span><br></pre></td></tr></table></figure></li><li><p>系统，可以跨系统创建内存栅栏，挂起调用的线程，以确保该线程对全局内存、锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程是可见的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __threadfence_system();</span><br></pre></td></tr></table></figure></li></ul><h4 id="Volatile-修饰符"><a href="#Volatile-修饰符" class="headerlink" title="Volatile 修饰符"></a>Volatile 修饰符</h4><p>在全局或共享内存中使用 volatile 修饰符声明一个变量，阻止编译器优化，可以防止这个变量存入缓存，这个变量的任何引用都会直接被编译到全局内存中，忽略缓存。举例如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">volatile</span> <span class="type">float</span> vfloat;</span><br></pre></td></tr></table></figure><h2 id="共享内存的数据布局"><a href="#共享内存的数据布局" class="headerlink" title="共享内存的数据布局"></a>共享内存的数据布局</h2><p>这部分我们通过研究如何组织共享内存的数据布局，以达到更少的 bank conflict 和最佳的性能</p><h3 id="方形共享内存"><a href="#方形共享内存" class="headerlink" title="方形共享内存"></a>方形共享内存</h3><p>SMEM 可以直接缓存方形维度的全局数据，如下图所示，字节地址与存储体地址的逻辑映射图，在每个维度假设有 32 个元素，且按行主序进行存储</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/12.png" alt=""></p><p>如下静态声明一个二维共享内存变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> N 32</span></span><br><span class="line">__shared__ <span class="type">int</span> a[N][N];</span><br></pre></td></tr></table></figure><p>我们可以用两种方式访问其中一个元素</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 行主序</span></span><br><span class="line">a[threadIdx.y][threadIdx.x];</span><br><span class="line"></span><br><span class="line"><span class="comment">//列主序</span></span><br><span class="line">a[threadIdx.y][threadIdx.x];</span><br></pre></td></tr></table></figure><p>行主序和列主序哪个效率更高，这取决于线程与共享内存存储体的映射关系。在一个线程束中的线程由连续的<code>threadIdx.x</code>来确定，也就是说，<code>threadIdx.y</code>对应上图中的 Row 行，<code>threadIdx.x</code>对应上图中的 Bank 列，而每个 bank 和线程束中的每个线程对应，邻近线程在最内层数组维度上访问相邻的阵列单元。因此，相比列主序，行主序有更好的性能和更少的 bank conflict</p><h4 id="行主序读写和列主序读写对比"><a href="#行主序读写和列主序读写对比" class="headerlink" title="行主序读写和列主序读写对比"></a>行主序读写和列主序读写对比</h4><p>下面的程序将全局线程索引值存入二维共享内存，再从共享内存中读取这些值并存到全局内存中，对比行主序和列主序</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_X 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_Y 32</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadRow</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setColReadCol</span><span class="params">(<span class="type">int</span> * out)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.x][threadIdx.y]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaSharedMemConfig</span> sharedMemConfig;</span><br><span class="line">    <span class="built_in">cudaDeviceGetSharedMemConfig</span>(&amp;sharedMemConfig);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, sharedMemConfig);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> nElem = DIM_X * DIM_Y;</span><br><span class="line">    <span class="type">int</span> nByte = <span class="built_in">sizeof</span>(<span class="type">int</span>)*nElem;</span><br><span class="line">    <span class="type">int</span> * out;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">int</span>**)&amp;out,nByte);</span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(DIM_Y, DIM_X)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    setRowReadRow&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out);</span><br><span class="line">    setColReadCol&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 4 字节访问模式下，因为相邻线程引用相邻字，可以看到按行访问使用的时间比列访问少了很多</p><p><img src="/image/CUDA编程-CUDA内存管理（二）/13.png" alt=""></p><p>使用 <code>ncu --metrics l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum</code> 命令可以获取核函数运行阶段的共享内存加载事务数，<code>ncu --metrics l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum</code>命令可以获取共享内存存储事务数，关于<code>ncu --metrics</code>更多参数的具体解释可以看看<a href="https://aeeeeeep.top/2023/01/07/CUDA%E7%BC%96%E7%A8%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7-metrics%E5%8F%82%E6%95%B0%E5%90%AB%E4%B9%89/">这篇博客</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadRow</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">32</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">32</span></span><br><span class="line"><span class="built_in">setColReadCol</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">1024</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">1024</span></span><br></pre></td></tr></table></figure><p>在对共享内存存储事务数/执行共享内存的访问次数，也就是二维共享内存的行<code>DIM_Y</code>，这里为 32，得到每次访问共享内存时的存储事务数 32/32 = 1，不会产生 bank conflict。但是在<code>setColReadCol</code>中每次访问共享内存时的存储事务数为 1024/32 = 32，会有 32 路  bank conflict，对应<code>DIM_Y=32</code>，就是因为<code>setRowReadRow</code>是邻近线程在最内层数组维度上访问相邻的阵列单元</p><p>下面的核函数为按行主序写和按列主序读</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadCol</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_X][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setColReadRow</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.x][threadIdx.y]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行上面两个核函数，可以看到冲突情况符合我们的理论，即邻近线程在最内层数组维度上访问相邻的阵列单元会减少冲突</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadCol</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">1024</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">32</span></span><br><span class="line"><span class="built_in">setColReadRow</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">32</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">1024</span></span><br></pre></td></tr></table></figure><p>设矩阵 size(4,4) 执行上面的四个核函数，输出<code>out</code>的值可以看到<code>setRowReadCol</code>和<code>setColReadRow</code>会对数组转置，这为之后我们将会讲到的转置算法作了基础</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_X 4</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_Y 4</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadRow</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setColReadCol</span><span class="params">(<span class="type">int</span> * out)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.x][threadIdx.y]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadCol</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_X][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setColReadRow</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.x][threadIdx.y]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">enum</span> <span class="title class_">cudaSharedMemConfig</span> sharedMemConfig;</span><br><span class="line">    <span class="built_in">cudaDeviceGetSharedMemConfig</span>(&amp;sharedMemConfig);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, sharedMemConfig);</span><br><span class="line">    </span><br><span class="line">    <span class="type">int</span> nElem = DIM_X * DIM_Y;</span><br><span class="line">    <span class="type">int</span> nByte = <span class="built_in">sizeof</span>(<span class="type">int</span>)*nElem;</span><br><span class="line">    <span class="type">int</span> * out_h, * out_d;</span><br><span class="line">    out_h = (<span class="type">int</span>*)<span class="built_in">malloc</span>(nByte);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">int</span>**)&amp;out_d,nByte);</span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(DIM_Y, DIM_X)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;setRowReadRow: &quot;</span>);</span><br><span class="line">    setRowReadRow&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out_d);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(out_h, out_d, nByte, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, out_h[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;setColReadCol: &quot;</span>);</span><br><span class="line">    setColReadCol&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out_d);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(out_h, out_d, nByte, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, out_h[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;setRowReadCol: &quot;</span>);</span><br><span class="line">    setRowReadCol&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out_d);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(out_h, out_d, nByte, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, out_h[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;setColReadRow: &quot;</span>);</span><br><span class="line">    setColReadRow&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out_d);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(out_h, out_d, nByte, cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, out_h[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">setRowReadRow: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> </span><br><span class="line">setColReadCol: <span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">8</span> <span class="number">9</span> <span class="number">10</span> <span class="number">11</span> <span class="number">12</span> <span class="number">13</span> <span class="number">14</span> <span class="number">15</span> </span><br><span class="line">setRowReadCol: <span class="number">0</span> <span class="number">4</span> <span class="number">8</span> <span class="number">12</span> <span class="number">1</span> <span class="number">5</span> <span class="number">9</span> <span class="number">13</span> <span class="number">2</span> <span class="number">6</span> <span class="number">10</span> <span class="number">14</span> <span class="number">3</span> <span class="number">7</span> <span class="number">11</span> <span class="number">15</span> </span><br><span class="line">setColReadRow: <span class="number">0</span> <span class="number">4</span> <span class="number">8</span> <span class="number">12</span> <span class="number">1</span> <span class="number">5</span> <span class="number">9</span> <span class="number">13</span> <span class="number">2</span> <span class="number">6</span> <span class="number">10</span> <span class="number">14</span> <span class="number">3</span> <span class="number">7</span> <span class="number">11</span> <span class="number">15</span></span><br></pre></td></tr></table></figure><p>下面给出动态声明版本，前面提到过，动态共享内存数组只能是一维的，且要将所需大小按字节数作为核函数三重括号内的第三个参数</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColDyn</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> tile[];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> row_idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> col_idx=threadIdx.x*blockDim.y+threadIdx.y;</span><br><span class="line"></span><br><span class="line">    tile[row_idx]=row_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[row_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setRowReadColDyn&lt;&lt;&lt;grid, block, <span class="function">DIM_X * DIM_Y * <span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(out)</span></span>;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadColDyn</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">1024</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">32</span></span><br></pre></td></tr></table></figure><h4 id="memory-padding"><a href="#memory-padding" class="headerlink" title="memory padding"></a>memory padding</h4><p>为了解决<code>setRowReadCol</code>，<code>setColReadRow</code>核函数的 bank conflict，我们要根据具体的数据分布来填充内存，在静态声明中，只需要将填充的列添加到二维共享内存分配中就可以了，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> PAD 1</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColPad</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y][DIM_X+PAD];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.y][threadIdx.x];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setRowReadColPad&lt;&lt;&lt;grid,block&gt;&gt;&gt;(out);</span><br></pre></td></tr></table></figure><p>在动态声明中，由于需要执行二维线程索引到一维线程索引的转换，所以对于每一行，都要跳过填充的部分，代码如下</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> PAD 1</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColDynPad</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> tile[];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> row_idx=threadIdx.y* (blockDim.x + PAD) +threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> col_idx=threadIdx.x* (blockDim.y + PAD) +threadIdx.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> g_idx=threadIdx.y*blockDim.x +threadIdx.x;</span><br><span class="line"></span><br><span class="line">    tile[row_idx]=g_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[g_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">setRowReadColDynPad&lt;&lt;&lt;grid, block, (DIM_X + PAD) * <span class="function">DIM_Y * <span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(out)</span></span>;</span><br></pre></td></tr></table></figure><p>使用 ncu 工具可以看到每次访问共享内存请求的事务数量为 1，无 bank conflict</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadColPad</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">32</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">32</span></span><br><span class="line"><span class="built_in">setRowReadColDynPad</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">32</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">32</span></span><br></pre></td></tr></table></figure><h3 id="矩形共享内存"><a href="#矩形共享内存" class="headerlink" title="矩形共享内存"></a>矩形共享内存</h3><p>矩形共享内存和方形共享内存非常相似，不同的地方在于线程索引的要先映射为一维，保证访问是合并的</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br></pre></td></tr></table></figure><p>再通过二维方式访问，这里对原矩阵按列读取，irow 和 icol 对应的是转置后矩阵中的坐标</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> icol=idx%blockDim.y;</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> irow=idx/blockDim.y;</span><br></pre></td></tr></table></figure><p>下面的程序将全局线程索引值存入二维共享内存，再从共享内存中读取这些值并存到全局内存中，对比行主序和列主序</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_X_RECT 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_Y_RECT 16</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColRect</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y_RECT][DIM_X_RECT];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> icol=idx%blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> irow=idx/blockDim.y;</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[icol][irow];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 ncu 工具可以看到加载操作有 256/16 = 16 路冲突，而存储操作没有冲突</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadColRect</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">256</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">16</span></span><br></pre></td></tr></table></figure><p>为了解决 bank conflict，下面给出 memory padding 版本的核函数，这里的<code>PAD_RECT=2</code>是因为将长方形矩阵一行有 16 个元素，为了满足 32 个存储体的数量，每次会访问两行的数据，所以要填充对应两行的 2 个元素以错开访问，而方形矩阵一行有 32 个元素，匹配 32 个存储体的数量，每行只需要填充一个数据即可错开访问</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> PAD_RECT 2</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColRectPad</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    __shared__ <span class="type">int</span> tile[DIM_Y_RECT][DIM_X_RECT+PAD_RECT];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> icol=idx%blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> irow=idx/blockDim.y;</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[icol][irow];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadColRectPad</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">16</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">16</span></span><br></pre></td></tr></table></figure><p>下面再给出动态声明和 memory padding 版本的核函数供参考</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_X_RECT 32</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> DIM_Y_RECT 16</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> PAD_RECT 2</span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColRectDyn</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> tile[];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> icol=idx%blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> irow=idx/blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> col_idx=icol*blockDim.x+irow;</span><br><span class="line">    tile[idx]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">setRowReadColRectDynPad</span><span class="params">(<span class="type">int</span> * out)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">extern</span> __shared__ <span class="type">int</span> tile[];</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> icol=idx%blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> irow=idx/blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> row_idx=threadIdx.y*(PAD_RECT+blockDim.x)+threadIdx.x;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> col_idx=icol*(PAD_RECT+blockDim.x)+irow;</span><br><span class="line">    tile[row_idx]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">dim3 <span class="title">block_rect</span><span class="params">(DIM_X_RECT,DIM_Y_RECT)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid_rect</span><span class="params">(<span class="number">1</span>,<span class="number">1</span>)</span></span>;</span><br><span class="line">setRowReadColRectDyn&lt;&lt;&lt;grid_rect,block_rect, <span class="function">DIM_X_RECT * DIM_Y_RECT*<span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(out)</span></span>;</span><br><span class="line">setRowReadColRectDynPad&lt;&lt;&lt;grid_rect,block_rect, (DIM_X_RECT+PAD_RECT) * <span class="function">DIM_Y_RECT*<span class="title">sizeof</span><span class="params">(<span class="type">int</span>)</span>&gt;&gt;&gt;<span class="params">(out)</span></span>;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">setRowReadColRectDyn</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">256</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">16</span></span><br><span class="line"><span class="built_in">setRowReadColRectDynPad</span>(<span class="type">int</span>*)</span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum<span class="number">16</span></span><br><span class="line">l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum<span class="number">16</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA内存管理（一）</title>
      <link href="/2024/01/09/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2024/01/09/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>各种内存的对齐访问以及实验，避免带宽浪费。还对比了 AoS 与 SoA 结构体在 GPU 上的性能表现。</p><span id="more"></span><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h3><p>分配的主机内存默认是可分页的（pageable），操作系统会分配给程序一个很大的虚拟内存，但实际的物理内存会小很多，为了让程序正常运行，操作系统对物理内存以页为单位组织，将数据存储到不同的页中，这些页是不连续的，程序只可以看到虚拟内存地址，而操作系统可能随时更换数据的在物理内存中的页，但是在使用 GPU 时，如果从主机传输到设备上的时候，页被更改了，对于传输数据而言是致命的，所以在传输之前，CUDA 驱动会锁定页面，或者直接分配固定的主机内存，将主机数据复制到固定内存上，再从固定内存传输数据到设备上，如下图左边所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/1.png" alt=""></p><p>CUDA 运行时可以使用以下指令直接分配固定主机内存，会使传输带宽高很多，如上图右边所示</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMallocHost</span> <span class="params">( <span class="type">void</span>** ptr, <span class="type">size_t</span> size )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>ptr</code>: 指针，指向要分配的内存的位置</li><li><code>size</code>: 要分配的内存大小</li></ul><p>分配后的内存必须使用下面的命令释放</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaFreeHost</span> <span class="params">( <span class="type">void</span>* ptr )</span></span></span><br></pre></td></tr></table></figure><p>我们对第三章中的矩阵相加代码作修改，对比固定内存和分页内存的传输效率，也就是对比<code>cudaMalloc</code>和<code>cudaMallocHost</code>函数所开辟内存空间的传输效率，下面是<code>cudaMalloc</code>版本</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">   <span class="type">int</span> w;</span><br><span class="line">   <span class="type">int</span> h;</span><br><span class="line">   <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">   A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    Matrix *A = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    Matrix *B = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    Matrix *C = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    A-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    B-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    C-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">        C-&gt;v[i] = <span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Matrix *A_d, *B_d, *C_d;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *B_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *C_d_v = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d_v, A-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d_v, B-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d_v, C-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_d_v_t = A-&gt;v;</span><br><span class="line">    <span class="type">float</span> *B_d_v_t = B-&gt;v;</span><br><span class="line">    <span class="type">float</span> *C_d_v_t = C-&gt;v;</span><br><span class="line"></span><br><span class="line">    A-&gt;v = A_d_v;</span><br><span class="line">    B-&gt;v = B_d_v;</span><br><span class="line">    C-&gt;v = C_d_v;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d, A, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d, B, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d, C, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    A-&gt;v = A_d_v_t;</span><br><span class="line">    B-&gt;v = B_d_v_t;</span><br><span class="line">    C-&gt;v = C_d_v_t;</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A_d, B_d, C_d);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是<code>cudaMallocHost</code>版本</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">   <span class="type">int</span> w;</span><br><span class="line">   <span class="type">int</span> h;</span><br><span class="line">   <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">   A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;C-&gt;v, nBytes);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Matrix *A_d, *B_d, *C_d;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *B_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *C_d_v = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d_v, A-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d_v, B-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d_v, C-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_d_v_t = A-&gt;v;</span><br><span class="line">    <span class="type">float</span> *B_d_v_t = B-&gt;v;</span><br><span class="line">    <span class="type">float</span> *C_d_v_t = C-&gt;v;</span><br><span class="line"></span><br><span class="line">    A-&gt;v = A_d_v;</span><br><span class="line">    B-&gt;v = B_d_v;</span><br><span class="line">    C-&gt;v = C_d_v;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d, A, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d, B, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d, C, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    A-&gt;v = A_d_v_t;</span><br><span class="line">    B-&gt;v = B_d_v_t;</span><br><span class="line">    C-&gt;v = C_d_v_t;</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A_d, B_d, C_d);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>nsys nvprof &#123;&#125;.o</code>得到两个版本的数据传输时间</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cudaMalloc 版本</span></span><br><span class="line"> <span class="title class_">Time</span> (%)  <span class="title class_">Total</span> <span class="title class_">Time</span> (ns)  <span class="title class_">Count</span>  <span class="title class_">Avg</span> (ns)  <span class="title class_">Med</span> (ns)  <span class="title class_">Min</span> (ns)  <span class="title class_">Max</span> (ns)  <span class="title class_">StdDev</span> (ns)      <span class="title class_">Operation</span>     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">    <span class="number">100.0</span>             <span class="number">6817</span>      <span class="number">6</span>    <span class="number">1136.2</span>     <span class="number">992.5</span>       <span class="number">960</span>      <span class="number">1536</span>        <span class="number">243.4</span>  [<span class="variable constant_">CUDA</span> memcpy <span class="title class_">HtoD</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// cudaMallocHost 版本</span></span><br><span class="line"> <span class="title class_">Time</span> (%)  <span class="title class_">Total</span> <span class="title class_">Time</span> (ns)  <span class="title class_">Count</span>  <span class="title class_">Avg</span> (ns)  <span class="title class_">Med</span> (ns)  <span class="title class_">Min</span> (ns)  <span class="title class_">Max</span> (ns)  <span class="title class_">StdDev</span> (ns)      <span class="title class_">Operation</span>     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">    <span class="number">100.0</span>             <span class="number">6304</span>      <span class="number">6</span>    <span class="number">1050.7</span>     <span class="number">992.0</span>       <span class="number">928</span>      <span class="number">1344</span>        <span class="number">153.4</span>  [<span class="variable constant_">CUDA</span> memcpy <span class="title class_">HtoD</span>]</span><br></pre></td></tr></table></figure><p>可以看到使用<code>cudaMallocHost</code>函数相比<code>cudaMalloc</code>，因其固定内存的特性，传输效率高了一些</p><blockquote><p>上面程序中用<code>cudaMalloc</code>将结构体从主机传输到设备的方法十分繁琐且已经过时了，在这里只作测试，建议使用第三章中提到的<code>cudaMallocManaged</code>函数，后续会详细介绍该函数</p></blockquote><h3 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h3><p>在 GPU 编程中，主机变量和设备变量之间一般不能直接相互访问，但主机变量和设备变量都可以访问零拷贝内存。GPU线程可以直接访问主机里的零拷贝内存，当有以下几种情况时核函数会使用零拷贝内存</p><ul><li>当设备内存不足时</li><li>避免主机和设备之间的显式内存传输</li><li>为了提高PCIe传输率</li></ul><p>当使用零拷贝内存时要同步设备和主机间的内存访问，避免内存竞争。零拷贝内存是固定内存，不可分页。使用以下函数可以创建零拷贝内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaHostAlloc</span> <span class="params">( <span class="type">void</span>** pHost, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span>  flags )</span></span></span><br></pre></td></tr></table></figure><ul><li><p><code>pHost</code>: 指向将被分配的内存地址</p></li><li><p><code>size</code>: 要分配的内存块的大小</p></li><li><p><code>flags</code>: 用于控制函数行为，可选如下</p><ul><li><p><code>cudaHostAllocDefault</code>: 默认，和<code>cudaMallocHost</code>一致，使用系统的默认内存类型分配内存，</p></li><li><p><code>cudaHostAllocPortable</code>: 分配固定内存，可以被所有 CUDA 上下文使用</p></li><li><p><code>cudaHostAllocMapped</code>: 分配零拷贝内存，将内存分配映射到设备的地址空间，必须使用以下函数得到指向主机上零拷贝内存的设备指针 <code>pDevice</code>，设备才能访问零拷贝内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>pDevice</code>: 访问主机零拷贝内存的设备指针</li><li><code>pHost</code>: 同上<code>pHost</code></li><li><code>flags</code>: 此处必须置0，后续介绍原因</li></ul></li><li><p><code>cudaHostAllocWriteCombined</code>: 将内存分配为写组合（Write-combined memory），可以在某些系统配置上更快地通过 PCIe 传输，提高设备写入内存的性能</p></li></ul></li></ul><p>主机上固定内存的释放</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaFreeHost</span> <span class="params">( <span class="type">void</span>* ptr )</span></span></span><br></pre></td></tr></table></figure><p>下面的程序计算了向量乘法，对比了<code>cudaMallocHost</code>和<code>cudaHostAlloc</code>的传输效率</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_hp,*B_hp;</span><br><span class="line">    <span class="type">float</span> *A_dp,*B_dp,*C_dp;</span><br><span class="line">    A_hp = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    B_hp = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(A_hp,<span class="number">1</span>,nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(B_hp,<span class="number">2</span>,nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;A_dp,nBytes);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;B_dp,nBytes);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;C_dp,nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_dp,A_hp,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_dp,B_hp,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_hm, *B_hm, *C_hm;</span><br><span class="line">    <span class="type">float</span> *A_dm, *B_dm, *C_dm;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;A_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;B_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;C_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A_hm[i]=<span class="number">1.0</span>;</span><br><span class="line">        B_hm[i]=<span class="number">2.0</span>;</span><br><span class="line">        C_hm[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;A_dm, (<span class="type">void</span>*)A_hm, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;B_dm, (<span class="type">void</span>*)B_hm, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;C_dm, (<span class="type">void</span>*)C_hm, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_dp, B_dp, C_dp);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;cudaMallocHost: %f ms\n&quot;</span>,  elapsedTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_dm, B_dm, C_dm);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;cudaHostAlloc: %f ms\n&quot;</span>,  elapsedTime);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(A_hp);</span><br><span class="line">    <span class="built_in">free</span>(B_hp);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(A_hm);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(B_hm);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(C_hm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(A_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(A_dm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B_dm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C_dm);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">cudaMallocHost: <span class="number">0.050720</span> ms</span><br><span class="line">cudaHostAlloc: <span class="number">1.258496</span> ms</span><br></pre></td></tr></table></figure><p>可以看到零拷贝内存的内核函数执行效率会大大降低，因为其数据的传输是在执行内核函数时，这种执行方式不适合离散架构，如我们平时用的通过 PCIe 总线传输数据的独显 + CPU 架构，而非常适合在 Nvidia 集成架构的设备上使用，也就是共享物理内存的架构</p><h3 id="统一虚拟寻址"><a href="#统一虚拟寻址" class="headerlink" title="统一虚拟寻址"></a>统一虚拟寻址</h3><p>统一虚拟寻址（UVA）是 Nvidia 在计算能力 2.0 之后的设备支持的特殊寻址方式，设备内存和主机内存被映射到统一虚拟内存地址中，共享一个虚拟地址空间，如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/2.png" alt=""></p><p>在 UVA 出来之前，代码中的指针变量需要区分指向主机内存还是设备内存，通过UVA，可以通过<code>cudaHostAlloc</code> 函数分配固定主机内存，指针指向相同的主机和设备地址，可以直接将指针传递给核函数，相当于省略了零拷贝内存的<code>cudaHostGetDevicePointer</code>步骤</p><p>删除前面的零拷贝内存代码中<code>cudaHostGetDevicePointer</code>函数和设备指针部分，如下代码就使用了 UVA 的寻址方式</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_h, *B_h, *C_h;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;A_h, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;B_h, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;C_h, nBytes, cudaHostAllocMapped);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A_h[i]=<span class="number">1.0</span>;</span><br><span class="line">        B_h[i]=<span class="number">2.0</span>;</span><br><span class="line">        C_h[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_h, B_h, C_h);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(A_h);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(B_h);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(C_h);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统一内存寻址"><a href="#统一内存寻址" class="headerlink" title="统一内存寻址"></a>统一内存寻址</h3><p>在 CUDA 6.0 中，又引入了统一内存寻址的特性，用于简化内存管理。统一内存中创建了一个托管内存池，内存池中已分配的空间可以用相同的指针在 CPU 和 GPU 上进行访问。底层系统在统一的内存空间中自动的进行设备和主机间的传输</p><p>统一内存寻址依赖于前面提到的 UVA，不同之处在于</p><ul><li>统一内存寻址提供了一个“单指针到数据”的编程模型，通过底层系统进行统一内存管理，被称为托管内存，自己分配的内存称为未托管内存，两种类型的内存可以同时传递给核函数</li><li>UVA 的分配是先在主机上完成，在核函数运行前才传输数据给设备</li></ul><p>托管内存可以被静态分配或动态分配，在定义设备变量时添加 <code>__managed__</code> 关键字修饰静态托管内存变量，如下代码所示，静态声明的托管内存作用域是文件，该变量可以在主机或设备代码中直接引用</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__managed__ <span class="type">float</span> x;</span><br></pre></td></tr></table></figure><p>第三章使用过的<code>cudaMallocManaged</code>函数就是动态分配托管内存变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMallocManaged</span> <span class="params">( <span class="type">void</span>** devPtr, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span>  flags = cudaMemAttachGlobal )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>devPtr</code>: 开辟数据的首指针</li><li><code>size</code>: 开辟的设备内存空间长度</li><li><code>flags</code>: 默认为<code>cudaMemAttachGlobal</code><ul><li><code>cudaMemAttachGlobal</code>: 开辟的内存可以被任何设备上的任何流访问，流的概念将在下一章节介绍</li><li><code>cudaMemAttachHost</code>: 开辟的内存不能被任何设备上的任何流访问</li></ul></li></ul><p>使用<code>cudaMallocManaged</code>函数动态声明数组，分配托管内存变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="内存访问模式"><a href="#内存访问模式" class="headerlink" title="内存访问模式"></a>内存访问模式</h2><p>多数 GPU 程序容易受到内存带宽的限制，所以最大程度的利用全局内存带宽，提高全局加载效率，是调控内核函数性能的基本条件。如果不能正确管理全局内存，那么优化方案可能也收效甚微<br>在 CUDA 执行模型中得知 CUDA 执行的基本单位是线程束，所以内存访问和数据存储也是以线程束为基本单位发布和执行的，在线程束的 32 个线程中，每个线程都会提出一个包含请求地址的单一内存访问请求，根据线程束中内存地址的分布，内存访问会被分成不同的模式，下面将介绍这些不同的内存访问模式，并学习实现最佳的全局内存访问</p><h3 id="对齐与合并访问"><a href="#对齐与合并访问" class="headerlink" title="对齐与合并访问"></a>对齐与合并访问</h3><p>全局内存（DRAM）是一个逻辑内存空间，可以通过核函数访问它，所有的程序数据都是储在物理设备内存上，也就是 DRAM 设备上，核函数的内存请求通常是在 DRAM 设备和片上内存间上以 32 字节或 128 字节粒度的内存事务来实现，内存事务就是从内核函数发起请求，到硬件响应返回数据过程</p><p>所有对全局内存的访问都会通过二级缓存，也有许多访问会通过一级缓存，这取决于访问类型和 GPU 架构</p><p>称 L1 为一级缓存，L2 为二级缓存，如下图所示，每个 SM 都有自己 L1，但是 L2 是所有 SM 公用的，核函数运行时需要从 DRAM 中读取数据，读取时如果使用 L1 则从 DRAM 上一次加载的数据是 128 字节，如果不使用 L1 则从 DRAM 上一次加载的数据是 32 字节</p><p>一行一级缓存是 128 字节，映射到设备内存中一个 128 字节的对齐段，如果线程束中的每个线程请求一个 4 字节的值，那么每次请求都会获取 4 x 32 = 128 字节的数据，这恰好与缓存行和设备内存段的大小相契合，因此我们要注意设备内存访问的：</p><ul><li>对齐内存访问</li><li>合并内存访问</li></ul><p><img src="/image/CUDA编程-CUDA内存管理（一）/3.png" alt=""></p><p>当设备内存事务的第一个地址是用于事务服务的缓存粒度的偶数倍数时（32或128字节），称为对齐内存访问，当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问。对齐与合并访问的情况如下图所示，只需要 128 字节的内存事务从设备内存中读取数据</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/4.png" alt=""></p><p>但如果一个内存事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况</p><ul><li>内存地址连续，但不在一个对齐的段上，如请求访问的数据分布在内存地址 1-128，那么 0-127 和 128-255 这两段数据要传递两次到 SM</li><li>内存地址不连续，也不在一个对齐的段上，如请求访问的数据分布在内存地址 0-63 和 128-191 上，那么这两段数据也要传递两次</li></ul><p>上述两种情况会使内存事务获取的大部分字节不能使用，造成带宽的浪费，如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/5.png" alt=""></p><p>我们需要优化内存事务的效率，提高吞吐量</p><h3 id="全局内存读取"><a href="#全局内存读取" class="headerlink" title="全局内存读取"></a>全局内存读取</h3><p>SM 中的数据根据不同的设备和类型以 3 种不同的路径进行传输</p><ul><li>L1 和 L2</li><li>常量缓存</li><li>只读缓存</li></ul><p>默认路径是 L1 和 L2，需要使用常量和只读缓存的需要在代码中显式声明。但是提高性能还是要取决于访问模式，全局加载操作是否通过 L1 可以通过编译选项来控制，在计算能力为 2.X 和 3.5 以上的 GPU 中，可以通过以下编译器标识符禁用 L1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=cg</span><br></pre></td></tr></table></figure><p>通过以下编译器标识符启用 L1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=ca</span><br></pre></td></tr></table></figure><p>当 SM 有全局加载请求会首先尝试通过 L1，如果 L1 被禁用，请求转向 L2，如果 L2 缺失，则由 DRAM 完成请求，在这种情况下，内存加载请求由 128 字节的设备内存事务实现</p><blockquote><p>Kepler K10, K20, K20X GPU 中，L1 不用来缓存全局内存访问，只用来存储寄存器溢出的本地数据</p></blockquote><h4 id="缓存加载"><a href="#缓存加载" class="headerlink" title="缓存加载"></a>缓存加载</h4><p>缓存加载是指经过 L1，在粒度为 128 字节的 L1 上由设备内存事务进行传输。缓存加载可以分为对齐/非对齐但合并/非合并</p><p>下图所示为对齐合并的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/6.png" alt=""></p><p>对齐但不连续的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/7.png" alt=""></p><p>非对齐但连续的情况，因为线程束请求的 32 个连续的 4 字节元素在两个 128 字节段内，所以利用率为 50%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/8.png" alt=""></p><p>线程束中所有线程请求同一个地址的情况，利用率为 4/128 = 3.125%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/9.png" alt=""></p><p>最坏的情况，每个线程束内的线程请求的都是不同的缓存行内，比较坏的情况就是所有数据分布在 N 个缓存行上，其中 1≤N≤32，请求 32 个 4 字节的数据就需要 N 个事务来完成，利用率为 1/N</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/10.png" alt=""></p><blockquote><p>CPU 和 GPU 的 L1 有明显的差异，CPU 的 L1 优化了时间和空间局部性，GPU 的 L1 是专为空间局部性设计的，频繁访问 L1 中内存位置不会增加数据留存缓存中的概率</p></blockquote><h4 id="没有缓存的加载"><a href="#没有缓存的加载" class="headerlink" title="没有缓存的加载"></a>没有缓存的加载</h4><p>没有缓存的加载是指不经过 L1，只经过 L2，在粒度为 32 字节的 L2 上由设备内存事务进行传输，更细的粒度代表更高的利用率</p><p>下图所示为对齐合并的情况，128 字节请求的地址占用了 4 个内存段，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/11.png" alt=""></p><p>对齐但不连续的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/12.png" alt=""></p><p>非对齐但连续的情况，因为线程束请求的 32 个连续的 4 字节元素但加载没有对齐到 128 个字节的边界，请求的地址最多落在 5 个内存段内，所以利用率为 4/5 = 80%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/13.png" alt=""></p><p>线程束中所有线程请求同一个地址的情况，利用率为 4/32 = 12.5%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/14.png" alt=""></p><p>最坏的情况，每个线程束内的线程请求的都是不同的缓存行内，由于请求的 128 个字节最多落在 N 个 32 字节的内存段内而不是 N 个 128 字节的缓存行内，所以相比缓存加载，即使是最坏的情况也有所改善</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/15.png" alt=""></p><h4 id="非对齐读取示例"><a href="#非对齐读取示例" class="headerlink" title="非对齐读取示例"></a>非对齐读取示例</h4><p>对之前的<code>ArraySum</code>核函数代码执行加上偏移量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> offset, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">int</span> k = i+offset;</span><br><span class="line">    <span class="keyword">if</span>(k &lt; n)</span><br><span class="line">        c[i] = a[k]+b[k];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> offset=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">        offset = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, offset, nElem);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed: %f ms offset: %d \n&quot;</span>, grid.x, block.x, elapsedTime, offset);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们这里使用 Nsight Compute 交互式内核分析工具，一般安装完 CUDA Toolkit 会在安装目录中，我们只需要添加环境变量即可，如没有请在<a href="https://developer.nvidia.com/gameworksdownload#?dn=nsight-compute-2022-4-0">官网下载</a>，详细的使用方法请查看<a href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">官网文档</a></p><blockquote><p>注意类似 Docker 之类的虚拟机用户（如租赁的 GPU）没有权限使用 Nsight Compute 访问 GPU，需要在物理机上以管理员权限使用</p></blockquote><p>编译命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=cg &#123;&#125;.cu -o &#123;&#125;_cg.out<span class="comment"># 禁用 L1</span></span><br><span class="line">nvcc -Xptxas -dlcm=ca &#123;&#125;.cu -o &#123;&#125;_ca.out<span class="comment"># 启用 L1</span></span><br></pre></td></tr></table></figure><p>测试不同的偏移量获得的全局加载效率</p><blockquote><p>全局加载效率 = 请求的全局内存加载吞吐量 / 所需的全局内存加载吞吐量</p></blockquote><p>启用 L1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 0</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 11</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 128</span><br></pre></td></tr></table></figure><ul><li>offset=0 :    gld_efficiency    100%</li><li>offset=11 :    gld_efficiency    40%</li><li>offset=128 :    gld_efficiency    100%</li></ul><p>可以看出偏移量会直接导致性能损失</p><p>禁用 L1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 0</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 11</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 128</span><br></pre></td></tr></table></figure><ul><li>offset=0 :    gld_efficiency    100%</li><li>offset=11 :    gld_efficiency    80%</li><li>offset=128 :    gld_efficiency    100%</li></ul><p>禁用 L1 后同样偏移 11 的全局加载效率提高了不多，也验证了上面提到的更细的词粒度会带来更好的性能</p><h4 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h4><p>只读缓存最初是留给纹理内存加载用的，在 计算能力 3.5 以上的设备，只读缓存也支持使用全局内存加载代替一级缓存</p><p>只读缓存粒度为 32 字节，对于分散读取，细粒度优于一级缓存，有两种方法让内存从只读缓存读取</p><ul><li>使用函数<code>__ldg</code></li><li>在间接引用的指针上使用修饰符</li></ul><p>考虑如下代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>__ldg</code>通过只读缓存对数组进行读取访问</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]= __ldg(&amp;a[i]) + __ldg(&amp;b[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以常量<code>__restrict__</code>修饰符应用到指针上，nvcc 将自动通过只读缓存指导无别名指针的加载</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* __restrict__ a, <span class="type">const</span> <span class="type">float</span>* __restrict__ b, <span class="type">float</span>* __restrict__ res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="全局内存写入"><a href="#全局内存写入" class="headerlink" title="全局内存写入"></a>全局内存写入</h3><p>内存的存储和加载是完全不同的，并且存储相对简单很多。存储操作在32个字节的粒度上被执行，内存事务也被分为一段、两端或者四段，例如两个地址在一个 128 字节的段内但不在对齐的 64 字节区域内，则会产生一个四段的事务，执行四段的事务比执行两个一段事务的效果更好</p><blockquote><p>Fermi 和 Kepler 架构的存储操作不经过 L1 ，只经过 L2</p></blockquote><p>如下图所示，内存访问是对齐的，访问一个连续的 128 字节范围，存储请求使用一个四段事务完成，这里最理想的情况</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/16.png" alt=""></p><p>数据分散在一个 192 字节的范围内，存储不连续，使用三个一段事务实现</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/17.png" alt=""></p><p>内存访问是对齐的，访问一个连续的 64 字节范围，存储请求使用一个两段事务完成</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/18.png" alt=""></p><h4 id="非对齐写入示例"><a href="#非对齐写入示例" class="headerlink" title="非对齐写入示例"></a>非对齐写入示例</h4><p>只需要对核函数作如下修改即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> offset, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">int</span> k = i+offset;</span><br><span class="line">    <span class="keyword">if</span>(k &lt; n)</span><br><span class="line">        c[k] = a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结论与非对齐读取示例相同</p><h3 id="结构体数组与数组结构体（SoA-和-AoS）"><a href="#结构体数组与数组结构体（SoA-和-AoS）" class="headerlink" title="结构体数组与数组结构体（SoA 和 AoS）"></a>结构体数组与数组结构体（SoA 和 AoS）</h3><p>在 C 语言中，结构体是一种强大的数据组织方式，结构体中的成员在内存里对齐的依次排开，但是我们保存一组数据有两种方式，可以定义如下结构体</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>再定义一个数组结构体（AoS）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> _<span class="title">Aos</span>[<span class="title">N</span>];</span></span><br></pre></td></tr></table></figure><p>也可以定义一个结构体数组（SoA）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> x[N];</span><br><span class="line">    <span class="type">float</span> y[N];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> _<span class="title">SoA</span>[<span class="title">N</span>];</span></span><br></pre></td></tr></table></figure><p>AoS 方式组织的数据在空间上是相邻的，这在 CPU 上会有良好的缓存局部性，但是在 GPU 的并行架构中，读写一个结构体字段 x 时会同时加载 x 和 y 两个字段，这就导致有 50 % 的带宽损失，再看 SoA，访问一个 SoA 布局的结构体时，由于没有交叉存储的字段，所以是合并内存访问，可以充分利用带宽，图示如下</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/19.png" alt=""></p><p>前面我们介绍的矩阵相乘中定义矩阵的结构体就是 SoA 方式，下面是 AoS 方式的代码，与 SoA 方式的代码作对比，只做测试用，无实际意义</p><blockquote><p>这里指出第三章中矩阵相乘代码的错误，w 和 h 设置为 <code>1 &lt;&lt; 20</code> 会导致数值溢出，这里改为<code>1 &lt;&lt; 12</code></p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="type">int</span> w;</span><br><span class="line">    <span class="type">int</span> h;</span><br><span class="line">    <span class="type">float</span> v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">__device__ <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> A[row * w + col].v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">    A[row * w + col].v = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMul</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> k = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i &lt; w; i++)</span><br><span class="line">        k += <span class="built_in">getValue</span>(A, row, i) * <span class="built_in">getValue</span>(B, i, col);</span><br><span class="line">    <span class="built_in">setValue</span>(C, row, col, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i &lt; w*h; i++) &#123;</span><br><span class="line">        A[i].w = w;</span><br><span class="line">        A[i].h = h;</span><br><span class="line">        B[i].w = w;</span><br><span class="line">        B[i].h = h;</span><br><span class="line">        C[i].w = w;</span><br><span class="line">        C[i].h = h;</span><br><span class="line">        A[i].v = <span class="number">1.0</span>;</span><br><span class="line">        B[i].v = <span class="number">2.0</span>;</span><br><span class="line">        C[i].v = <span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 nsys 对比主机到设备的数据传输</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># AoS 方式</span><br><span class="line">Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------</span><br><span class="line">    402.653   6951     0.058     0.016     0.004     1.040        0.143  [CUDA Unified Memory memcpy HtoD]</span><br><span class="line"></span><br><span class="line"># SoA 方式</span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------</span><br><span class="line">    134.283   2772     0.048     0.008     0.004     1.028        0.142  [CUDA Unified Memory memcpy HtoD]</span><br></pre></td></tr></table></figure><p>可以直观看到，AoS 对某个字段读写时会同时加载所有字段，<code>Matrix</code>结构体有三个字段，134 * 3 = 402，与数据显示一致</p><h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><p>优化设备内存带宽利用率有两个目标</p><ul><li>对齐合并内存访问，减少带宽浪费</li><li>足够的并发内存操作，降低内存延迟</li></ul><p>我们已经了解了如何组织内存访问以对内存对齐的内存访问，这可以在 DRAM 和 SM 片上内存或寄存器之间确保有效利用字节移动，实现内存访问最大化一般通过增加每个线程中执行独立内存操作的数量，以及对核函数启动的执行配置进行试验</p><h4 id="展开循环"><a href="#展开循环" class="headerlink" title="展开循环"></a>展开循环</h4><p>我们使用 8 循环展开技术对<code>ArraySum</code>核函数作修改</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x*<span class="number">4</span> +threadIdx.x;</span><br><span class="line">    c[i] = a[i]+b[i];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x &lt; n)</span><br><span class="line">        c[i+blockDim.x] = a[i+blockDim.x]+b[i+blockDim.x];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">2</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">2</span>] = a[i+blockDim.x *<span class="number">2</span>]+b[i+blockDim.x *<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">3</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">3</span>] = a[i+blockDim.x *<span class="number">3</span>]+b[i+blockDim.x *<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">4</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">4</span>] = a[i+blockDim.x *<span class="number">4</span>]+b[i+blockDim.x *<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">5</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">5</span>] = a[i+blockDim.x *<span class="number">5</span>]+b[i+blockDim.x *<span class="number">5</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">6</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">6</span>] = a[i+blockDim.x *<span class="number">6</span>]+b[i+blockDim.x *<span class="number">6</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">7</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">7</span>] = a[i+blockDim.x *<span class="number">7</span>]+b[i+blockDim.x *<span class="number">7</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">28</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid.x/<span class="number">8</span>, block&gt;&gt;&gt;(A, B, C, nElem);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed: %f ms \n&quot;</span>, grid.x/<span class="number">8</span>, block.x, elapsedTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">32768</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">353.544189</span> ms</span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">262144</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">739.468262</span> ms</span><br></pre></td></tr></table></figure><p>与最初的版本对比，循环展开技术节省了大半的运算时间</p><h4 id="增大并行性"><a href="#增大并行性" class="headerlink" title="增大并行性"></a>增大并行性</h4><p>对数组求和所用的<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>测试，寻找最佳执行配置</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">262144</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">865.561584</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">524288</span>,<span class="number">512</span>&gt;&gt;&gt; Time elapsed: <span class="number">757.686401</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">1048576</span>,<span class="number">256</span>&gt;&gt;&gt; Time elapsed: <span class="number">687.030212</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">2097152</span>,<span class="number">128</span>&gt;&gt;&gt; Time elapsed: <span class="number">786.416626</span> ms </span><br></pre></td></tr></table></figure><p>可以看到在<code>block.x</code>为 256 时，执行效率最高，这是因为较高的<code>block.x</code>会使 SM 的并行性降低，而过低的<code>block.x</code>不能充分利用 SM 的计算资源，具体的<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>大小取决于当前的 GPU 架构中的 SM 配置，需要实验得出</p>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA内存模型概述</title>
      <link href="/2024/01/02/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"/>
      <url>/2024/01/02/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>讲解 CUDA 内存层次结构，如寄存器，共享内存，纹理内存，全局内存等。</p><span id="more"></span><h2 id="内存层次结构"><a href="#内存层次结构" class="headerlink" title="内存层次结构"></a>内存层次结构</h2><p>首先了解一下应用程序遵循的局部性原则</p><ul><li><p>时间局部性</p><p>一个内存地址被访问，那么这个内存地址很可能会被多次访问，被访问的概率会随着时间逐渐降低</p></li><li><p>空间局部性</p><p>如果一个内存地址被访问，那么附近的地址也有可能被访问</p></li></ul><p>随着科技的发展，更低延时和低容量的内存层次结构被设计出来以提高计算机性能，内存结构变得复杂，诞生出了由多级带宽，容量组成的内存层次结构，如下图所示</p><p><img src="/image/CUDA编程-CUDA内存模型概述/1.png" alt=""></p><p>上述结构从下往上有如下特点</p><ul><li>更高的成本/bit</li><li>更低的容量</li><li>更低的延时</li><li>更高的访问频率</li></ul><p>CPU 的主存采用动态随机存储器（DRAM），更快的 CPU 一级缓存使用的是静态随机存储器（SRAM），当数据被频繁使用时，会保存在低延时、低容量的内存层次中，否则会保存在高延时，大容量的容器中。GPU 的主存和 CPU 一样使用 DRAM，内存层次结构也非常相似，与 CPU 内存模型不同的是，通过 CUDA，我们可以方便地控制 GPU 的内存</p><h2 id="CUDA-内存模型"><a href="#CUDA-内存模型" class="headerlink" title="CUDA 内存模型"></a>CUDA 内存模型</h2><p>CUDA 提供了多种可编程的不同类型的内存可以满足不同的计算需求。每种内存类型都有其特定的用途和性能特点</p><ul><li>寄存器（Registers）</li><li>共享内存（Shared Memory）</li><li>本地内存（Local Memory）</li><li>常量内存（Constant Memory）</li><li>纹理内存（Texture Memory）</li><li>全局内存（Global Memory）</li></ul><p><img src="/image/CUDA编程-CUDA内存模型概述/2.png" alt=""></p><p>如上图所示，每个核函数都有自己私有的本地内存，每个线程块有自己的共享内存，对同一线程块中所有的线程可见，其内容会持续线程块的整个生命周期。所有线程都可以访问全局内存。所有线程对常量内存和纹理内存都只读</p><p>在内存层次结构中，纹理内存为各种数据布局提供了不同的寻址模式和滤波模式，对于应用程序来说，全局内存、常量内存中的内容具有相同的生命周期</p><h2 id="寄存器"><a href="#寄存器" class="headerlink" title="寄存器"></a>寄存器</h2><p>寄存器是一种低容量、超高速度的内存类型，每个线程都可以使用寄存器来存储临时数据。当在核函数内的自变量没有其他修饰符，该变量就存储在寄存器中，在核函数中定义的的数组也存储在寄存器中</p><p>寄存器对于每个线程是私有的，核函数使用寄存器来通常保存被频繁使用的线程私有变量，寄存器变量的声明周期和核函数一致，执行完毕后，寄存器就不能访问了。</p><p>寄存器是 SM 中的较少资源，Fermi 架构中每个线程最多63个寄存器。Kepler结构扩展到255个寄存器，一个线程如果使用更少的寄存器，那么就会有更多的常驻线程块，SM上并发的线程块越多，效率越高，性能和使用率也就越高。</p><p>可以使用如下命令得到每个核函数运行时使用的寄存器数量、共享内存字节数以及每个线程所使用的常量内存和字节数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -v *.cu</span><br></pre></td></tr></table></figure><p>以第三章中的矩阵乘法和加法为例，输出表示编译器进行了两个操作：编译矩阵乘法函数<code>Z9MatrixMulP6MatrixS0_S0</code>和矩阵加法函数<code>Z9MatrixAddP6MatrixS0_S0</code>，分别针对<code>sm_52</code>架构，对于每个函数，都会输出函数属性，如堆栈帧大小、溢出存储和溢出加载的大小，并且报告使用的寄存器数量和全局内存<code>cmem[0]</code>大小</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ptxas info    : 0 bytes gmem</span><br><span class="line">ptxas info    : Compiling entry <span class="keyword">function</span> <span class="string">&#x27;_Z9MatrixMulP6MatrixS0_S0_&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_52&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _Z9MatrixMulP6MatrixS0_S0_</span><br><span class="line">    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads</span><br><span class="line">ptxas info    : Used 32 registers, 344 bytes cmem[0]</span><br><span class="line">ptxas info    : Compiling entry <span class="keyword">function</span> <span class="string">&#x27;_Z9MatrixAddP6MatrixS0_S0_&#x27;</span> <span class="keyword">for</span> <span class="string">&#x27;sm_52&#x27;</span></span><br><span class="line">ptxas info    : Function properties <span class="keyword">for</span> _Z9MatrixAddP6MatrixS0_S0_</span><br><span class="line">    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads</span><br><span class="line">ptxas info    : Used 18 registers, 344 bytes cmem[0]</span><br></pre></td></tr></table></figure><p>如果一个核函数使用了超过硬件数量的寄存器，会用本地内存代替多占用的寄存器。nvcc 会使用启发式策略来最小化寄存器的使用，为了避免寄存器溢出，可以在核函数的代码中配置额外的信息来辅助编译器优化，下面代码中的<code>maxThreadsPerBlock</code>意为每个块最多可以启动的线程数量，<code>minBlocksPerMultiprocessor</code>意为每个 SM 最少运行的线程块数量</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> __launch_bounds__ (maxThreadsPerBlock, minBlocksPerMultiprocessor)</span><br><span class="line"><span class="built_in">kernel_func</span> (...) &#123;</span><br><span class="line"><span class="comment">//kernel body</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在调用下面的核函数时，最多可以使用 1024 个线程来执行该内核函数，在每个 SM 最少运行的线程块数量为 1</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="type">void</span> __launch_bounds__ (<span class="number">1024</span>, <span class="number">1</span>) <span class="built_in">MatrixAdd</span>(Matrix *A, Matrix *B, Matrix *C) &#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>还可以在编译时使用<code>-maxrregcount</code>来控制一个编译单元里所有核函数使用的寄存器的最大数量，但这可能会和<code>__launch_bounds__</code>产生冲突，如果使用<code>-maxrregcount</code>参数限制每个线程使用的寄存器数量为32，并且使用<code>__launch_bounds__</code>属性限制每个块可以启动的线程数量为1024，那么每个块中实际可以启动的线程数量就会受到限制，只能启动32个线程</p><h2 id="本地内存"><a href="#本地内存" class="headerlink" title="本地内存"></a>本地内存</h2><p>本地内存是每个线程私有的内存空间，用来存储线程私有的临时数据。核函数中符合存储在寄存器中但不能进入被核函数分配的寄存器空间中的变量将存储在本地内存中，以下几种变量可能存放在本地内存中的</p><ul><li>使用未知索引引用的本地数组</li><li>可能会占用大量寄存器空间的较大本地数组或者结构体</li><li>任何不满足核函数寄存器限定条件的变量</li></ul><p>本地内存本质上和全局内存存储在同一块存储区域，但本地内存为每个线程私有，且会比访问全局内存更快，对于2.0以上的设备，本地内存存储在每个 SM 的一级缓存和设备的二级缓存上</p><h2 id="共享内存"><a href="#共享内存" class="headerlink" title="共享内存"></a>共享内存</h2><p>共享内存是一种由多个线程共同使用的内存，是线程之间相互通信的基本方式，用来存储临时数据和高频使用的数据。共享内存类似于 CPU 的一级缓存，但可被编程。每个 SM 都有一些由线程块分配的共享内存，因此，不能过度使用共享内存，否则可能会限制活跃线程束的数量。</p><p>共享内存在核函数内声明，生命周期和线程块一致，线程块运行开始，此块的共享内存被分配，当此块结束，则共享内存被释放</p><p>可以通过在核函数中使用<code>__shared__</code>修饰符将变量放在共享内存中</p><p>因为共享内存是线程块中线程都可以访问，且线程是并发执行的，所以当同一个线程块中的多个线程访问同一个内存地址时可能会发生以下情况</p><ul><li>线程 a 和线程 b 同时将同一数组中的数据拷贝到共享内存中，导致数据冲突</li><li>线程 a 和线程 b 同时计算 x 和 y 数组对应位置的和，并将结果存储到 z 数组中，导致结果不正确</li></ul><p>所以访问共享内存前必须使用如下的同步语句</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> __syncthreads();</span><br></pre></td></tr></table></figure><p>如果频繁使用以上语句让 SM 进入空闲状态，会影响性能</p><p>SM中的一级缓存和共享内存共享片上内存，片上内存（on-chip memory）是指位于GPU片上的内存，即与 SM 处理器相连的内存，包括一级缓存、共享内存和常量缓存等</p><p>片上内存的大小根据 SM  版本而不同，以本人电脑的 sm_52 版本为例，一级缓存和共享内存共享的片上内存大小默认为</p><ul><li>一级缓存：64KB</li><li>共享内存：32KB</li></ul><p>因此，sm_52 版本的 SM 中共享的片上内存大小为 64KB + 32KB = 96KB，默认通过静态划分，运行时可以通过下面语句进行设置分配方案</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * func,<span class="keyword">enum</span> cudaFuncCache cacheConfig)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>func</code>: 指向内核函数的指针，表示需要设置缓存配置的内核函数</li><li><code>cudaFuncCache</code>: 表示内核函数的缓存配置，可以是以下值之一<ul><li><code>cudaFuncCachePreferNone</code>: 表示不使用缓存</li><li><code>cudaFuncCachePreferShared</code>: 表示优先使用共享内存</li><li><code>cudaFuncCachePreferL1</code>: 表示优先使用一级缓存</li><li><code>cudaFuncCachePreferEqual</code>: 表示优先使用 L1 缓存或共享内存，取决于哪个更快，使用该选项可能会带来额外的性能开销，不建议使用</li></ul></li></ul><p>下面的程序定义了一个用于设置缓存配置的函数<code>cudaFuncSetCacheConfig</code>，它接受一个指向CUDA函数的指针、一个预定义的缓存配置枚举值、一个备选的缓存配置枚举值作为参数，并返回一个错误码，还定义了一个空的内核函数<code>Kernel_func</code>用于演示</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 设置指定函数的缓存配置</span></span><br><span class="line"><span class="function">cudaError_t <span class="title">cudaFuncSetCacheConfig</span><span class="params">(<span class="type">const</span> <span class="type">void</span> * func, <span class="keyword">enum</span> cudaFuncCache cacheConfig, <span class="keyword">enum</span> cudaFuncCache cacheConfigAlt)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义错误码</span></span><br><span class="line">  cudaError_t error = cudaSuccess;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用 CUDA 驱动 API 设置缓存配置</span></span><br><span class="line">  error = <span class="built_in">cudaFuncSetCacheConfig</span>(func, cacheConfig);</span><br><span class="line">  <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">    <span class="keyword">return</span> error;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 使用 CUDA 驱动 API 设置备用缓存配置</span></span><br><span class="line">  error = <span class="built_in">cudaFuncSetCacheConfig</span>(func, cacheConfigAlt);</span><br><span class="line">  <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">    <span class="keyword">return</span> error;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 返回成功</span></span><br><span class="line">  <span class="keyword">return</span> cudaSuccess;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">Kernel_func</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 这是一个什么也不做的空内核</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 定义要设置的函数</span></span><br><span class="line">  <span class="type">void</span> *func = (<span class="type">void</span>*)Kernel_func;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义所需的缓存配置</span></span><br><span class="line">  <span class="keyword">enum</span> <span class="title class_">cudaFuncCache</span> cacheConfig = cudaFuncCachePreferL1;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 为函数设置缓存配置</span></span><br><span class="line">  cudaError_t error = <span class="built_in">cudaFuncSetCacheConfig</span>(func, cacheConfig, cudaFuncCachePreferNone);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 检查错误</span></span><br><span class="line">  <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">    <span class="comment">// 打印错误消息并退出</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Error setting cache configuration: %s\n&quot;</span>, <span class="built_in">cudaGetErrorString</span>(error));</span><br><span class="line">    <span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Success!\n&quot;</span>);</span><br><span class="line">  <span class="comment">// 缓存配置设置成功</span></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="常量内存"><a href="#常量内存" class="headerlink" title="常量内存"></a>常量内存</h2><p>常量内存驻留在设备内存中，每个SM都有专用的常量内存缓存，可以通过在核函数中使用<code>__constant__</code>修饰符将变量放在常量内存中</p><p>常量内存需要在核函数外，全局范围内声明，对于所有设备，只可以声明 64KB 的常量内存，常量内存是静态声明的，主机端代码可以初始化常量内存，初始化后不能被核函数修改，并且对同一编译单元中的所有核函数可见，相关函数将数据从主内存复制到常量缓存（constant memory）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMemcpyToSymbol</span><span class="params">(<span class="type">const</span> <span class="type">void</span>* symbol,<span class="type">const</span> <span class="type">void</span> *src,<span class="type">size_t</span> count)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>symbol</code>: 指向常量缓存的指针，常量缓存是一种特殊的内存类型，用于存储在编译时不变的变量</li><li><code>src</code>: 指向主内存中的数据的指针，要复制的数据必须位于主内存中，因为SM处理器无法直接访问主内存</li><li><code>count</code>: 要复制的数据的字节数</li></ul><p>下面的程序在常量内存中定义了一个名为 <code>a</code> 的常量数组，并通过 <code>cudaMemcpyToSymbol</code> 函数将一个主机上的数组 <code>h_a</code> 复制到该常量数组中</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line">__constant__ <span class="type">int</span> a[<span class="number">100</span>];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> h_a[<span class="number">100</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">100</span>; i++)</span><br><span class="line">        h_a[i] = i;</span><br><span class="line">    <span class="built_in">cudaMemcpyToSymbol</span>(a, h_a, <span class="built_in">sizeof</span>(h_a));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="纹理内存"><a href="#纹理内存" class="headerlink" title="纹理内存"></a>纹理内存</h2><p>纹理内存是一种用来存储纹理数据的内存类型，在每个 SM 的只读缓存中缓存，纹理内存是通过指定的缓存访问的全局内存，只读缓存包括硬件滤波的支持，它可以将浮点插入作为读取过程中的一部分来执行，纹理内存是对二维空间局部性的优化，所以通常用来存储渲染图像和视频的数据，同时对于某些需要滤波的程序性能更好，可以直接通过硬件完成计算</p><p>定义一个 CUDA 纹理对象需要使用<code>cudaCreateTextureObject</code> 函数，解释如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaCreateTextureObject</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    cudaTextureObject_t *pTexObject,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> cudaResourceDesc *pResDesc,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> cudaTextureDesc *pTexDesc,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">const</span> cudaResourceViewDesc *pResViewDesc</span></span></span><br><span class="line"><span class="params"><span class="function">)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>pTexObject</code>：指向一个 <code>cudaTextureObject_t</code> 类型的指针，用于存储新创建的纹理对象</li><li><code>pResDesc</code>：指向一个 <code>cudaResourceDesc</code> 类型的指针，用于描述纹理资源</li><li><code>pTexDesc</code>：指向一个 <code>cudaTextureDesc</code> 类型的指针，用于描述纹理对象的属性</li><li><code>pResViewDesc</code>：指向一个 <code>cudaResourceViewDesc</code> 类型的指针，用于描述纹理视图的属性</li></ul><p>更具体的解释请看<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TEXTURE__OBJECT.html#group__CUDART__TEXTURE__OBJECT_1g16ac75814780c3a16e4c63869feb9ad3">官方文档</a></p><p>下面的代码创建了一个二维数据简单地模拟图像使用纹理内存</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 CUDA 内核函数，名为 transformKernel</span></span><br><span class="line"><span class="comment">// 该函数有三个输入参数：</span></span><br><span class="line"><span class="comment">// - output：浮点型指针，指向结果数组</span></span><br><span class="line"><span class="comment">// - texObj：cudaTextureObject_t 类型，表示一个 CUDA 纹理对象</span></span><br><span class="line"><span class="comment">// - width 和 height：表示纹理对象的宽度和高度</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transformKernel</span><span class="params">(<span class="type">float</span>* output, cudaTextureObject_t texObj, <span class="type">int</span> width, <span class="type">int</span> height)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 计算当前线程的纵横坐标</span></span><br><span class="line">    <span class="type">int</span> x = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line">    <span class="type">int</span> y = blockIdx.y * blockDim.y + threadIdx.y;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果超出纹理对象的范围，则退出该函数</span></span><br><span class="line">    <span class="keyword">if</span> ( x&lt;<span class="number">0</span> || x&gt;width || y&lt;<span class="number">0</span> || y&gt;height )</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 在纹理对象中查找 (x+0.5, y+0.5) 处的值，并将其赋值给 output 数组对应的位置</span></span><br><span class="line">    output[ y*width+x ] = <span class="built_in">tex2D</span>&lt;<span class="type">float</span>&gt;(texObj, x+<span class="number">0.5f</span>, y+<span class="number">0.5f</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 定义纹理对象的尺寸</span></span><br><span class="line">    <span class="type">int</span> width = <span class="number">10</span>;</span><br><span class="line">    <span class="type">int</span> height = <span class="number">10</span>;</span><br><span class="line">    <span class="type">int</span> size = width*height*<span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配内存空间给源数据</span></span><br><span class="line">    <span class="type">float</span> *h_data = <span class="keyword">new</span> <span class="type">float</span>[width*height];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 初始化原始数据</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y&lt;height; y++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x&lt;width; x++) &#123;</span><br><span class="line">                h_data[y*width + x] = x;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印原始数据</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;old:\n&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y&lt;height; y++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x&lt;width; x++) &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%f &quot;</span>, h_data[y*width + x]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建一个用于存储源数据的 CUDA 数组</span></span><br><span class="line">    cudaChannelFormatDesc channelDesc = <span class="built_in">cudaCreateChannelDesc</span>(<span class="number">32</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, cudaChannelFormatKindFloat);</span><br><span class="line">    cudaArray* cuArray;</span><br><span class="line">    <span class="built_in">cudaMallocArray</span>(&amp;cuArray, &amp;channelDesc, width, height);</span><br><span class="line">    <span class="comment">// 将源数据复制到 CUDA 数组中</span></span><br><span class="line">    <span class="built_in">cudaMemcpyToArray</span>(cuArray, <span class="number">0</span>, <span class="number">0</span>, h_data, size,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个 cudaResourceDesc 结构体，用于描述纹理资源</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaResourceDesc</span> resDesc;</span><br><span class="line">    <span class="comment">// 将该结构体的所有字段清零</span></span><br><span class="line">    <span class="built_in">memset</span>(&amp;resDesc, <span class="number">0</span>, <span class="built_in">sizeof</span>(resDesc));</span><br><span class="line">    <span class="comment">// 设置纹理资源的类型为数组资源</span></span><br><span class="line">    resDesc.resType = cudaResourceTypeArray;</span><br><span class="line">    <span class="comment">// 设置纹理资源所指向的数组</span></span><br><span class="line">    resDesc.res.array.array = cuArray;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个 cudaTextureDesc 结构体，用于描述纹理对象</span></span><br><span class="line">    <span class="keyword">struct</span> <span class="title class_">cudaTextureDesc</span> texDesc;</span><br><span class="line">    <span class="comment">// 将该结构体的所有字段清零</span></span><br><span class="line">    <span class="built_in">memset</span>(&amp;texDesc, <span class="number">0</span>, <span class="built_in">sizeof</span>(texDesc));</span><br><span class="line">    <span class="comment">// 设置纹理坐标超出纹理范围时的采样模式</span></span><br><span class="line">    texDesc.addressMode[<span class="number">0</span>] = cudaAddressModeBorder;</span><br><span class="line">    texDesc.addressMode[<span class="number">1</span>] = cudaAddressModeBorder;</span><br><span class="line">    <span class="comment">// 设置纹理采样滤波模式</span></span><br><span class="line">    texDesc.filterMode = cudaFilterModeLinear;</span><br><span class="line">    <span class="comment">// 设置纹理的读取模式</span></span><br><span class="line">    texDesc.readMode = cudaReadModeElementType;</span><br><span class="line">    <span class="comment">// 设置纹理坐标是否归一化</span></span><br><span class="line">    texDesc.normalizedCoords = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义一个 cudaTextureObject_t 类型的变量，表示纹理对象</span></span><br><span class="line">    cudaTextureObject_t texObj = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// 使用纹理资源和纹理描述创建纹理对象</span></span><br><span class="line">    <span class="built_in">cudaCreateTextureObject</span>(&amp;texObj, &amp;resDesc, &amp;texDesc, <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 分配内存给存储结果数据的数组</span></span><br><span class="line">    <span class="type">float</span>* output;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;output, size);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算线程块和网格的维度</span></span><br><span class="line">    <span class="function">dim3 <span class="title">dimBlock</span><span class="params">(<span class="number">4</span>, <span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">dimGrid</span><span class="params">( max( (width  + dimBlock.x - <span class="number">1</span>) / dimBlock.x,<span class="number">1</span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                      max( (height + dimBlock.y - <span class="number">1</span>) / dimBlock.y,<span class="number">1</span>) )</span></span>;</span><br><span class="line">    <span class="comment">// 执行 CUDA 内核函数</span></span><br><span class="line">    transformKernel &lt;&lt;&lt;dimGrid, dimBlock &gt;&gt;&gt;(output, texObj, width, height);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 将结果数据从 GPU 拷贝到 CPU 上</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(h_data, output, size, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印结果数据</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;new:\n&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">0</span>; y&lt;height; y++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">0</span>; x&lt;width; x++) &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;%f &quot;</span>,h_data[y*width + x]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 销毁纹理对象</span></span><br><span class="line">    <span class="built_in">cudaDestroyTextureObject</span>(texObj);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放 CUDA 数组的内存</span></span><br><span class="line">    <span class="built_in">cudaFreeArray</span>(cuArray);</span><br><span class="line">    <span class="comment">// 释放结果数据的内存</span></span><br><span class="line">    <span class="built_in">cudaFree</span>(output);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 释放原始数据的内存</span></span><br><span class="line">    <span class="keyword">delete</span>[]h_data;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>纹理内存这部分知识点偏多，后面有机会和大家细细道来</p><h2 id="全局内存"><a href="#全局内存" class="headerlink" title="全局内存"></a>全局内存</h2><p>全局内存也可以说是 GPU 的主存。它是 GPU 内存层次结构中最大容量、最高延时的内存类型，它的声明可以在所有 SM 设备上被访问到，并且与程序同生命周期，全局变量支持静态声明和动态声明</p><p>可以通过在核函数中使用<code>__device__</code>修饰符将变量放在全局内存中</p><p>我们在第三章中的所有程序在 GPU 上访问的内存都是全局内存，因为线程的执行不能跨线程块同步，当有多个线程并发地修改全局内存的同一位置时，会导致未定义的程序行为</p><p>全局内存访问必须是自然对齐的，也就是一次要读取 32 的整数倍字节的内存，所以当线程束执行内存加载或存储时，需要满足的传输数量通常取决于</p><ul><li>跨线程的内存地址分布</li><li>内存事务的对齐方式。</li></ul><p>一般满足内存请求的事务越多，未使用的字节被传输的可能性越大，数据吞吐量就会降低，也可以说，对齐的读写模式使得不需要的数据也被传输，所以，利用率低到时吞吐量下降。过去的设备因为没有足够的缓存，对内存访问要求非常严格，现在要求宽松了一些</p><h2 id="GPU-缓存"><a href="#GPU-缓存" class="headerlink" title="GPU 缓存"></a>GPU 缓存</h2><p>在 CUDA 中，GPU 缓存是不可编程的内存，有如下四种缓存</p><ul><li>一级缓存</li><li>二级缓存</li><li>只读常量缓存</li><li>只读纹理缓存</li></ul><p>每个 SM 都有一个一级缓存，所有 SM 公用一个二级缓存，一级和二级缓存都被用来存储本地内存和全局内存中的数据，也包括寄存器溢出的部分。CUDA 允许我们配置读操作的数据是使用一级缓存和二级缓存，还是只使用二级缓存</p><p>CPU 读写过程都有可能被缓存，与 CPU 不同的是，GPU 写的过程不被缓存，只有读取会被缓存，每个 SM 有一个只读常量缓存，只读纹理缓存，它们用于设备内存中提高来自于各自内存空间内的读取性能</p><h2 id="CUDA-变量声明总结"><a href="#CUDA-变量声明总结" class="headerlink" title="CUDA 变量声明总结"></a>CUDA 变量声明总结</h2><p>下面总结了 CUDA 变量声明和它们相应的存储位置、作用域、生命周期和修饰符</p><div class="table-container"><table><thead><tr><th>修饰符</th><th>变量名称</th><th>存储器</th><th>作用域</th><th>生命周期</th></tr></thead><tbody><tr><td></td><td><code>float var</code></td><td>寄存器</td><td>线程</td><td>线程</td></tr><tr><td></td><td><code>float var[100]</code></td><td>本地</td><td>线程</td><td>线程</td></tr><tr><td><code>__shared__</code></td><td><code>float var +</code></td><td>共享</td><td>块</td><td>块</td></tr><tr><td><code>__device__</code></td><td><code>float var +</code></td><td>全局</td><td>全局</td><td>应用程序</td></tr><tr><td><code>__constant__</code></td><td><code>float var +</code></td><td>常量</td><td>全局</td><td>应用程序</td></tr></tbody></table></div><blockquote><p><code>float var +</code> 表示标量或数组</p></blockquote><p>下面总结了各类存储器的主要特征</p><div class="table-container"><table><thead><tr><th>存储器</th><th>片上/片外</th><th>缓存</th><th>存取</th><th>范围</th><th>生命周期</th></tr></thead><tbody><tr><td>寄存器</td><td>片上</td><td>N/A</td><td>R/W</td><td>一个线程</td><td>线程</td></tr><tr><td>本地</td><td>片外</td><td>+</td><td>R/W</td><td>一个线程</td><td>线程</td></tr><tr><td>共享</td><td>片上</td><td>N/A</td><td>R/W</td><td>块内所有线程</td><td>块</td></tr><tr><td>全局</td><td>片外</td><td>+</td><td>R/W</td><td>所有线程 + 主机</td><td>主机配置</td></tr><tr><td>常量</td><td>片外</td><td>Yes</td><td>R</td><td>所有线程 + 主机</td><td>主机配置</td></tr><tr><td>纹理</td><td>片外</td><td>Yes</td><td>R</td><td>所有线程 + 主机</td><td>主机配置</td></tr></tbody></table></div><blockquote><p><code>+</code> 表示计算能力在 2.X 以上的 GPU 支持</p></blockquote><h2 id="静态全局内存"><a href="#静态全局内存" class="headerlink" title="静态全局内存"></a>静态全局内存</h2><p>我们在第三章中使用 cudaMalloc 函数申请的都是动态内存，也就是动态分配，在 CUDA 中也支持静态内存，也可以说是静态分配，与动态分配相同，需要显式的将内存拷贝到设备端，需要使用的函数如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMemcpyToSymbol</span> <span class="params">( <span class="type">const</span> <span class="type">void</span>* symbol, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count, <span class="type">size_t</span> offset = <span class="number">0</span>, cudaMemcpyKind kind = cudaMemcpyHostToDevice )</span></span></span><br></pre></td></tr></table></figure><p>从 CPU 内存中的变量值复制到 GPU 的全局内存中</p><ul><li><code>symbol</code>: 要复制数据的标识符，指的是定义在 GPU 的全局内存中的变量，不是变量地址</li><li><code>src</code>: 源数据的地址</li><li><code>count</code>: 要复制的数据的字节数</li><li><code>offset</code>: 目标标识符中的偏移量，表示从符号的哪个位置开始复制数据</li><li><code>kind</code>: 复制数据的类型，可以是 <code>cudaMemcpyHostToDevice</code> 或 <code>cudaMemcpyDeviceToHost</code></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMemcpyFromSymbol</span> <span class="params">( <span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* symbol, <span class="type">size_t</span> count, <span class="type">size_t</span> offset = <span class="number">0</span>, cudaMemcpyKind kind = cudaMemcpyDeviceToHost )</span></span></span><br></pre></td></tr></table></figure><p>将 GPU 的全局内存中的变量值复制到 CPU 内存中</p><ul><li>dst：目标数据的地址</li><li>symbol：要复制数据的标识符，指的是定义在 GPU 的全局内存中的变量，不是变量地址</li><li>count：要复制的数据的字节数</li><li>offset：源标识符中的偏移量，表示从符号的哪个位置开始复制数据</li><li>kind：复制数据的类型，可以是 <code>cudaMemcpyHostToDevice</code> 或 <code>cudaMemcpyDeviceToHost</code></li></ul><p>举例程序如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line">__device__ <span class="type">float</span> devData;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">checkGlobalVariable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Device: The value of the global variable is %f\n&quot;</span>,devData);</span><br><span class="line">    devData+=<span class="number">2.0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> value=<span class="number">3.1415926f</span>;</span><br><span class="line">    <span class="built_in">cudaMemcpyToSymbol</span>(devData,&amp;value,<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: copy %f to the global variable\n&quot;</span>,value);</span><br><span class="line">    checkGlobalVariable&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();</span><br><span class="line">    <span class="built_in">cudaMemcpyFromSymbol</span>(&amp;value,devData,<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Host: the value changed by the kernel to %f \n&quot;</span>,value);</span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> EXIT_SUCCESS;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在以上代码中，如果使用如下代码拷贝是无效的，因为动态拷贝的方法无法对静态变量赋值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cudaMemcpy</span>(&amp;value,devData,<span class="built_in">sizeof</span>(<span class="type">float</span>));</span><br></pre></td></tr></table></figure><p>但是可以使用 <code>cudaGetSymbolAddress</code> 函数获取设备的全局变量的地址，而不能使用 <code>&amp;</code> 直接取地址，之后 再使用 <code>cudaMemcpy</code> 将值拷贝到主机上</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> *dptr=<span class="literal">NULL</span>;</span><br><span class="line"><span class="built_in">cudaGetSymbolAddress</span>((<span class="type">void</span>**)&amp;dptr,devData);</span><br><span class="line"><span class="built_in">cudaMemcpy</span>(dptr,&amp;value,<span class="built_in">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure><p>有一个例外，CUDA 固定内存可以直接从主机引用 GPU 内存，下一章节我们将详细了解</p>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: PyCUDA编程简介</title>
      <link href="/2023/12/26/CUDA%E7%BC%96%E7%A8%8B-PyCUDA%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/12/26/CUDA%E7%BC%96%E7%A8%8B-PyCUDA%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>PyCUDA GPU 编程。主要内容是掌握几种PyCUDA 的基本内核函数，在 Python 中生成和调用并行化的 CUDA C 内核函数，以及了解 PyCUDA 的并行前缀算法。</p><span id="more"></span><h2 id="获取-GPU-信息"><a href="#获取-GPU-信息" class="headerlink" title="获取 GPU 信息"></a>获取 GPU 信息</h2><p>首先，需要初始化 CUDA，使用 <code>pycuda.driver.init()</code>或通过<code>import pycuda.autoinit</code>使用<code>autoinit</code>子模块初始化，接下来使用<code>pycuda.driver.Device(i)</code>来访问不同的 GPU 设备，这里笔者的电脑为单 GPU</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line">drv.init()</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="comment"># import pycuda.autoinit</span></span><br><span class="line"><span class="comment"># 进行自动初始化</span></span><br><span class="line"></span><br><span class="line">gpu_device = drv.Device(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GPU Device Name&#123;&#125;: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="number">0</span>, gpu_device.name()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SM Count: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MULTIPROCESSOR_COUNT))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Shared Memory Size per Thread Block: &#123;&#125; KB&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_SHARED_MEMORY_PER_BLOCK / <span class="number">1024.0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Threads per Thread Block: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_BLOCK))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Threads per SM: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_MULTIPROCESSOR))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Warps per SM: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_MULTIPROCESSOR / <span class="number">32</span>))</span><br></pre></td></tr></table></figure><p>运行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python infogpu.py</span><br></pre></td></tr></table></figure><p>输出如下，SM 数量为 30，每个线程块的共享内存为 48KB，每个线程块有 1024 个线程，每个 SM 有 1536 个线程，每个 SM 有 48 个线程束</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU Device Name0: NVIDIA GeForce RTX 3060 Laptop GPU</span><br><span class="line">SM Count: 30</span><br><span class="line">Shared Memory Size per Thread Block: 48.0 KB</span><br><span class="line">Threads per Thread Block: 1024</span><br><span class="line">Threads per SM: 1536</span><br><span class="line">Warps per SM: 48.0</span><br></pre></td></tr></table></figure><h2 id="使用-gpuarray-类"><a href="#使用-gpuarray-类" class="headerlink" title="使用 gpuarray 类"></a>使用 gpuarray 类</h2><p>PyCUDA 库中，gpuarray 为数据的存储类型，正如 Numpy 库中的 array 为数据的存储类型</p><h3 id="传输数据"><a href="#传输数据" class="headerlink" title="传输数据"></a>传输数据</h3><p>在 CUDA C 中，我们需要使用 <code>cudaMemcpyHostToDevice</code>和<code>cudaMemcpyDeviceToHost</code>等函数来进行主机与设备的数据传输，需要跟踪主机与设备内存空间中的多个指针，还要进行内存分配，会使代码变得复杂，PyCUDA 的 gpuarray 能够自动完成内存分配，并根据生命周期自动释放。举例如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_host = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=np.float32)</span><br><span class="line">y_host = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], dtype=np.float32)</span><br><span class="line">z_host = np.array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">x_device = gpuarray.to_gpu(x_host)</span><br><span class="line">y_device = gpuarray.to_gpu(y_host)</span><br><span class="line">z_device = gpuarray.to_gpu(z_host)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((x_device + y_device + z_device).get())</span><br><span class="line"><span class="built_in">print</span>((x_device ** y_device).get())</span><br><span class="line"><span class="built_in">print</span>((x_device / z_device).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">[<span class="number">12.</span> <span class="number">15.</span> <span class="number">18.</span>]</span><br><span class="line">[  <span class="number">1.</span>       <span class="number">32.</span>      <span class="number">729.00006</span>]</span><br><span class="line">[<span class="number">0.14285715</span> <span class="number">0.25</span>       <span class="number">0.33333334</span>]</span><br></pre></td></tr></table></figure><p>上面的程序使用<code>gpuarray.to_gpu()</code>将 array 转为 gpuarray，使用 gpuarray 的<code>get()</code>属性将 gpuarray 转为 array，除此之外，gpuarray 还具有如下属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pycuda.gpuarray.zeros(shape, dtype=np.float64, *, allocator=<span class="literal">None</span>, order=<span class="string">&#x27;C&#x27;</span>)<span class="comment"># 开辟gpu内存空间，创建 (m,n) 的0矩阵</span></span><br><span class="line">pycuda.gpuarray.empty(shape, dtype, *, allocator=<span class="literal">None</span>, order=<span class="string">&#x27;C&#x27;</span>)<span class="comment"># 开辟gpu内存空间，创建 (m,n) 的空矩阵</span></span><br><span class="line">pycuda.gpuarray.zeros_like(other_ary, dtype=<span class="literal">None</span>, order=<span class="string">&#x27;K&#x27;</span>)<span class="comment"># 开辟gpu内存空间，创建 size 同 ary 的 0矩阵，因此ary最好</span></span><br><span class="line">pycuda.gpuarray.empty_like(other_ary, dtype=<span class="literal">None</span>, order=<span class="string">&#x27;K&#x27;</span>)<span class="comment"># 开辟gpu内存空间，创建 size 同 ary 的空矩阵</span></span><br><span class="line">pycuda.gpuarray.arange(start, stop, step, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)<span class="comment"># 创建顺序序列</span></span><br><span class="line">pycuda.gpuarray.take(a, indices, stream=<span class="literal">None</span>)<span class="comment"># 返回 gpu_ary[a[index[0]], ..., a[index[n]]]</span></span><br><span class="line">pycuda.gpuarray.maximum(a, b, out=<span class="literal">None</span>, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 对应a和b中的较大元素</span></span><br><span class="line">pycuda.gpuarray.minimum(a, b, out=<span class="literal">None</span>, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 对应a和b中的较小元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">max</span>(a, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 所有元素的最大元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">min</span>(a, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 所有元素的最小元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">sum</span>(a, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 所有元素求和</span></span><br><span class="line">pycuda.gpuarray.dot(a, b, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)<span class="comment"># gpu_ary 求点积</span></span><br></pre></td></tr></table></figure><p>上述函数中的<code>stream=None</code>意为指定 CUDA 流，我们会在后面的章节详细讲解。</p><p>更多属性使用方法请查看<a href="https://documen.tician.de/pycuda/array.html#constructing-gpuarray-instances">官方文档</a></p><h2 id="PyCUDA-生成并行化-Kernel-函数"><a href="#PyCUDA-生成并行化-Kernel-函数" class="headerlink" title="PyCUDA 生成并行化 Kernel 函数"></a>PyCUDA 生成并行化 Kernel 函数</h2><p>PyCUDA 提供了大量函数来生成内联的 CUDA C 内核函数，内联函数通过 NVIDIA NVCC 编译器在外部编译缓存，由 PyCUDA 程序在运行时调用，下面为大家举几个主要例子，更详细的 PyCUDA 内核函数介绍请看<a href="https://documen.tician.de/pycuda/array.html#module-pycuda.elementwise">官方文档</a></p><h3 id="ElementwiseKernel-逐元素运算"><a href="#ElementwiseKernel-逐元素运算" class="headerlink" title="ElementwiseKernel 逐元素运算"></a>ElementwiseKernel 逐元素运算</h3><p><code>ElementwiseKernel</code>函数用于对 gpuarray 数组并行化逐元素计算，函数声明如下</p><p><code>class pycuda.elementwise.ElementwiseKernel(arguments, operation, name=&#39;kernel&#39;, keep=False, options=[], preamble=&#39;&#39;)</code></p><ul><li><code>arguments</code>: 意为参数列表，内容是声明变量的字符串，为 C/C++ 语法，末尾无分号</li><li><code>operation</code>: 为变量赋值，对数组的操作需要用 <code>i</code> 进行索引，类会自动 并行化 GPU 各核心中与 <code>i</code> 有关的计算，为 C/C++ 语法，末尾无分号</li><li><code>name</code>: 指定内核函数名称，命名空间在 CUDA C 中</li><li><code>options</code>: 构建时要使用的编译器选项，为<code>list</code>类型</li><li><code>keep</code>: 为<code>True</code>则保留编译器输出目录，并打印一行指示其所在目录以用于调试，<code>False</code>则不保留</li><li><code>preamble</code>: 指定内核构造之前包含的代码，可以使用它来包含其他文件和定义<code>operation</code>使用的函数</li></ul><p>下例的程序定义了一个名为 <code>gpu_x2_kernel</code>的内核函数，实现了输入数组<code>in</code>中每个元素的平方，将结果数组<code>out</code>中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pycuda.elementwise <span class="keyword">import</span> ElementwiseKernel</span><br><span class="line">host_data = np.float32(np.random.random(<span class="number">1</span>&lt;&lt;<span class="number">20</span>))</span><br><span class="line">gpu_x2_kernel = ElementwiseKernel(</span><br><span class="line">    <span class="string">&quot;float *in, float *out&quot;</span>,    <span class="comment"># arguments</span></span><br><span class="line">    <span class="string">&quot;out[i] = in[i] * in[i]&quot;</span>,   <span class="comment"># operation</span></span><br><span class="line">    <span class="string">&quot;gpu_x2_kernel&quot;</span>             <span class="comment"># name</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下面的程序比较了 gpuarray 数组运算与<code>ElementwiseKernel</code>类运算的速度，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pycuda.elementwise <span class="keyword">import</span> ElementwiseKernel</span><br><span class="line"></span><br><span class="line">host_data = np.float32(np.random.random(<span class="number">1</span>&lt;&lt;<span class="number">29</span>))</span><br><span class="line"></span><br><span class="line">gpu_x2_kernel = ElementwiseKernel(</span><br><span class="line">    <span class="string">&quot;float *in, float *out&quot;</span>,</span><br><span class="line">    <span class="string">&quot;out[i] = in[i] * in[i]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;gpu_x2_kernel&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">speedcomparison</span>():</span><br><span class="line">    t1 = time()</span><br><span class="line">    host_data_x2 =  host_data * host_data</span><br><span class="line">    t2 = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time to compute on CPU: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t2 - t1))</span><br><span class="line">    device_data = gpuarray.to_gpu(host_data)</span><br><span class="line">    <span class="comment"># allocate memory for output</span></span><br><span class="line">    device_data_2x = gpuarray.empty_like(device_data)</span><br><span class="line">    t1 = time()</span><br><span class="line">    gpu_x2_kernel(device_data, device_data_2x)</span><br><span class="line">    t2 = time()</span><br><span class="line">    from_device = device_data_2x.get()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time to compute on GPU: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t2 - t1))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Is the host computation the same as the GPU computation? : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(from_device, host_data_x2)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    speedcomparison()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 在 ipython 中运行输出为</span></span><br><span class="line">In [<span class="number">1</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.36846113204956055</span></span><br><span class="line">total time to compute on GPU: <span class="number">0.07349300384521484</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.368114709854126</span></span><br><span class="line">total time to compute on GPU: <span class="number">0.03887605667114258</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.3670005798339844</span>dair_v2x_root</span><br><span class="line">total time to compute on GPU: <span class="number">0.039081573486328125</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>可以看到在首次运行 GPU 的内核函数后，运算时间会大幅减少，这正是因为 NVCC 编译器会编译好的内核函数缓存起来，以重复使用</p><p>其中在内核函数中定义的<code>float *out</code>为 C 语言的浮点型指针，所以会使用<code>gpuarray.empty_like</code>先在 GPU 开辟内存空间，将运算结果放入其中，再使用<code>get</code>函数拷贝回主机</p><h3 id="InclusiveScanKernel-扫描内核函数"><a href="#InclusiveScanKernel-扫描内核函数" class="headerlink" title="InclusiveScanKernel 扫描内核函数"></a>InclusiveScanKernel 扫描内核函数</h3><p><code>InclusiveScanKernel</code>函数通过指定输入类型<code>dtype</code>和<code>string</code>类型的算术表达式<code>scan_expr</code>进行内核函数构造，再对数组逐元素进行并行化运算，函数声明如下</p><p><code>class pycuda.scan.InclusiveScanKernel(dtype, scan_expr, neutral=None, name_prefix=&#39;scan&#39;, options=[], preamble=&#39;&#39;, devices=None)</code></p><ul><li><code>dtype</code>: 指定输入数组类型</li><li><code>scan_expr</code>: 任意符合结合律的二元运算表达式，必须是 C 语言编写</li><li><code>neutral</code>: 中性元，加法应为 0，乘法应为 1，本小节后面会详细讲解原因</li><li><code>name_prefix</code>: 用于内核名称以确保配置文件和日志中的可识别性</li><li><code>options</code> 与 <code>preamble</code> 含义同上</li></ul><p>下面的程序对向量逐元素作加法以及逐元素查找最大值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.scan <span class="keyword">import</span> InclusiveScanKernel</span><br><span class="line"></span><br><span class="line">seq1 = np.array(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">10</span>)),dtype=np.int32)</span><br><span class="line">seq2 = np.array([<span class="number">1</span>,<span class="number">100</span>,-<span class="number">3</span>,-<span class="number">1000</span>,<span class="number">4</span>,<span class="number">1000000</span>,<span class="number">65</span>,<span class="number">2454</span>],dtype=np.int32)</span><br><span class="line">seq1_gpu = gpuarray.to_gpu(seq1)</span><br><span class="line">seq2_gpu = gpuarray.to_gpu(seq2)</span><br><span class="line">sum_gpu = InclusiveScanKernel(np.int32, <span class="string">&quot;a + b&quot;</span>)</span><br><span class="line">max_gpu = InclusiveScanKernel(np.int32, <span class="string">&quot;a&gt;b ? a : b&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sum_gpu(seq1_gpu).get())</span><br><span class="line"><span class="built_in">print</span>(max_gpu(seq2_gpu).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">6</span> <span class="number">10</span> <span class="number">15</span> <span class="number">21</span> <span class="number">28</span> <span class="number">36</span> <span class="number">45</span>]</span><br><span class="line">[      <span class="number">1</span>     <span class="number">100</span>     <span class="number">100</span>     <span class="number">100</span>     <span class="number">100</span> <span class="number">1000000</span> <span class="number">1000000</span> <span class="number">1000000</span>]</span><br></pre></td></tr></table></figure><p>上例所示，<code>sum_gpu</code>为<code>&quot;a + b&quot;</code>，包含<code>a</code>和<code>b</code>代指的二元运算的两个元素，类似 Python 中的 <code>lambda a,b : a + b</code>，<code>max_gpu</code>类似<code>lambda a,b : max(a,b)</code></p><h3 id="ReductionKernel-规约内核函数"><a href="#ReductionKernel-规约内核函数" class="headerlink" title="ReductionKernel 规约内核函数"></a>ReductionKernel 规约内核函数</h3><p><code>ReductionKernel</code>函数对<code>arguments</code>并行执行 <code>map expr</code>，然后对其结果执行 <code>reduce expr</code>，相当于<code>InclusiveScanKernel</code>函数后接着<code>ElementwiseKernel</code>函数，函数声明如下</p><p><code>class pycuda.reduction.ReductionKernel(dtype_out, neutral, reduce_expr, map_expr=None, arguments=None, name=&#39;reduce_kernel&#39;, keep=False, options=[], preamble=&#39;&#39;, allocator=None)</code></p><ul><li><code>dtype_out</code>: 指定返回数据类型</li><li><code>neutral</code>: 中性元，加法应为 0，乘法应为 1，本小节后面会详细讲解原因</li><li><code>reduce_expr</code>: 相当于<code>InclusiveScanKernel</code>函数的<code>reduce_expr</code></li><li><code>map_expr</code>: 相当于<code>ElementwiseKernel</code>函数的<code>operation</code></li><li>其余关键字同上</li></ul><p>下面的程序计算了两个向量的点积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.reduction <span class="keyword">import</span> ReductionKernel</span><br><span class="line"></span><br><span class="line">seq1 = np.array(<span class="built_in">range</span>(<span class="number">10</span>),dtype=np.float32)</span><br><span class="line">seq2 = np.array(<span class="built_in">range</span>(<span class="number">10</span>),dtype=np.float32)</span><br><span class="line">seq1_gpu = gpuarray.to_gpu(seq1)</span><br><span class="line">seq2_gpu = gpuarray.to_gpu(seq2)</span><br><span class="line">dot_prod = ReductionKernel(np.float32, neutral=<span class="string">&quot;0&quot;</span>, reduce_expr=<span class="string">&quot;a+b&quot;</span>, map_expr=<span class="string">&quot;x[i] * y[i]&quot;</span>,arguments=<span class="string">&quot;float *x, float *y&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.dot(seq1,seq2))</span><br><span class="line"><span class="built_in">print</span>(dot_prod(seq1_gpu, seq2_gpu).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line"><span class="number">285.0</span></span><br><span class="line"><span class="number">285.0</span></span><br></pre></td></tr></table></figure><h2 id="PyCUDA-SourceModule-并行函数"><a href="#PyCUDA-SourceModule-并行函数" class="headerlink" title="PyCUDA SourceModule 并行函数"></a>PyCUDA SourceModule 并行函数</h2><p><code>SourceModule</code>函数将原来的内联 CUDA C 代码编译成可以从 Python 启动的内核函数，与上述所有函数不同的是，<code>SourceModule</code>函数在<code>source</code>中直接编写 CUDA C 整体内核函数代码，函数声明如下</p><p><code>class pycuda.compiler.SourceModule(source, nvcc=&#39;nvcc&#39;, options=None, keep=False, no_extern_c=False, arch=None, code=None, cache_dir=None, include_dirs=[])</code></p><ul><li><code>source</code>: 内核函数代码，使用 CUDA C 编写</li><li><code>nvcc</code>: 编译器命令</li><li><code>no_extern_c</code>: 默认为 <code>False</code>，<code>source</code>会被包装在 extern “C” { … } 中以防止 C++ 名称重整，为<code>True</code>则不会被包装在 extern “C” { … } 中</li><li><code>arch</code>: 指定传递给 nvcc 命令上 <code>-arch</code> 选项的值</li><li><code>code</code>: 指定传递给 nvcc 命令上 <code>-code</code> 选项的值</li><li><code>cache_dir</code>: 指定用于编译器缓存的目录，默认为<code>PYCUDA_CACHE_DIR</code>所指目录，为<code>False</code>则禁用缓存，如果环境变量<code>PYCUDA_DISABLE_CACHE</code>被赋值，意为全局禁用缓存，<code>cache_dir</code>将被覆盖</li><li><code>include_dirs</code>: 将一个或多个外部类传递给<code>source</code>，类型为 string list</li><li>其余关键字同上</li></ul><p>需要使用<code>get_function</code>得到已编译好的内核函数的引用，才可使用内核函数，下例程序计算了矩阵乘法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda</span><br><span class="line"><span class="keyword">import</span> pycuda.tools</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> la</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"> </span><br><span class="line">mod = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void matrixMultiply(float * A, float * B, float * C,</span></span><br><span class="line"><span class="string">                   int A_shape_0,int A_shape_1,int B_shape_1) &#123;</span></span><br><span class="line"><span class="string">    float cValue = 0;</span></span><br><span class="line"><span class="string">    int Row = blockIdx.y * blockDim.y + threadIdx.y;</span></span><br><span class="line"><span class="string">    int Col = blockIdx.x * blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">    if ((Row &lt; A_shape_0) &amp;&amp; (Col &lt; B_shape_1)) &#123;</span></span><br><span class="line"><span class="string">        for (int k = 0; k &lt; A_shape_1; k++) &#123;</span></span><br><span class="line"><span class="string">            cValue += A[Row*A_shape_1 + k] * B[k*B_shape_1 + Col];</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        C[Row*B_shape_1 + Col] = cValue;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">matrixMultiply = mod.get_function(<span class="string">&quot;matrixMultiply&quot;</span>)</span><br><span class="line">n = <span class="number">1000</span></span><br><span class="line">A = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1000000</span>).reshape(<span class="number">1000</span>,<span class="number">1000</span>).astype(np.float32)</span><br><span class="line">B = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1000000</span>).reshape(<span class="number">1000</span>,<span class="number">1000</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">BLOCK_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">A_gpu = gpuarray.to_gpu(A)</span><br><span class="line">B_gpu = gpuarray.to_gpu(B)</span><br><span class="line">C_gpu = gpuarray.empty_like(A_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n % BLOCK_SIZE != <span class="number">0</span>:</span><br><span class="line">    grid=(n // BLOCK_SIZE+<span class="number">1</span>, n // BLOCK_SIZE+<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    grid=(n // BLOCK_SIZE, n // BLOCK_SIZE,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;grid size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid))</span><br><span class="line">start = time.time()</span><br><span class="line">matrixMultiply(A_gpu, B_gpu, C_gpu, np.int32(A.shape[<span class="number">0</span>]),np.int32(A.shape[<span class="number">1</span>]),np.int32(B.shape[<span class="number">1</span>]), block=(BLOCK_SIZE,BLOCK_SIZE,<span class="number">1</span>), grid=grid);</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;result:\n&quot;</span>, C_gpu.get())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;time cost: %ss&quot;</span> %(time.time() - start))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">grid size: (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">result:</span><br><span class="line"> [[<span class="number">19305.</span> <span class="number">18274.</span> <span class="number">18786.</span> ... <span class="number">19130.</span> <span class="number">19501.</span> <span class="number">18229.</span>]</span><br><span class="line"> [<span class="number">19945.</span> <span class="number">19823.</span> <span class="number">20158.</span> ... <span class="number">19853.</span> <span class="number">20397.</span> <span class="number">19848.</span>]</span><br><span class="line"> [<span class="number">20651.</span> <span class="number">19959.</span> <span class="number">20910.</span> ... <span class="number">20715.</span> <span class="number">21033.</span> <span class="number">20364.</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">20196.</span> <span class="number">19777.</span> <span class="number">20198.</span> ... <span class="number">20291.</span> <span class="number">20736.</span> <span class="number">19916.</span>]</span><br><span class="line"> [<span class="number">20239.</span> <span class="number">19820.</span> <span class="number">20210.</span> ... <span class="number">20171.</span> <span class="number">20685.</span> <span class="number">19244.</span>]</span><br><span class="line"> [<span class="number">19595.</span> <span class="number">19444.</span> <span class="number">19546.</span> ... <span class="number">19862.</span> <span class="number">20462.</span> <span class="number">19508.</span>]]</span><br><span class="line">time cost: <span class="number">0.003673076629638672</span>s</span><br></pre></td></tr></table></figure><h2 id="并行前缀算法"><a href="#并行前缀算法" class="headerlink" title="并行前缀算法"></a>并行前缀算法</h2><p>前面提到的<code>InclusiveScanKernel</code>函数和函数就是并行扫描算法之一，并行扫描算法定义如下</p><p>定义二元运算符 $\oplus$ 和一个有 $n$ 个元素的数组 $[x<em>0, x_1, x_2, \cdots, x</em>{n-1}]$</p><p>返回</p><script type="math/tex; mode=display">[x_0, (x_0 \oplus x_1), (x_0 \oplus x_1 \oplus x_2), \cdots, (x_0 \oplus x_1 \oplus \cdots \oplus x_{n-1})]</script><p>以加法举例，输入为 <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code>，返回为<code>[ 0  1  3  6 10 15 21 28 36 45]</code>，如果用串行的计算方式，需要计算 $n$ 次求和，时间复杂度为$O(n)$，我们要尽量降低时间复杂度</p><h3 id="朴素并行前缀算法"><a href="#朴素并行前缀算法" class="headerlink" title="朴素并行前缀算法"></a>朴素并行前缀算法</h3><p>朴素并行扫描算法是并行扫描算法的初始版本，我们假设输入元素为 n 个，且 n 为 2 的次幂，同时在 n 个线程上并行计算，时间复杂度为 $O(\log_{2} n)$，下图为算法示意图（<a href="https://www.geeksforgeeks.org/hillis-steele-scan-parallel-prefix-scan-algorithm/">原图链接</a>）</p><p><img src="/image/CUDA编程-PyCUDA编程简介/1.png" alt=""></p><p>下面的程序以加法为例，计算了长度为 1024 的数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">naive_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void naive_scan(double *vec, double *out) &#123;</span></span><br><span class="line"><span class="string">     __shared__ double sum_buf[1024];     </span></span><br><span class="line"><span class="string">     int tid = threadIdx.x;</span></span><br><span class="line"><span class="string">     sum_buf[tid] = vec[tid];</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     // begin parallel prefix sum algorithm</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int iter = 1;</span></span><br><span class="line"><span class="string">     for (int i=0; i &lt; 10; i++) &#123;</span></span><br><span class="line"><span class="string">         __syncthreads();</span></span><br><span class="line"><span class="string">         if (tid &gt;= iter ) &#123;</span></span><br><span class="line"><span class="string">             sum_buf[tid] = sum_buf[tid] + sum_buf[tid - iter];</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         iter *= 2;</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">    __syncthreads();</span></span><br><span class="line"><span class="string">    out[tid] = sum_buf[tid];</span></span><br><span class="line"><span class="string">    __syncthreads();</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">naive_gpu = naive_ker.get_function(<span class="string">&quot;naive_scan&quot;</span>)</span><br><span class="line"></span><br><span class="line">testvec = np.random.randn(<span class="number">1024</span>).astype(np.float64)</span><br><span class="line">testvec_gpu = gpuarray.to_gpu(testvec)</span><br><span class="line">    </span><br><span class="line">outvec_gpu = gpuarray.empty_like(testvec_gpu)</span><br><span class="line">naive_gpu( testvec_gpu , outvec_gpu, block=(<span class="number">1024</span>,<span class="number">1</span>,<span class="number">1</span>), grid=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">total_sum = <span class="built_in">sum</span>( testvec)</span><br><span class="line">total_sum_gpu = outvec_gpu[-<span class="number">1</span>].get()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Does our kernel work correctly? : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(total_sum_gpu , total_sum)))</span><br></pre></td></tr></table></figure><p>其中使用了<code>__syncthreads()</code>函数，保证了线程块中的所有线程在进入下一次迭代前，当前迭代里每一个线程的所有部分都被保存在了全局内存中</p><h3 id="高效并行前缀算法"><a href="#高效并行前缀算法" class="headerlink" title="高效并行前缀算法"></a>高效并行前缀算法</h3><p>上一例的算法只能计算长度为 1024 的数组进行计算，且需要与长度等量的线程数量，接下来我们实现支持长度超过 1024 的任意数组的算法，此算法需要两个内核函数和 python 函数，分加进行上行扫描和下行扫描。上行扫描阶段类似于一次规约操作，即$x<em>0 \oplus \cdots \oplus x</em>{n-1}$，下行扫描阶段将对上行扫描的返回值进行计算，返回最终结果，具体代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># this is a work-efficent parallel prefix-sum algorithm.</span></span><br><span class="line"><span class="comment"># written by Brian Tuomanen for &quot;Hands On GPU Programming with Python and CUDA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel for up-sweep phase</span></span><br><span class="line">up_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void up_ker(double *x, double *x_old, int k ) &#123;</span></span><br><span class="line"><span class="string">     int tid =  blockIdx.x*blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int _2k = 1 &lt;&lt; k;</span></span><br><span class="line"><span class="string">     int _2k1 = 1 &lt;&lt; (k+1);</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int j = tid* _2k1;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     x[j + _2k1 - 1] = x_old[j + _2k -1 ] + x_old[j + _2k1 - 1];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">up_gpu = up_ker.get_function(<span class="string">&quot;up_ker&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># implementation of up-sweep phase</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">up_sweep</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># let&#x27;s typecast to be safe.</span></span><br><span class="line">    x = np.float64(x)</span><br><span class="line">    x_gpu = gpuarray.to_gpu(np.float64(x) )</span><br><span class="line">    x_old_gpu = x_gpu.copy()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>( <span class="built_in">int</span>(np.log2(x.size) ) ) : </span><br><span class="line">        num_threads = <span class="built_in">int</span>(np.ceil( x.size / <span class="number">2</span>**(k+<span class="number">1</span>)))</span><br><span class="line">        grid_size = <span class="built_in">int</span>(np.ceil(num_threads / <span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            block_size = <span class="number">32</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            block_size = num_threads</span><br><span class="line">            </span><br><span class="line">        up_gpu(x_gpu, x_old_gpu, np.int32(k)  , block=(block_size,<span class="number">1</span>,<span class="number">1</span>), grid=(grid_size,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        x_old_gpu[:] = x_gpu[:]</span><br><span class="line">        </span><br><span class="line">    x_out = x_gpu.get()</span><br><span class="line">    <span class="keyword">return</span>(x_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel for down-sweep phase</span></span><br><span class="line">down_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void down_ker(double *y, double *y_old,  int k) &#123;</span></span><br><span class="line"><span class="string">     int tid =  blockIdx.x*blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int _2k = 1 &lt;&lt; k;</span></span><br><span class="line"><span class="string">     int _2k1 = 1 &lt;&lt; (k+1);</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int j = tid*_2k1;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     y[j + _2k - 1 ] = y_old[j + _2k1 - 1];</span></span><br><span class="line"><span class="string">     y[j + _2k1 - 1] = y_old[j + _2k1 - 1] + y_old[j + _2k - 1];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">down_gpu = down_ker.get_function(<span class="string">&quot;down_ker&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># implementation of down-sweep phase</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_sweep</span>(<span class="params">y</span>):</span><br><span class="line">    y = np.float64(y)</span><br><span class="line">    y[-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y_gpu = gpuarray.to_gpu(y)</span><br><span class="line">    y_old_gpu = y_gpu.copy()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">int</span>(np.log2(y.size)))):</span><br><span class="line">        num_threads = <span class="built_in">int</span>(np.ceil( y.size / <span class="number">2</span>**(k+<span class="number">1</span>)))</span><br><span class="line">        grid_size = <span class="built_in">int</span>(np.ceil(num_threads / <span class="number">32</span>))</span><br><span class="line">        <span class="keyword">if</span> grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            block_size = <span class="number">32</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            block_size = num_threads</span><br><span class="line">        down_gpu(y_gpu, y_old_gpu, np.int32(k), block=(block_size,<span class="number">1</span>,<span class="number">1</span>), grid=(grid_size,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        y_old_gpu[:] = y_gpu[:]</span><br><span class="line">    y_out = y_gpu.get()</span><br><span class="line">    <span class="keyword">return</span>(y_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># full implementation of work-efficient parallel prefix sum</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficient_prefix</span>(<span class="params">x</span>):</span><br><span class="line"><span class="keyword">return</span>(down_sweep(up_sweep(x)))</span><br><span class="line"></span><br><span class="line">testvec = np.random.randn(<span class="number">32</span>*<span class="number">1024</span>).astype(np.float64)</span><br><span class="line">testvec_gpu = gpuarray.to_gpu(testvec)</span><br><span class="line"></span><br><span class="line">outvec_gpu = gpuarray.empty_like(testvec_gpu)</span><br><span class="line"></span><br><span class="line">prefix_sum = np.roll(np.cumsum(testvec), <span class="number">1</span>)</span><br><span class="line">prefix_sum[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">prefix_sum_gpu = efficient_prefix(testvec)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Does our work-efficient prefix work? &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(prefix_sum_gpu, prefix_sum)))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA C编程简介</title>
      <link href="/2023/12/19/CUDA%E7%BC%96%E7%A8%8B-CUDA-C%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/12/19/CUDA%E7%BC%96%E7%A8%8B-CUDA-C%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>讲解 CUDA C 编程中的简单的内存管理，线程操作，如何编写核函数，使用 Thrust 库，并行计算，性能分析工具。</p><span id="more"></span><h2 id="获取-GPU-信息"><a href="#获取-GPU-信息" class="headerlink" title="获取 GPU 信息"></a>获取 GPU 信息</h2><p>CUDA  提供了几种获取 GPU 信息的方法，这里介绍一下通过调用 <code>cuda_runtime.h</code>中的 API 得到 GPU 的一些属性。</p><blockquote><p>在编写 CUDA C 程序时， 要将文件命名为 <code>*.cu</code>，一般使用 nvcc 命令编译运行，为 CUDA程序文件，支持 C/C++ 语法。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp devProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;devProp, dev);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;GPU Device Name&quot;</span> &lt;&lt; dev &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; devProp.name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;SM Count: &quot;</span> &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Shared Memory Size per Thread Block: &quot;</span> &lt;&lt; devProp.sharedMemPerBlock / <span class="number">1024.0</span> &lt;&lt; <span class="string">&quot; KB&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per Thread Block: &quot;</span> &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Warps per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor / <span class="number">32</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc checkDeviceInfor.cu -o checkDeviceInfor</span><br></pre></td></tr></table></figure><p>输出如下，SM 数量为 30，每个线程块的共享内存为 48KB，每个线程块有 1024 个线程，每个 SM 有 1536 个线程，每个 SM 有 48 个线程束</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU Device Name0: NVIDIA GeForce RTX 3060 Laptop GPU</span><br><span class="line">SM Count: 30</span><br><span class="line">Shared Memory Size per Thread Block: 48 KB</span><br><span class="line">Threads per Thread Block: 1024</span><br><span class="line">Threads per SM: 1536</span><br><span class="line">Warps per SM: 48</span><br></pre></td></tr></table></figure><h2 id="初步内存管理"><a href="#初步内存管理" class="headerlink" title="初步内存管理"></a>初步内存管理</h2><p>主机和设备各自拥有独立的内存，C 拥有标准库可以管理主机的内存，CUDA 提供的 API 管理设备的内存，下面是 C 和 CUDA 的部分内存管理函数</p><div class="table-container"><table><thead><tr><th>C</th><th>CUDA</th><th>功能</th></tr></thead><tbody><tr><td>malloc</td><td>cudaMalloc</td><td>分配内存</td></tr><tr><td>memcpy</td><td>cudaMemcpy</td><td>复制内存</td></tr><tr><td>memset</td><td>cudaMemset</td><td>设置内存</td></tr><tr><td>free</td><td>cudaFree</td><td>释放内存</td></tr></tbody></table></div><h3 id="主机与设备的数据拷贝"><a href="#主机与设备的数据拷贝" class="headerlink" title="主机与设备的数据拷贝"></a>主机与设备的数据拷贝</h3><p>下面的程序举例了如何使用进行主机与设备的数据拷贝，使用了 <code>cudaMalloc</code>，<code>cudaMemcpy</code> 和 <code>cudaFree</code> 函数，函数形参如下</p><ul><li><p><code>__host__ cudaError_t cudaMalloc (void** devPtr, size_t size)</code></p><ul><li><code>devPtr</code>: 开辟数据的首指针</li><li><code>size</code>: 开辟的设备内存空间长度</li></ul></li><li><p><code>__host__ cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)</code></p><ul><li><code>dst</code>: 目的数据内存首指针</li><li><code>src</code>: 源数据首指针</li><li><code>count</code>: 数据长度</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li><li><p><code>__host__ cudaError_t cudaFree (void* devPtr)</code></p></li><li><code>devPtr</code>: 设备变量指针</li></ul><p>上述函数的返回值类型都是 <code>cudaError_t</code>，以枚举形式保存各种错误类型</p><p>更多运行时函数详解见<a href="https://docs.nvidia.com/cuda/cuda-runtime-api/">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;dev_dets, <span class="built_in">sizeof</span>(dets));</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(dev_dets, dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(&amp;host_dets, dev_dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>);i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>上面的程序使用<code>cudaMalloc</code>来申请设备内存，但二维数组不推荐这么做，在 kernel 运算时较高的性能损失，CUDA 给出了二维数组专用的内存申请函数<code>cudaMallocPitch</code>，在设备间内存拷贝时，也要使用<code>cudaMemcpy2D</code>函数，形参如下</p><ul><li><code>__host__cudaError_t cudaMallocPitch ( void** devPtr, size_t* pitch, size_t width, size_t height )</code><ul><li><code>devPtr</code>: 开辟矩阵的数据的首指针</li><li><code>pitch</code>: 分配存储器的宽度</li><li><code>width</code>: 二维数组列数</li><li><code>height</code>: 二维数组行数</li></ul></li><li><code>__host__ cudaError_t cudaMemcpy2D ( void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind )</code><ul><li><code>dst</code>: 目的矩阵内存首指针</li><li><code>dpitch</code>:  dst指向的 2D 数组中的内存宽度，以字节为单位，是cuda为了读取方便，对齐过的内存宽度，可能大于一行元素占据的实际内存</li><li><code>src</code>: 源矩阵内存首指针</li><li><code>spitch</code>: src 指向的 2D 数组中的内存宽度</li><li><code>width</code>: src指向的2D数组中一行元素占据的实际宽度，为 <code>width*sizeof(type)</code></li><li><code>height</code>: src指向的2D数组的行数</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="type">size_t</span> width = <span class="number">4</span>;</span><br><span class="line">    <span class="type">size_t</span> height = <span class="number">6</span>;</span><br><span class="line">    <span class="type">size_t</span> pitch;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMallocPitch</span>((<span class="type">void</span> **)&amp;dev_dets, &amp;pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height);</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(dev_dets, pitch, dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(&amp;host_dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, dev_dets, pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;width*height;i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>这两个函数应该会使 kernel 的运行时间变短，因为 pitch 对齐后可实现 global 内存联合访问，但<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>会变慢，因为比一维的操作多了对齐的考虑</p><h2 id="Kernel-函数"><a href="#Kernel-函数" class="headerlink" title="Kernel 函数"></a>Kernel 函数</h2><h3 id="kernel-限定词"><a href="#kernel-限定词" class="headerlink" title="kernel 限定词"></a>kernel 限定词</h3><ul><li><code>__device__</code>: 在设备上执行，只能在设备上调用；</li><li><code>__global__</code>: 在设备上执行，只能在主机上调用；</li><li><code>__host__</code>: 在主机上执行，只能在主机上调用。</li></ul><p><code>__device__</code>和<code>__global__</code>代表函数在设备上执行，不支持递归，不能在函数体内声明静态变量，静态变量对应于CPU的整个程序生命过程，不能有可变长参数；</p><p><code>__global__</code>和<code>__host__</code>不能一起使用，而<code>__device__</code>和<code>__host__</code>可以一起使用，编译器会在 CPU 和 GPU 各复制一份函数。</p><p>不添加限定词时，函数默认为<code>__host__</code>，也就是在主机上执行。</p><p>所有的 kernel 函数返回类型都是 void，且 kernel 函数都是异步执行。</p><h3 id="kernel-调用方式"><a href="#kernel-调用方式" class="headerlink" title="kernel 调用方式"></a>kernel 调用方式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_func</span> <span class="params">(param list)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line">kernel_func &lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt; (param list);</span><br></pre></td></tr></table></figure><ul><li><code>&lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt;</code>: 是运算符内是核函数的执行参数，告诉编译器运行时如何启动核函数</li><li><code>Dg</code>: grid 的维度和尺寸，dim3 类型，意为一个 grid 有多少个 block</li><li><code>Db</code>: block 的维度和尺寸， dim3 类型，意为一个 block 有多少个 thread</li><li><code>Ns</code>: （可选）用于设置每个block除了静态分配的 shared Memory 以外，最多能动态分配的 shared Memory 大小，单位为 byte 不需要动态分配时该值为0或省略不写</li><li><code>S</code>: （可选） cudastream 类型的参数，表示该核函数处在哪个流之中</li></ul><p>这里我们实现一下第二章最后的例子，下面的程序使用了<code>cudaDeviceSynchronize</code>和<code>cudaDeviceReset</code>函数，解释如下</p><ul><li><code>__host__ __device__ cudaDeviceSynchronize</code>: 使设备阻塞到完成所有前面请求的任务，CUDA 11.6 后已弃用</li><li><code>__host__ cudaDeviceReset</code>: 显式销毁并清除当前进程中与设备关联的所有资源，资源不能再被访问，可能导致未定义的行为</li></ul><p>由于 CUDA printf 的输出存储在缓冲中，后台同步机制会有延时，需要使用上面两个同步函数中任意一个使 printf 函数的内容与主机同步，即可输出</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">printThreadIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy*blockDim.x * gridDim.x + ix;</span><br><span class="line">    <span class="keyword">if</span>(threadIdx.x == <span class="number">3</span> &amp;&amp; threadIdx.y == <span class="number">1</span> &amp;&amp; blockIdx.x == <span class="number">0</span> &amp;&amp; blockIdx.y == <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;thread_id (%d,%d) block_id (%d,%d) coordinate (%d, %d), global index %2d \n&quot;</span>, threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, ix, iy, idx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    printThreadIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    <span class="comment">// cudaDeviceSynchronize(); </span></span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="built_in">thread_id</span> (<span class="number">3</span>,<span class="number">1</span>) <span class="built_in">block_id</span> (<span class="number">0</span>,<span class="number">1</span>) <span class="built_in">coordinate</span> (<span class="number">3</span>, <span class="number">3</span>), global index <span class="number">27</span></span><br></pre></td></tr></table></figure><blockquote><p>在输出时不能使用 std::cout, std 命名空间不能使用到 GPU 上</p></blockquote><h2 id="CUDA-的-Thrust-库"><a href="#CUDA-的-Thrust-库" class="headerlink" title="CUDA 的 Thrust 库"></a>CUDA 的 Thrust 库</h2><p>CUDA 的 Thrust 库是基于标准模板库 STL 的 CUDA 的 C++ 模板库， 通过与 CUDA C 配合使用，节省了大量优化算法的时间，保证了性能与开发效率，在 CUDA Toolkit 中包含 Thrust，无需额外安装，只需导入相应头文件，在调用时使用 <code>thrust</code> 命名空间，并尽量不要使用 <code>using namespace std;</code> 语句，因为 thrust 库和 STL 库非常多的重名</p><h3 id="Vector-容器"><a href="#Vector-容器" class="headerlink" title="Vector 容器"></a>Vector 容器</h3><p>Thrust 中定义了主机端和设备端的两种 vector，分别定义在 host_vector.h 和 device_vector.h 中，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// H has storage for 4 integers</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(<span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize individual elements</span></span><br><span class="line">    H[<span class="number">0</span>] = <span class="number">14</span>;</span><br><span class="line">    H[<span class="number">1</span>] = <span class="number">20</span>;</span><br><span class="line">    H[<span class="number">2</span>] = <span class="number">38</span>;</span><br><span class="line">    H[<span class="number">3</span>] = <span class="number">46</span>;</span><br><span class="line">    H.<span class="built_in">push_back</span>(<span class="number">52</span>);</span><br><span class="line">    <span class="comment">// H.size() returns the size of vector H</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// print contents of H</span></span><br><span class="line">    <span class="comment">// for(int i = 0; i &lt; H.size(); i++)</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:H)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// resize H</span></span><br><span class="line">    H.<span class="built_in">resize</span>(<span class="number">2</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H now has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// Copy host_vector H to device_vector D</span></span><br><span class="line">    thrust::device_vector&lt;<span class="type">int</span>&gt; D = H; </span><br><span class="line">    <span class="comment">// elements of D can be modified</span></span><br><span class="line">    D[<span class="number">0</span>] = <span class="number">99</span>;</span><br><span class="line">    D[<span class="number">1</span>] = <span class="number">88</span>;</span><br><span class="line">    <span class="comment">// print contents of D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// H and D are automatically deleted when the function returns</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">H has size <span class="number">5</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">38</span></span><br><span class="line"><span class="number">46</span></span><br><span class="line"><span class="number">52</span></span><br><span class="line">H now has size <span class="number">2</span></span><br><span class="line"><span class="number">99</span></span><br><span class="line"><span class="number">88</span></span><br></pre></td></tr></table></figure><p>Thrust 允许使用 <code>=</code> 运算符对 <code>host_vector</code> 和 <code>device_vector</code> 的相互拷贝，也允许使用 <code>[i]</code> 下标访问 <code>device_vector</code> 的各个元素，但是用这种方法访问每一次都需要调用 <code>cudaMemcpy</code>，性能损失较大，应谨慎使用。 下面我们将介绍一些更有效的技术</p><p>下面展示 Thrust 提供的几种对 vector 操作的方法，包括初始化，赋值，<code>iterator</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize all ten integers of a device_vector to 1</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">D</span><span class="params">(<span class="number">10</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the first seven elements of a vector to 9</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">begin</span>() + <span class="number">7</span>, <span class="number">9</span>);</span><br><span class="line">    <span class="comment">// initialize a host_vector with the first five elements of D</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(D.begin(), D.begin() + <span class="number">5</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the elements of H to 0, 1, 2, 3, ...</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// copy all of H back to the beginning of D</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>(), D.<span class="built_in">begin</span>());</span><br><span class="line">    <span class="comment">// print D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)dd</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">9</span> <span class="number">9</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>上面的程序使用了<code>thrust::fill</code>，当它对 <code>device_vector iterator</code> 操作时，会在编译时检查 <code>iterator</code> 在主机上还是在设备上，这个过程被称为静态调度，意味着调度过程没有运行时开销</p><h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><p>thrust 中定义了 <code>device_ptr</code> 数据类型，当传入函数的指针指向设备端内存时，需要用<code>device_ptr</code>进行封装</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> N = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// raw pointer to device memory</span></span><br><span class="line"><span class="type">int</span> * raw_ptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;raw_ptr, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"><span class="comment">// wrap raw pointer with a device_ptr </span></span><br><span class="line"><span class="function">thrust::device_ptr&lt;<span class="type">int</span>&gt; <span class="title">dev_ptr</span><span class="params">(raw_ptr)</span></span>;</span><br><span class="line"><span class="comment">// use device_ptr in thrust algorithms</span></span><br><span class="line">thrust::<span class="built_in">fill</span>(dev_ptr, dev_ptr + N, (<span class="type">int</span>) <span class="number">0</span>);</span><br></pre></td></tr></table></figure><h3 id="数值操作"><a href="#数值操作" class="headerlink" title="数值操作"></a>数值操作</h3><h4 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h4><p>Transformations 是对一个输入范围中的每一个元素应用操作，将结果存储在给定范围中的方法，上面程序中已经 <code>thrust::fill</code> 就是一个 Transformations，它将范围内的所有元素设置为指定值。下面的程序用到了 <code>thrust::sequence</code>，<code>thrust::replace</code>，<code>thrust::transform</code>，更多 Transformations 请查看<a href="https://thrust.github.io/doc/group__transformations.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/replace.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// allocate three device_vectors with 10 elements</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">X</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Y</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Z</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize X to 0,1,2,3, ....</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// compute Y = -X</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">negate</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// fill Z with twos</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(Z.<span class="built_in">begin</span>(), Z.<span class="built_in">end</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="comment">// compute Y = X mod 2</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Z.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">modulus</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// replace all the ones in Y with tens</span></span><br><span class="line">    thrust::<span class="built_in">replace</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), <span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="comment">// print Y</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), std::<span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(std::cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> </span><br></pre></td></tr></table></figure><h4 id="SAXPY"><a href="#SAXPY" class="headerlink" title="SAXPY"></a>SAXPY</h4><p>SAXPY（Scalar Alpha X Plus Y）是一个在 BLAS（Basic Linear Algebra Subprograms）函数库提供中的函数，并且是一个并行向量处理机（vector processor）中常用的计算操作指令，为标量乘法和向量加法的组合，如 $y = a*x + y$，其中 $x$ 和 $y$ 为向量，$a$ 为标量常数。下面的程序定义了一个 functor 实现 SAXPY</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">saxpy_functor</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> a;</span><br><span class="line">    <span class="built_in">saxpy_functor</span>(<span class="type">float</span> _a) : <span class="built_in">a</span>(_a) &#123;&#125;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        <span class="type">float</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> <span class="type">float</span>&amp; x, <span class="type">const</span> <span class="type">float</span>&amp; y)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> a * x + y;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">saxpy</span><span class="params">(<span class="type">float</span> A, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; X, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; Y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// y = a * x + y</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), <span class="built_in">saxpy_functor</span>(A));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Reductions"><a href="#Reductions" class="headerlink" title="Reductions"></a>Reductions</h4><p>使用 <code>thrust::reduce</code> 函数对一组数据进行操作，返回值为一个具体数值，下例就是对一组数据求和</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>, thrust::<span class="built_in">plus</span>&lt;<span class="type">int</span>&gt;());</span><br></pre></td></tr></table></figure><p>上列中<code>(int) 0</code>为计算的初始值，<code>thrust::plus&lt;int&gt;()</code>为操作符，当没有定义初始值和操作符时，它们是默认值，因此下面的两条语句和上面的等价，更多操作符请查看<a href="https://nvidia.github.io/thrust/api/groups/group__reductions.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>);</span><br><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p><code>thrust::transform_reduce</code>允许接受多个操作符来对一组数据求值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform_reduce.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// square&lt;T&gt; computes the square of a number f(x) -&gt; x*x</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">square</span> &#123;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        T <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> T&amp; x)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> x * x;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize host array</span></span><br><span class="line">    <span class="type">float</span> x[<span class="number">4</span>] = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;;</span><br><span class="line">    <span class="comment">// transfer to device</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">float</span>&gt; <span class="title">d_x</span><span class="params">(x, x + <span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// setup arguments</span></span><br><span class="line">    square&lt;<span class="type">float</span>&gt; unary_op;</span><br><span class="line">    thrust::plus&lt;<span class="type">float</span>&gt; binary_op;</span><br><span class="line">    <span class="type">float</span> init = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// compute norm</span></span><br><span class="line">    <span class="type">float</span> norm = std::<span class="built_in">sqrt</span>( thrust::<span class="built_in">transform_reduce</span>(d_x.<span class="built_in">begin</span>(), d_x.<span class="built_in">end</span>(), unary_op, init, binary_op));</span><br><span class="line">    std::cout &lt;&lt; norm &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">5.47723</span></span><br></pre></td></tr></table></figure><p>上面的程序对一组数据计算平方和再开方，这种写法会大大优化性能。</p><h4 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h4><p>对数据进行排序，很常用的排序功能，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sort.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort</span>(A, A + N);</span><br><span class="line"><span class="comment">// A is now &#123;1, 2, 4, 5, 7, 8&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span>    keys[N] = &#123;  <span class="number">1</span>,   <span class="number">4</span>,   <span class="number">2</span>,   <span class="number">8</span>,   <span class="number">5</span>,   <span class="number">7</span>&#125;;</span><br><span class="line"><span class="type">char</span> values[N] = &#123;<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort_by_key</span>(keys, keys + N, values);</span><br><span class="line"><span class="comment">// keys is now   &#123;  1,   2,   4,   5,   7,   8&#125;</span></span><br><span class="line"><span class="comment">// values is now &#123;&#x27;a&#x27;, &#x27;c&#x27;, &#x27;b&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;d&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">stable_sort</span>(A, A + N, thrust::<span class="built_in">greater</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line"><span class="comment">// A is now &#123;8, 7, 5, 4, 2, 1&#125;</span></span><br></pre></td></tr></table></figure><p>上例中的 <code>thrust::stable_sort</code>接受用户自定义比较运算符</p><h4 id="max-element-min-element"><a href="#max-element-min-element" class="headerlink" title="max_element(min_element)"></a>max_element(min_element)</h4><p>求最大（小）值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/extrema.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line">thrust::device_vector&lt;type&gt;::iterator iter = thrust::<span class="built_in">max_element</span>(dvec.<span class="built_in">begin</span>()，dvec.<span class="built_in">end</span>());</span><br><span class="line"><span class="type">int</span> position = iter - dvec.<span class="built_in">begin</span>();</span><br><span class="line">type max_val = *iter;</span><br></pre></td></tr></table></figure><p>其返回值是一个迭代器，需要获取最大（小）值所在位置，再得到结果</p><h4 id="unique"><a href="#unique" class="headerlink" title="unique"></a>unique</h4><p>将一组数据中满足条件的数据筛选出来，可自定义筛选条件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/unique.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_same</span> &#123;</span><br><span class="line"><span class="function">__host__ __device__</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> float3 &amp;p1, <span class="type">const</span> float3 &amp;p2)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">return</span> (p1.x==p2.x) &amp;&amp; (p1.y==p2.y) &amp;&amp; (p1.z==p2.z);</span><br><span class="line">&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_same</span>()),p.<span class="built_in">end</span>();</span><br><span class="line">p.<span class="built_in">erase</span>(thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_sam</span>()),p.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p>unique 函数的功能只是将满足条件的数据筛选出来，无法直接删除，需要结合 vector 的 erase 函数进行删除</p><h2 id="建立-CUDA-的并行线程计算"><a href="#建立-CUDA-的并行线程计算" class="headerlink" title="建立 CUDA 的并行线程计算"></a>建立 CUDA 的并行线程计算</h2><p>下面的程序为大家演示以结构体类型存储的矩阵计算，后续章节会教大家使用 cuBLAS 库进行并行计算</p><h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p>下面的程序进行了 $C = A + B$ 矩阵加法运算，下面的程序中使用了<code>cudaMallocManaged</code>函数，简单来说，就是结合了之前讲到的<code>cudaMalloc</code>和<code>cudaMemcpy</code>等内存迁移拷贝的操作，自动内存管理，方便代码编写，弊端是在 kernel 执行时会降低 kernel 的执行效率，在后续章节，我们会详细讲解有关 CUDA 的内存管理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="type">int</span> w;</span><br><span class="line">    <span class="type">int</span> h;</span><br><span class="line">    <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">        A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>下面的程序进行了 $C = A * B$ 矩阵乘法运算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMul</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">float</span> k = <span class="number">0.0</span>;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;A-&gt;w; i++)</span><br><span class="line">                k += <span class="built_in">getValue</span>(A, row, i) * <span class="built_in">getValue</span>(B, i, col);</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="为运行程序计时"><a href="#为运行程序计时" class="headerlink" title="为运行程序计时"></a>为运行程序计时</h2><h3 id="nvprof"><a href="#nvprof" class="headerlink" title="nvprof"></a>nvprof</h3><p>nvprof 是过去比较常用的命令行工具，但在终端直接输入<code>nvprof ./*.o</code>会得到以下 Warning</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.</span><br><span class="line">                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.</span><br><span class="line">                  Refer https://developer.nvidia.com/tools-overview for more details.</span><br></pre></td></tr></table></figure><p>目前主流的 CUDA 驱动不再支持<code>nvprof</code>命令，但我们仍可以在 NVIDIA Nsight Systems 中使用，在终端输入 <code>nsys nvprof ./*.o</code>就可以看到CUDA 程序执行的具体内容</p><p>这里我们以主机与设备的数据拷贝的两个程序为例</p><p>使用<code>cudaMalloc</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 1d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-01f4.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report5.nsys-rep</span><br><span class="line">[2/7] [========================100%] report5.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)     Name   </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------</span><br><span class="line">     99.9        137542088          1  137542088.0  137542088.0  137542088  137542088          0.0  cudaMalloc</span><br><span class="line">      0.1           163239          1     163239.0     163239.0     163239     163239          0.0  cudaFree  </span><br><span class="line">      0.0            36460          2      18230.0      18230.0      18070      18390        226.3  cudaMemcpy</span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     51.6             1504      1    1504.0    1504.0      1504      1504          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     48.4             1408      1    1408.0    1408.0      1408      1408          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report5.nsys-rep</span><br><span class="line">    /root/report5.sqlite</span><br></pre></td></tr></table></figure><p>使用<code>cudaMallocPitch</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 2d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-6614.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report6.nsys-rep</span><br><span class="line">[2/7] [========================100%] report6.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)       Name      </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------</span><br><span class="line">    100.0        745692893          1  745692893.0  745692893.0  745692893  745692893          0.0  cudaMallocPitch</span><br><span class="line">      0.0           161820          1     161820.0     161820.0     161820     161820          0.0  cudaFree       </span><br><span class="line">      0.0            39090          2      19545.0      19545.0      16590      22500       4179.0  cudaMemcpy2D   </span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     64.8             2880      1    2880.0    2880.0      2880      2880          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     35.2             1567      1    1567.0    1567.0      1567      1567          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report6.nsys-rep</span><br><span class="line">    /root/report6.sqlite</span><br></pre></td></tr></table></figure><p>这里我们可以看到<code>Total Time</code>中<code>cudaMallocPitch</code>函数用时几乎是<code>cudaMalloc</code>的 5 倍，更加肯定了我们的说法，<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>需要额外对二维数据进行对齐操作</p><h3 id="cudaEvent-计时函数"><a href="#cudaEvent-计时函数" class="headerlink" title="cudaEvent 计时函数"></a>cudaEvent 计时函数</h3><p>以前面的矩阵乘法为例，分别计算 CUDA 开辟内存时间和矩阵乘法运算时间</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;cudaMalloc cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Matrix multiplication cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">cudaMalloc cost: <span class="number">0.161696</span>s</span><br><span class="line">Matrix multiplication cost: <span class="number">0</span>s</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: CUDA模型概述</title>
      <link href="/2023/12/19/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/"/>
      <url>/2023/12/19/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>讨论 GPU 的并行计算是如何在硬件上实现的，CUDA 中的模块理解以及 CPU和 GPU 之间的交互，指令的同步。</p><span id="more"></span><h2 id="GPU-架构与异构并行计算"><a href="#GPU-架构与异构并行计算" class="headerlink" title="GPU 架构与异构并行计算"></a>GPU 架构与异构并行计算</h2><h3 id="什么是异构并行计算"><a href="#什么是异构并行计算" class="headerlink" title="什么是异构并行计算"></a>什么是异构并行计算</h3><p>最初的计算机只包含中央处理器，为了处理越来越复杂的图形计算，GPU 营运而生，因其数据众多的轻量级线程，非常适合处理大规模异构并行计算。</p><p>下图所示是一个典型的异构并行架构，包括一个 CPU及其内存 和一个 GPU及其内存，GPU 设备端通过 PCIe 总线与基于 CPU 主机端进行交互。一个异构并行应用包括主机代码和设备代码，分别运行在主机端和设备端。应用由 CPU 初始化，在设备端进行数据运算前，CPU 负责管理设备端的环境，代码和数据。我们称 host 为 CPU 及其内存，device 为 GPU 及其内存。</p><p><img src="/image/CUDA编程-CUDA模型概述/0.png" alt=""></p><p>CPU 计算适合处理控制密集型任务，GPU 计算适合处理包含数据并行的计算密集型任务。在 CPU 上执行串行部分或任务并行部分，在 GPU 上执行数据密集型并行部分，这种异构并行架构使得计算能力可以充分被利用。</p><h3 id="NVIDIA-GPU-显卡架构发展历程"><a href="#NVIDIA-GPU-显卡架构发展历程" class="headerlink" title="NVIDIA GPU 显卡架构发展历程"></a>NVIDIA GPU 显卡架构发展历程</h3><ul><li><strong>Tesla</strong>（特斯拉）2008年，应用于早期的 CUDA 系列显卡芯片中，并不是真正意义上的 GPU 芯片。</li><li><strong>Fermi</strong>（费米）2010年，是第一个完整的 GPU 计算架构。首款可支持与共享存储结合纯 cache 层次的 GPU 架构，支持 ECC(Error Correcting Code) 的 GPU 架构。</li><li><strong>Kepler</strong>（开普勒）2012年，Fermi 的优化版。</li><li><strong>Maxwell</strong>（麦克斯韦）2014年，首次支持实时的动态全局光照效果，</li><li><strong>Pascal</strong>（帕斯卡）2016年，GPU 将处理器和数据集成在同一个程序包内，以实现更高的计算效率。</li><li><strong>Volta</strong>（伏打）2017年，首次将一个 CUDA 内核拆分为FP32 和 INT32 两部分，首次支持混合精度运算，提高了5倍于 Pascal 计算速度，还增加了专用于深度学习的 Tensor Core 张量单元。</li><li><strong>Turing</strong>（图灵）2018年，增加了 RT Core 专用光线追踪处理器，将实时光线追踪运算加速至上一代架构的 25 倍，并能以高出 CPU 30 多倍的速度进行电影效果的最终帧渲染。去掉了对 FP64 计算的支持。</li><li><strong>Ampere</strong>（安培）2020年，重新支持 FP64，新增异步拷贝指令能够从 global memory 中将数据直接加载到 SM shared memory，降低中间寄存器堆（RF）的需求。新增 BF16 数据类型，专为深度学习优化。</li></ul><h2 id="CUDA-编程模型"><a href="#CUDA-编程模型" class="headerlink" title="CUDA 编程模型"></a>CUDA 编程模型</h2><p>CUDA 是一个通用并行计算平台和编程模型，如下图所示，CUDA 平台可以通过 CUDA 加速库、编译器指令、应用程序编程接口或编程语言接口来使用。后面的章节我们会重点讲解 CUDA C 以及 PyCUDA 的编程。</p><p><img src="/image/CUDA编程-CUDA模型概述/1.png" alt=""></p><h3 id="CUDA-软件体系"><a href="#CUDA-软件体系" class="headerlink" title="CUDA 软件体系"></a>CUDA 软件体系</h3><p>CUDA 提供了两层 API 来调用底层 GPU 硬件</p><ul><li><p>CUDA 驱动 API (CUDA Driver API)</p><p>是一种基于句柄的底层接口,大多数对象通过句柄被引用,其函数前缀均为<code>cu</code>，在调用 Driver API 前必须进行初始化，再创建 CUDA  上下文，该上下文关联到特定设备并成为主机线程的当前上下文，通过加载 PTX 汇编形式 或 二进制对象形式 的内核，然后启动内核计算。Driver API 可以通过直接操作硬件执行一些复杂的功能，但其编程较为复杂，难度较大。</p></li><li><p>CUDA 运行时 API (CUDA Runtime API)</p><p>Runtime API 对 Driver API 进行了一定的封装，隐藏了部分实现细节，因此使用起来更为方便，因此我们更多使用的是 Runtime API。Runtime API 没有专门的初始化函数，它将在第一次调用运行时函数时自动完成初始化。使用时，通常需要包含头文件 <code>cuda_runtime.h</code>，其函数前缀均为cuda。</p></li></ul><p>如下图所示</p><p><img src="/image/CUDA编程-CUDA模型概述/2.png" alt=""></p><p>Runtime API 和 Driver API 之间没有明显的性能差距，这两种 API 不能混合使用，只用单独使用其一。</p><h3 id="CUDA-函数库-CUDA-Libraries"><a href="#CUDA-函数库-CUDA-Libraries" class="headerlink" title="CUDA 函数库 (CUDA Libraries)"></a>CUDA 函数库 (CUDA Libraries)</h3><p>CUDA 提供了几个较为成熟的高效函数库，可以直接调用这些库函数进行计算，常见的包括</p><ul><li><p>CUFFT：利用 CUDA 进行傅立叶变换的函数库</p></li><li><p>CUBLAS：利用 CUDA 进行加速的完整标准矩阵与向量的运算库</p></li><li><p>CUDPP：并行操作函数库</p></li><li><p>CUDNN：利用CUDA进行深度卷积神经网络</p></li></ul><h3 id="CUDA-应用程序-CUDA-Application"><a href="#CUDA-应用程序-CUDA-Application" class="headerlink" title="CUDA 应用程序 (CUDA Application)"></a>CUDA 应用程序 (CUDA Application)</h3><p>CUDA 程序包含在 host 上运行的主机代码和在 device 上运行的设备代码，设备代码会在编译时通过 CUDA  nvcc 编译器从主机代码中分离，再转换成 PTX(ParallelThread Execution) 汇编语言，由 GPU 并行线程执行，主机代码由 CPU 执行。如下图所示</p><p><img src="/image/CUDA编程-CUDA模型概述/3.png" alt=""></p><p>执行流程如下</p><ul><li><p>分配 host 内存，并进行数据初始化（CPU初始化）</p></li><li><p>分配 device 内存，并从 host 将数据拷贝到 device 上（GPU初始化）</p></li><li><p>调用 CUDA 的核函数在 device 上完成指定的运算（GPU并行运算）</p></li><li><p>将 device上的运算结果拷贝到 host 上（将GPU结果传回CPU）</p></li><li><p>释放 device 和 host 上分配的内存（初始化清空）</p></li></ul><h3 id="CUDA-硬件结构"><a href="#CUDA-硬件结构" class="headerlink" title="CUDA 硬件结构"></a>CUDA 硬件结构</h3><ul><li><p>SP（Streaming Processor）也称为 CUDA core，是最基本的处理单元，最后具体的指令和任务都是在 SP 上处理的。GPU 进行并行计算，也就是很多个 SP 同时做处理。</p></li><li><p>SM（Streaming Multiprocessor）多个 SP 加上其他资源组成一个 SM，也叫 GPU 大核，其他资源如包括warp scheduler，register，shared memory 等。SM可以看做GPU的心脏（类似 CPU 核心）。每个 SM 都拥有 register 和 shared memory，CUDA 将这些资源分配给所有驻留在 SM 中的线程，但资源非常有限，SM 结构如下图所示。</p></li></ul><p><img src="/image/CUDA编程-CUDA模型概述/5.png" alt=""></p><p>每个 SM 包含的 SP 数量依据 GPU 架构而不同，如 Fermi 架构 GF100 是 32 个，GF10X 是 48 个，Kepler 架构都是 192 个，Maxwell 都是128 个。</p><p><img src="/image/CUDA编程-CUDA模型概述/6.png" alt=""></p><p>在软件逻辑上是所有 SP 是并行计算的，但是物理上并不是，比如只有 8 个 SM 却有 1024 个线程块需要调度处理，因为有些会处于挂起，就绪等其他状态，这有关 GPU 的线程调度，后续章节会展开讨论。</p><h2 id="三-理解-kernel-thread-block-grid-与-warp"><a href="#三-理解-kernel-thread-block-grid-与-warp" class="headerlink" title="三 理解 kernel, thread, block , grid 与 warp"></a>三 理解 kernel, thread, block , grid 与 warp</h2><h3 id="CUDA-线程模型"><a href="#CUDA-线程模型" class="headerlink" title="CUDA 线程模型"></a>CUDA 线程模型</h3><p>线程是程序执行的最基本单元，CUDA 的并行计算通过成千上万个线程的并行执行来实现。下图为 GPU 的线程结构</p><p><img src="/image/CUDA编程-CUDA模型概述/7.png" alt=""></p><p>CUDA的线程模型从小往大依次是</p><ul><li><p>Thread，线程，并行的基本单位</p></li><li><p>Block，线程块，互相合作的线程组，线程块有如下几个特点：</p><ul><li><p>以1维、2维或3维组织</p></li><li><p>允许彼此同步</p></li><li>可以通过共享内存快速交换数据</li></ul></li><li><p>Grid，网格，由一组 Block 组成</p><ul><li>以1维、2维组织</li></ul><ul><li>共享全局内存</li></ul></li></ul><h3 id="kernel"><a href="#kernel" class="headerlink" title="kernel"></a>kernel</h3><p>kernel 是在 device 上线程中并行执行的函数，是软件概念，核函数用<code>__global__</code>符号声明，并用 <code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code> 执行配置语法指定内核调用的 CUDA 线程数，每个 kernel 的 thread 都有一个唯一的线程 ID，可以通过内置变量在内核中访问。block 一旦被分配好 SM，该 block 就会一直驻留在该 SM 中，直到执行结束。一个 SM 可以同时拥有多个 blocks。</p><h3 id="warp"><a href="#warp" class="headerlink" title="warp"></a>warp</h3><p>warp 是 SM 的基本执行单元，也称线程束，一个 warp 有 32 个并行的 thread， SM 旨在同时执行数百个 thread，为了管理如此大量的线程，采用了 SIMT （Single-Instruction, Multiple-Thread：单指令，多线程）的架构，也就是一个 warp 中的所有 thread 一次执行一条公共指令，并且每个thread会使用各自的data执行该指令。</p><p>一个块中的 warp 总数计算如下</p><script type="math/tex; mode=display">WarpsPerBlock = ceil(\frac{ThreadsPerBlock}{WarpSize}, 1)</script><p>对应下图</p><p><img src="/image/CUDA编程-CUDA模型概述/8.png" alt=""></p><p>从硬件角度来看，所有的 thread 以一维形式组织，每个 thread 都有个唯一的 ID，于是作为补全整数倍的 thread 在所在的 warp 中为 inactive 状态，会额外消耗 SM 资源，所以要设定 block 中的 thread 一般为32的倍数。</p><p>下面从硬件角度和软件角度解释 CUDA 的线程模型</p><div class="table-container"><table><thead><tr><th>软件</th><th>硬件</th><th>描述</th></tr></thead><tbody><tr><td>Thread</td><td>SP</td><td>每个线程由每个线程处理器（SP）执行</td></tr><tr><td>Block</td><td>SM</td><td>线程块由多核处理器（SM）执行</td></tr><tr><td>Grid</td><td>Device</td><td>一个 kernel 由一个 grid 来执行，一次只能在一个 GPU 上执行</td></tr></tbody></table></div><h3 id="线程索引"><a href="#线程索引" class="headerlink" title="线程索引"></a>线程索引</h3><p>确定线程的唯一索引，以 2D grid 和 2D block 的情况为例。</p><p>我们要计算的数值矩阵在内存中是 row-major（行主序） 线性存储的，如下图</p><p><img src="/image/CUDA编程-CUDA模型概述/9.png" alt=""></p><p>将 thread 和 block 索引映射到矩阵坐标</p><p>ix = threadIdx.x + blockIdx.x * blockDim.x</p><p>iy = threadIdx.y + blockIdx.y * blockDim.y</p><p>idx = iy * nx + ix</p><p>下图为 block 和 thread 索引，矩阵坐标以及线性地址之间的关系</p><p><img src="/image/CUDA编程-CUDA模型概述/10.png" alt=""></p><p>在实践应用中，常常会多一维 grid， 那就是三维情况的索引，如下图所示，设  (gridDim.x,gridDim.y) = (2,3), (blockDim.x,blockDim.y) = (4,2)，我们以 thread_id(3,1) block_id(0,1) 为例</p><p><img src="/image/CUDA编程-CUDA模型概述/11.png" alt=""></p><p>可以得到</p><p>ix = threadIdx.x + blockIdx.x <em> blockDim.x = 3 + 0 </em> 4 = 3</p><p>iy = threadIdx.y + blockIdx.y <em> blockDim.y = 1 + 1 </em> 2 = 3</p><p>coordinate(3,3)</p><p>global index: idx = iy <em> blockDim.x </em> gridDim.x + ix = 3 <em> 4 </em> 2 + 3 = 27</p>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程: GPU编程概述和CUDA环境搭建</title>
      <link href="/2023/12/12/CUDA%E7%BC%96%E7%A8%8B-GPU%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%E5%92%8CCUDA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2023/12/12/CUDA%E7%BC%96%E7%A8%8B-GPU%E7%BC%96%E7%A8%8B%E6%A6%82%E8%BF%B0%E5%92%8CCUDA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>模型变得越来越深，参数愈加庞大，虽然准确率不断增长，由于硬件受限，对实际场景部署的要求也越来越高，CUDA 编程成为了一门必备的武林绝学。</p><span id="more"></span><h2 id="CUDA简介"><a href="#CUDA简介" class="headerlink" title="CUDA简介"></a>CUDA简介</h2><p>首先介绍一下 GPU，在计算机中，GPU 相比 CPU，拥有非常多的内核，这意味着 GPU 可以以非常高的吞吐量执行程序，如同一条非常宽阔的道路可以同时让很多车辆同时通行，例如最新的 RTX 3090 的核心数达到了恐怖的 10496 个，而当前的顶级 CPU 通常只有不超过 32 核心。因此我们需要正确地设计并行化加速算法，就可以发挥 GPU 的强大优势。</p><p>CUDA 是由英伟达 NVIDIA 于 2007 年所推出针对 NVIDIA GPU 专有系统，通过 CUDA，用户可方便地使用封闭好的 SDK 对 GPU 进行复杂的数值计算，在深度学习领域，CUDA 提供了一套强大的加速并行计算和人工智能相关的代码库，同时，NVIDIA 官方提供了非常完善的安装程序。</p><h2 id="CUDA-安装"><a href="#CUDA-安装" class="headerlink" title="CUDA 安装"></a>CUDA 安装</h2><h3 id="Linux-安装"><a href="#Linux-安装" class="headerlink" title="Linux 安装"></a>Linux 安装</h3><p>首先检查本机是否有 nvidia 的显卡</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep -i <span class="string">&quot;nvidia&quot;</span></span><br></pre></td></tr></table></figure><blockquote><p>请不要在vm虚拟机中安装 CUDA</p></blockquote><p>以笔主的电脑为例，有一张 RTX 3060 的移动显卡</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">01:00.0 VGA compatible controller: NVIDIA Corporation GA106M [GeForce RTX 3060 Mobile / Max-Q] (rev a1)</span><br><span class="line">01:00.1 Audio device: NVIDIA Corporation Device 228e (rev a1)</span><br></pre></td></tr></table></figure><p>之后去 NVIDIA 官网 下载对应发行版的 <a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA Toolkit</a>，这里根据自己的深度学习框架选择版本，在安装 Toolkit 时会自带 CUDA Driver</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/1.png" alt=""></p><p>下载文件推荐选择 runfile 格式</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/2.png" alt=""></p><p>文件比较大，等待时确保机器上有相应的依赖库，可以运行下面命令安装依赖库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install gcc g++ make</span><br><span class="line">sudo apt install libglu1-mesa libxi-dev libxmu-dev libglu1-mesa-dev freeglut3-dev</span><br></pre></td></tr></table></figure><p>如果系统为图形界面，需要检查系统是否自带开源 NVIDIA Nouveau 驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep -i <span class="string">&quot;nouveau&quot;</span></span><br></pre></td></tr></table></figure><p>如果有输出，需要禁用 Nouveau 驱动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/modprobe.d/blacklist.conf</span><br></pre></td></tr></table></figure><p>在文件中追加如下内容</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line">alias nouveau off</span><br><span class="line">alias lbm-nouveau off</span><br></pre></td></tr></table></figure><p>同时卸载 nvidia 相关包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt purge nvidia-*</span><br></pre></td></tr></table></figure><p>重启系统</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p>重启后使用<code>Ctrl + Alt + F2</code>进入 tt2，再次检查开源驱动是否启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep -i <span class="string">&quot;nouveau&quot;</span></span><br></pre></td></tr></table></figure><p>然后关闭显示服务，并修改安装文件权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line"><span class="built_in">chmod</span> 777 cuda_10.2.89_440.33.01_linux.run</span><br></pre></td></tr></table></figure><p>之后运行 CUDA 安装脚本，在安装时</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh cuda_*_linux.run</span><br></pre></td></tr></table></figure><p>安装完成后脚本会自动在<code>/usr/local</code>创建 <code>cuda -&gt; /usr/local/cuda-11.8/</code> 软链接，在 <code>~/.bashrc</code>写入</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/local/cuda</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure><p>在终端中输入 <code>nvcc -V</code> ，如有类似下面的输出，则安装成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2022 NVIDIA Corporation</span><br><span class="line">Built on Wed_Sep_21_10:33:58_PDT_2022</span><br><span class="line">Cuda compilation tools, release 11.8, V11.8.89</span><br><span class="line">Build cuda_11.8.r11.8/compiler.31833905_0</span><br></pre></td></tr></table></figure><p>安装完成后重启图形界面</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo service lightdm restart</span><br></pre></td></tr></table></figure><p>接下来安装 cuDNN</p><p><a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a></p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/9.png" alt=""></p><p>下载完成后解压</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xf cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz</span><br></pre></td></tr></table></figure><p>将解压后的文件拷贝到 CUDA 对应的安装目录下，并添加权限</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> cudnn-linux-x86_64-8.6.0.163_cuda11-archive/include/cudnn.h /usr/local/cuda/include</span><br><span class="line">sudo <span class="built_in">cp</span> cudnn-linux-x86_64-8.6.0.163_cuda11-archive/lib/libcudnn* /usr/local/cuda/lib64*</span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /usr/local/cuda-11.7/include/cudnn.h </span><br><span class="line">sudo <span class="built_in">chmod</span> a+r /usr/local/cuda-11.7/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>即安装完成</p><h3 id="Windows-安装"><a href="#Windows-安装" class="headerlink" title="Windows 安装"></a>Windows 安装</h3><p>打开设备管理器，查看当前显卡型号</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/3.png" alt=""></p><p>以笔主的电脑为例，有一张 RTX 3060 的移动显卡</p><p>再在桌面右击或打开系统小托盘，打开 NVIDIA 控制面板</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/4.png" alt=""></p><p>这里推荐 Studio 驱动程序，不建议 Game Ready 驱动程序，可以在 <a href="https://www.nvidia.cn/Download/index.aspx">NVIDIA 驱动程序下载</a> 找到对应显卡的驱动程序。安装完成后如图所示</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/5.png" alt=""></p><p>之后根据系统驱动程序版本<a href="https://developer.nvidia.com/cuda-toolkit-archive">下载 CUDA 安装程序</a>，<a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html#cuda-major-component-versions">点击查看系统驱动程序版本和 CUDA 版本对应关系</a>，推荐 exe(local)</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/16.png" alt=""></p><p>下载完成后双击打开，这里使用默认位置</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/6.png" alt=""></p><p>使用自定义安装</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/7.png" alt=""></p><p>这里要记住安装位置，推荐默认</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/11.png" alt=""></p><p>安装后会自动添加环境变量，打开 Powershell / CMD，输入<code>nvcc -V</code></p><p>如有类似下面的输出，则安装成功</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2022 NVIDIA Corporation</span><br><span class="line">Built on Wed_Sep_21_10:41:10_Pacific_Daylight_Time_2022</span><br><span class="line">Cuda compilation tools, release 11.8, V11.8.89</span><br><span class="line">Build cuda_11.8.r11.8/compiler.31833905_0</span><br></pre></td></tr></table></figure><p>接下来安装 cuDNN</p><p><a href="https://developer.nvidia.com/rdp/cudnn-download">https://developer.nvidia.com/rdp/cudnn-download</a></p><p>下载完成解压后如何下图</p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/10.png" alt=""></p><p>把这三个文件夹拷贝到 CUDA 的安装目录下，同名文件夹会自动合并。</p><p>将如下路径添加到系统环境变量中</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\bin</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\include</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\lib</span><br><span class="line">C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libnvvp</span><br></pre></td></tr></table></figure><p>即安装完成</p><h2 id="PyCUDA"><a href="#PyCUDA" class="headerlink" title="PyCUDA"></a>PyCUDA</h2><p>PyCUDA 是 NVIDIA 针对 python 编写的 CUDA API，底层使用 C++， 使用 PyCUDA 可以更方便地编写代码。同样 CUDA 错误都会自动转换为 Python 异常。</p><h3 id="Linux-安装-1"><a href="#Linux-安装-1" class="headerlink" title="Linux 安装"></a>Linux 安装</h3><p>确保电脑中安装了 python 环境和 pip，安装命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pycuda</span><br></pre></td></tr></table></figure><h3 id="Windows-安装-1"><a href="#Windows-安装-1" class="headerlink" title="Windows 安装"></a>Windows 安装</h3><p>根据自己的 CUDA 版本 和 Python 版本下载 whl 文件 <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda">http://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda</a></p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/17.png" alt=""></p><p>安装命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pycuda*.whl</span><br></pre></td></tr></table></figure><h2 id="Nvidia-Nsight-Systems"><a href="#Nvidia-Nsight-Systems" class="headerlink" title="Nvidia Nsight Systems"></a>Nvidia Nsight Systems</h2><p>简称 <code>nsys</code>，是一款低开销性能分析工具，旨在为开发人员提供优化软件所需的洞察力。无偏差的活动数据可在工具中可视化，可帮助用户调查瓶颈，避免推断误报，并以更高的性能提升概率实现优化。用户将能够识别问题，例如 GPU 闲置、不必要的 GPU 同步、CPU 并行化不足。</p><p>根据系统平台选择合适的安装包</p><p><a href="https://developer.nvidia.cn/gameworksdownload#?dn=nsight-systems-2022-4">https://developer.nvidia.cn/gameworksdownload#?dn=nsight-systems-2022-4</a></p><p><img src="/image/CUDA编程-GPU编程概述和CUDA环境搭建/18.png" alt=""></p><p>安装完成后输入命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nsys --version</span><br></pre></td></tr></table></figure><p>如有返回，即安装完成</p>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CUDA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记录第n次创建启用并清理临时swap</title>
      <link href="/2023/12/05/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E5%88%9B%E5%BB%BA%E5%B9%B6%E5%90%AF%E7%94%A8%E4%B8%B4%E6%97%B6swap/"/>
      <url>/2023/12/05/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E5%88%9B%E5%BB%BA%E5%B9%B6%E5%90%AF%E7%94%A8%E4%B8%B4%E6%97%B6swap/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>时常要用 chatgpt 辅助生成 swap，记录下来方便</p><span id="more"></span><h2 id="脚本"><a href="#脚本" class="headerlink" title="脚本"></a>脚本</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function create_swap &#123;</span><br><span class="line">    SWAPFILE=$1</span><br><span class="line">    SIZE=$2</span><br><span class="line"></span><br><span class="line">    mkdir -p $(dirname &quot;$SWAPFILE&quot;)</span><br><span class="line"></span><br><span class="line">    sudo dd if=/dev/zero of=&quot;$SWAPFILE&quot; bs=1G count=&quot;$SIZE&quot; status=progress</span><br><span class="line"></span><br><span class="line">    sudo chmod 600 &quot;$SWAPFILE&quot;</span><br><span class="line"></span><br><span class="line">    sudo mkswap &quot;$SWAPFILE&quot;</span><br><span class="line">    sudo swapon &quot;$SWAPFILE&quot;</span><br><span class="line"></span><br><span class="line">    echo &quot;Swap file created and enabled at $SWAPFILE with size $&#123;SIZE&#125;G&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function clean_swap &#123;</span><br><span class="line">    SWAPFILE=$1</span><br><span class="line"></span><br><span class="line">    if [ -f &quot;$SWAPFILE&quot; ]; then</span><br><span class="line">        sudo swapoff &quot;$SWAPFILE&quot;</span><br><span class="line">        sudo rm -f &quot;$SWAPFILE&quot;</span><br><span class="line">        echo &quot;Swap file at $SWAPFILE has been removed.&quot;</span><br><span class="line">    else</span><br><span class="line">        echo &quot;Swap file at $SWAPFILE does not exist.&quot;</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CLEAN_MODE=0</span><br><span class="line"></span><br><span class="line">while getopts &quot;c&quot; opt; do</span><br><span class="line">    case $opt in</span><br><span class="line">        c)</span><br><span class="line">            CLEAN_MODE=1</span><br><span class="line">            ;;</span><br><span class="line">        \?)</span><br><span class="line">            echo &quot;Invalid option: -$OPTARG&quot; &gt;&amp;2</span><br><span class="line">            exit 1</span><br><span class="line">            ;;</span><br><span class="line">    esac</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">shift $((OPTIND -1))</span><br><span class="line"></span><br><span class="line">if [ &quot;$CLEAN_MODE&quot; -eq 1 ]; then</span><br><span class="line">    if [ &quot;$#&quot; -ne 1 ]; then</span><br><span class="line">        echo &quot;Usage: $0 -c [swap file path]&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    clean_swap $1</span><br><span class="line">else</span><br><span class="line">    if [ &quot;$#&quot; -ne 2 ]; then</span><br><span class="line">        echo &quot;Usage: $0 [swap file path] [size in GB]&quot;</span><br><span class="line">        exit 1</span><br><span class="line">    fi</span><br><span class="line">    create_swap $1 $2</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">chmod +x create_swap.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">创建 swap 文件</span></span><br><span class="line">./create_swap.sh [swap file path] [size in GB]</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">清理 swap 文件</span></span><br><span class="line">./create_swap.sh -c [swap file path]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> swap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: ZeRO++: Extremely Eficient Collective Communication for Giant Model Training</title>
      <link href="/2023/11/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/"/>
      <url>/2023/11/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>原文链接：<a href="https://arxiv.org/pdf/2306.10209.pdf">https://arxiv.org/pdf/2306.10209.pdf</a></p><p>开源代码：<a href="https://github.com/microsoft/deepspeed">https://github.com/microsoft/deepspeed</a></p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>面对 LLM，3D并行工程实现复杂</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>通过以下 3 种方法减少通信</p><ul><li>基于块量化的 all-gather</li><li>通过数据重映射用通信减少内存开销</li><li>基于 all-to-all 的量化梯度平均方法，替代 reduce-scatter</li></ul><p>ZeRO++将 ZeRO 的通信量减少了 4 倍，并在低精度下保持准确性，使得在 384 张 GPU下，吞吐量可以提高 2.16 倍</p><h3 id="Limitations-of-ZeRO"><a href="#Limitations-of-ZeRO" class="headerlink" title="Limitations of ZeRO"></a>Limitations of ZeRO</h3><p>在低带宽集群中，单卡的吞吐量只有高带宽集群的一半，即使在高带宽集群中，使用数千个 GPU 进行训练，单卡的 batch size 也受到 global batch size 的限制（global batch size 不能无限增加，否则会降低模型收敛效率）</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/1.webp" width="500" /></p><p>所以在千卡训练时，单卡的 batch size 必须非常小，这会降低计算与通信的比率</p><p>ZeRO 由于对模型状态进行分区，无法直接对模型状态复制，所以与之前的节省通信工作不兼容</p><h3 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO++"></a>ZeRO++</h3><p>假设模型参数量为 M</p><p>FWD：参数 all gather</p><p>BWD：参数 all gather，梯度 reduce scatter</p><p>通信量总计 3M</p><h4 id="Quantized-Weight-Communication-qwZ"><a href="#Quantized-Weight-Communication-qwZ" class="headerlink" title="Quantized Weight Communication (qwZ)"></a>Quantized Weight Communication (qwZ)</h4><p>通信前，将参数从 fp16 量化为 int8，为了保证训练准确率，使用 block-based 量化的思想，并使用 CUDA kernel 保证高性能</p><h4 id="Hierarchical-Weight-Partition-hpZ"><a href="#Hierarchical-Weight-Partition-hpZ" class="headerlink" title="Hierarchical Weight Partition (hpZ)"></a>Hierarchical Weight Partition (hpZ)</h4><p>在单个节点中保存整个模型参数的副本，通过节点内的 all gather 代替节点间通信，牺牲显存，节省通信</p><h4 id="Quantized-Gradient-Communication-qgZ"><a href="#Quantized-Gradient-Communication-qgZ" class="headerlink" title="Quantized Gradient Communication  (qgZ)"></a>Quantized Gradient Communication  (qgZ)</h4><p>直接在 reduce scatter 之前量化会影响精度，可以在通信过程中使用 block-based INT4 量化压缩梯度，并在发送后恢复保证训练精度</p><p>两步通信，先节点内通信，再节点间通信</p><p>节点间通信使用流水线策略，融合 CUDA Kernel</p><h4 id="Communication-Volume-Reduction"><a href="#Communication-Volume-Reduction" class="headerlink" title="Communication Volume Reduction"></a>Communication Volume Reduction</h4><p>qwZ: 1M → 0.5</p><p>hpZ: 1M → 0M</p><p>qgZ: 1M → 0.25M</p><p>all: 3M → 0.75M</p><h2 id="先前工作"><a href="#先前工作" class="headerlink" title="先前工作"></a>先前工作</h2><h3 id="3D-并行"><a href="#3D-并行" class="headerlink" title="3D 并行"></a>3D 并行</h3><p>3D 并行训练流程</p><ol><li>all gather 参数</li><li>FWD</li><li>partition</li><li>all gather</li><li>BWD</li><li>partition</li><li>reduce scatter</li><li>优化器更新</li></ol><h4 id="ZeRO-优化器"><a href="#ZeRO-优化器" class="headerlink" title="ZeRO 优化器"></a>ZeRO 优化器</h4><p>ZeRO-3 最高效利用内存，但需要多次通信解决分区问题</p><h4 id="Communication-Reduction-Techniques"><a href="#Communication-Reduction-Techniques" class="headerlink" title="Communication Reduction Techniques"></a>Communication Reduction Techniques</h4><p>量化问题：fp32/16 对比 int8 有数据范围和粒度的差异</p><p>改进方法：</p><p>过滤异常值，缩小数值范围差距，但准确性被滤波算法影响，且有时间开销<br>分块量化优化器状态，但需要修改模型结构</p><p>梯度压缩：1-bit adam/lamb 可以实现高效的通信，但必须保证每个 GPU 上都有梯度副本，由于 ZeRO-3 的分区策略，不能应用</p><h4 id="ZeRO-Communication-Reduction"><a href="#ZeRO-Communication-Reduction" class="headerlink" title="ZeRO Communication Reduction"></a>ZeRO Communication Reduction</h4><p>MiCS (基于 DeepSpeed-v0.4.9 和 PyTorch-v1.11)：对节点分组，每个组保存模型状态的完整副本。在每个组内，模型状态被分区，使最频繁的参数 all gather操作在每个组上进行，且并行多个节点间的集体通信，在组内的梯度达到边界，则进行组间通信，此外，还采用了细粒度同步、合并通信 API 和内存碎片整理等优化</p><p>hpZ 与 MiCS 类似，但只对权重分组，在每个 gpu 上保留对模型状态的分区，相比 MiCS 节省了内存</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="qwZ"><a href="#qwZ" class="headerlink" title="qwZ"></a>qwZ</h3><p>为了解决精度下降严重的问题，使用块量化的思想，将每个权重张量被划分为更小的块，然后使用独立的量化缩放系数进行对称量化为 INT8，减少了 3 倍量化误差</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/2.webp" width="500" /></p><h3 id="hpZ"><a href="#hpZ" class="headerlink" title="hpZ"></a>hpZ</h3><p>采用两级分区策略</p><ul><li>全局主分区：所有模型状态在所有设备上全局分区（如ZeRO-3）</li><li>次级分区：在次全局级别（例如，计算节点）创建 FP16 参数的次级副本，并在多个次级分区中复制</li></ul><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/3.webp" width="500" /></p><p>在 FWD 阶段，对权重在主分区上 all gather。权重在 FWD 后，根据次级分区进行分区。由于 FWD/BWD 之间模型参数的存在时序一致性，在 BWD 中再次需要权重时，基于次级分组进行 all gather</p><p>当次级分区设置为计算节点时，避免了 all gather 过程中的跨节点通信</p><p>在迭代结束时，在优化器步骤中，所有模型状态以及 FP16 参数的主副本都根据主分区更新</p><p>支持任何次级分区大小，并控制次级分区中的GPU数量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/4.webp" width="500" /></p><h3 id="qgZ"><a href="#qgZ" class="headerlink" title="qgZ"></a>qgZ</h3><p>基于 all-to-all 的量化梯度通信策略，只在通信之前量化梯度，但在任何 reduce 操作之前将它们反量化到原有精度 ，功能上等同于压缩的 reduce-scatter 操作</p><p>解决了两个问题：</p><ol><li>简单地在 INT4/INT8 中实施 reduce-scatter 会导致显著精度损失</li><li>在传统 tree 或 ring-based reduce-scatter 中使用量化需要一长串量化和反量化步骤，这直接导致误差积累和显著的延迟</li></ol><p>qgZ 不使用tree或ring-based reduce-scatter算法，而是基于一种新的分层 all-to-all 方法</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/5.webp" width="500" /></p><p>qgZ 中有三个主要步骤：</p><ol><li>梯度切片重新排序，在任何通信发生之前，对梯度进行切片并对张量切片重新排序，以保证通信结束时每个 GPU 上的最终梯度位置是正确的</li><li>节点内通信和 reduce，量化重新排序的梯度切片，在每个节点内进行 all-to-all 通信，从 all-to-all 中对接收到的梯度切片进行反量化，并进行局部 reduce</li><li>节点间通信和 reduce，再次量化局部reduce后的梯度，进行节点间的all-to-all通信，再次对接收到的梯度进行反量化，并计算最终的高精度梯度 reduce</li></ol><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/6.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/7.webp" width="500" /></p><p>给定每个节点 N 个 GPU、M 的模型大小和 Z 的量化比率，NCCL all-to-all 将生成 M*N/Z 跨节点流量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/8.webp" width="1000" /></p><p>相比之下，通过 qgZ，将每个 GPU 的跨节点流量从 M/Z 减少到 M/(Z<em>N)。 因此，总通信量从 M</em>N/Z 减少到 M<em>N/(Z</em>N) = M/Z</p><p>此外，通过重叠节点内和节点间通信以及融合 CUDA 内核来进一步优化 qgZ 的端到端延迟</p><ul><li>张量切片重新排序 (Tensor Slice Reordering)</li><li>节点内量化 (Intra-node quantization)</li><li>节点内反量化 (Intra-node Dequantization)</li><li>节点内梯度整合 (Intra-node Reduction)</li><li>节点间量化 (inter-node quantization)</li></ul><h2 id="优化实现"><a href="#优化实现" class="headerlink" title="优化实现"></a>优化实现</h2><h3 id="Overlap-Compute-and-Communication"><a href="#Overlap-Compute-and-Communication" class="headerlink" title="Overlap Compute and Communication"></a>Overlap Compute and Communication</h3><ul><li>根据模型层的执行顺序</li><li>保证量化异步执行</li></ul><p>获取每一层的参数，在不同的 CUDA 流上同时启动当前层的通信和下一层的量化。当下一层需要量化数据时，ZeRO++同步量化流以确保量化数据准备就绪</p><p>这可以在当前层的通信时间跨度下隐藏了下一层的量化成本，隐藏了量化开销</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/9.webp" width="500" /></p><p>基于 all to all 的梯度通信分为两个阶段：节点内通信和节点间通信</p><p>为了利用节点间通信执行时，节点内通信的空闲，实现了输入梯度张量的分块和 pipeline 传输</p><p>pipeline 阶段越多，重新排序所需的细粒度张量切片就越多</p><p>提出了一种广义张量切片重新排序算法，涵盖了 w/ 和 w/o pipeline 数据传输的情况</p><p>这里的 stages 是指拥有的 pipeline stage 的数量，nodeSize 是每个节点的 GPU 数量，nodes 是节点的数量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/10.webp" width="500" /></p><h3 id="CUDA-Kernels"><a href="#CUDA-Kernels" class="headerlink" title="CUDA Kernels"></a>CUDA Kernels</h3><p>为了最大化带宽利用和最小化内核开销，论文中实现并优化了自定义CUDA核心，用于实现量化操作</p><p>开发了一个可组合运算符的核心量化和反量化库，利用高效的向量化内存访问来满足给定GPU架构支持的最大粒度。此外，利用指令级并行性重叠多个内存事务</p><p>使用多种技术减少量化内核的总内存流量。例如，调整每个量化块的大小，将张量重塑和量化融合到同一内核中，避免从全局内存中重复加载数据</p><p>此外，将连续的反量化、减少和量化操作融合到单一内核实现中</p><p>减少了 9 倍的内存流量</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ul><li>硬件配置：使用包含16个V100 SXM3 32 GB GPU的24个NVIDIA DGX-2节点。这些节点通过具有NVIDIA SHARP支持的InfiniBand（IB）连接，实现超过800 Gbps的节点间带宽</li><li>测试环境：为了评估ZeRO++在不同网络环境下的性能，展示了通过启用1到8个IB连接（即100 Gbps到800 Gbps）的ZeRO++运行性能</li><li>基准设置：使用ZeRO-3作为基准，因其便于大规模训练巨型模型。同时，为了评估优化内核的性能，还使用了PyTorch量化和非融合内核实现的ZeRO++作为消融研究的基线</li><li>模型配置：基于Megatron-Turing-NLG训练530B模型在2000个GPU上使用每GPU 2000 token的设置，对ZeRO++使用相同的2000 token设置进行评估。还评估了每 GPU 1000 token的设置，以测试ZeRO++在更极端规模的场景下的性能。调整层数和隐藏层大小以构建不同大小的模型</li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/11.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/12.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/13.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/14.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/15.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/16.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/17.webp" width="500" /></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/18.webp" width="500" /></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>合并上述三项优化，使用 384 V100 GPU 进行大规模模型训练时系统吞吐量提高 2.16 倍</p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>开个新坑: 精读pytorch源码</title>
      <link href="/2023/11/09/%E5%BC%80%E4%B8%AA%E6%96%B0%E5%9D%91-%E7%B2%BE%E8%AF%BBpytorch%E6%BA%90%E7%A0%81/"/>
      <url>/2023/11/09/%E5%BC%80%E4%B8%AA%E6%96%B0%E5%9D%91-%E7%B2%BE%E8%AF%BBpytorch%E6%BA%90%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>毕设打算做个简单的深度学习框架，调研了几种语言目前的实现，各有优劣，先从最经典的 torch 读起。欢迎朋友们提出建议</p><span id="more"></span><h2 id="mixed"><a href="#mixed" class="headerlink" title="mixed"></a>mixed</h2><p><a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></p><p><a href="https://github.com/tensorflow/tensorflow">https://github.com/tensorflow/tensorflow</a></p><p><a href="https://github.com/PaddlePaddle/Paddle">https://github.com/PaddlePaddle/Paddle</a></p><p><a href="https://github.com/mindspore-ai/mindspore">https://github.com/mindspore-ai/mindspore</a></p><h2 id="C"><a href="#C" class="headerlink" title="C"></a>C</h2><p><a href="https://github.com/ggerganov/ggml">https://github.com/ggerganov/ggml</a></p><h2 id="Rust"><a href="#Rust" class="headerlink" title="Rust"></a>Rust</h2><p><a href="https://github.com/huggingface/candle">https://github.com/huggingface/candle</a></p><p><a href="https://github.com/burn-rs/burn">https://github.com/burn-rs/burn</a></p><h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><p><a href="https://github.com/tensorlayer/TensorLayer">https://github.com/tensorlayer/TensorLayer</a></p><p><a href="https://github.com/alibaba/TinyNeuralNetwork">https://github.com/alibaba/TinyNeuralNetwork</a></p><h2 id="C-1"><a href="#C-1" class="headerlink" title="C++"></a>C++</h2><p><a href="https://github.com/xylcbd/EasyCNN">https://github.com/xylcbd/EasyCNN</a></p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
      <link href="/2023/10/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/"/>
      <url>/2023/10/26/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>ZeRO-Infinity 是基于 ZeRO 的扩展，Infinity 离线引擎可以同时利用 GPU、CPU 和 NVMe 内存，还提出了其他的优化技术。</p><span id="more"></span><p>原文链接：<a href="https://arxiv.org/pdf/2104.07857.pdf">https://arxiv.org/pdf/2104.07857.pdf</a></p><p>开源代码：<a href="https://github.com/microsoft/deepspeed">https://github.com/microsoft/deepspeed</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>GPU 内存墙：模型规模成长了 1000 倍，但 GPU 内存只增长了 5 倍</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>ZeRO-Infinity 是基于 ZeRO 的扩展，Infinity 离线引擎可以同时利用 GPU、CPU 和 NVMe 内存，还提出了以下三个技术</p><ul><li>Memory-Centric Tiling：减少大规模操作的GPU内存需求，而无需 MP</li><li>Bandwidth-Centric Partitioning：技术利用所有并行设备的聚合内存带宽</li><li>Ease-Inspired Implementation：避免模型代码重构</li></ul><h2 id="先前工作"><a href="#先前工作" class="headerlink" title="先前工作"></a>先前工作</h2><p>ZeRO：<a href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a></p><p>ZeRO-Offload：<a href="https://arxiv.org/pdf/2101.06840.pdf">https://arxiv.org/pdf/2101.06840.pdf</a></p><h2 id="显存需求"><a href="#显存需求" class="headerlink" title="显存需求"></a>显存需求</h2><blockquote><p>这一节没有根据论文的内容写，论文讲的太简略了，看的糊里糊涂</p></blockquote><p>内存可以从两个方面优化</p><ul><li>模型状态：梯度，参数，优化器状态</li><li>剩余状态：中间激活值、临时buffer、显存碎片等。下面我们只讨论中间激活值的显存占用</li></ul><h2 id="模型状态内存"><a href="#模型状态内存" class="headerlink" title="模型状态内存"></a>模型状态内存</h2><h3 id="模型参数量"><a href="#模型参数量" class="headerlink" title="模型参数量"></a>模型参数量</h3><p>为了方便分析，定义 transformer 模型的层数为 l，隐藏层维度为 h，注意力头数为 a。</p><p>词表大小为 v，训练数据的批次大小为 b，序列长度为 s</p><p>transformer模型由 l 个相同的层组成，每个层分为 self-attention 块和 MLP 块</p><ul><li><p>self-attention：参数有 q, k, v 的权重矩阵和 wq, wk, wv 的偏置 bq, bk, bv，输出权重矩阵 wo 和偏置 bo，4 个权重矩阵的形状为 [h, h]，4个偏置的形状为 [h]。所以 self-attention 块的参数量为  4h^2 + 4h</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/1.webp" width="800" /></p></li><li><p>MLP：由 2 个线性层组成，一般第一个线性层是先将维度从 h 映射到 4h ，第二个线性层再将维度从 4h 映射到 h。第一个线性层的权重矩阵  的形状为 [h,4h] ，偏置的形状为 [4h] 。第二个线性层权重矩阵的形状为 [4h,h] ，偏置形状为 [h] 。所以 MLP 块的参数量为 8h^2+5h</p></li><li><p>layer normalization：self-attention 块和 MLP 块各有一个 layer normalization，包含可训练的缩放参数和平移参数，形状都是 [h] 。2个layer normalization的参数量为 4h</p></li></ul><p>所以每个 transformer 层的参数量为 12h^2+13h，此外还有</p><ul><li>embdding 层：维度通常等于隐藏层维度 h，参数量为 vh。最后的输出层的权重矩阵通常与 embdding 层是参数共享的</li><li>位置编码：如果采用可训练式的位置编码，会有一些可训练模型参数，数量比较少。如果采用相对位置编码，例如 RoPE 和 ALiBi，则不包含可训练的模型参数。这里忽略这部分参数</li></ul><p>综上，l 层 transformer 模型的可训练模型参数量为 l(12h^2+13h)+vh。当隐藏维度 h 较大时，可以忽略一次项，近似为 12lh^2</p><p>目前主流的大模型训练方法是使用 Adam 优化器进行混合精度训练，参数和梯度存储为 FP16，优化器状态包括 FP32 的动量、方差、参数和梯度，因此每个参数需要 20b（2<em>2b + 4</em>4b） 的显存，总显存占用就是 240 lh^2 字节</p><h2 id="剩余状态内存"><a href="#剩余状态内存" class="headerlink" title="剩余状态内存"></a>剩余状态内存</h2><h3 id="中间激活值"><a href="#中间激活值" class="headerlink" title="中间激活值"></a>中间激活值</h3><p>可以理解为前向传递过程中计算得到的，并在后向传递过程中需要用到的所有 tensor</p><ul><li>layer normalization 层：计算梯度时需要用到层的输入 bsh 字节，输入的均值和方差 bs 字节，由于 h 一般是千位，所以 layer normalization 显存近似为 bsh 字节</li><li>dropout 层：在训练中存储为 mask 矩阵，每个元素只占 1 字节</li></ul><p>每个 transformer 层包含了一个 self-attention 块和 MLP 块，并分别对应一个 layer normalization 连接</p><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a>self-attention</h4><p>计算公式为</p><script type="math/tex; mode=display">Q=xW_q,K=xW_k,V=xW_v \\x_{out} = softmax(\frac{QK^T}{\sqrt{h}})\cdot V\cdot W_o +x</script><ul><li><p>Q, K, V：需要保存共同输入 x，这就是中间激活，x shape 为 [b, s, h]，元素个数为 bsh，占用显存为 2bsh 字节</p><blockquote><p>这里的 shape 中没有 a 是因为在计算时，每个 head 对应的 hidden size 被除以了 head，一乘一除，化简了</p></blockquote></li><li><p>QK^T：需要保存 Q, K，shape 都是 [b, s, h]，占用显存大小为 4bsh<br>softmax()：需要保存函数输入 QK^T，Q shape [b, a, s, h]，K^T shape [b, a, h, s]，QK^T shape [b, a, s, s]，占用显存为 2bs^2a</p></li><li><p>dropout：计算完 softmax 后得到 score，需要计算 dropout，保存 mask 矩阵，shape 与 QK^T 相同，占用显存为 bs^2a</p></li><li><p>计算 v 上的 attention ，score * v，需要保存 score：2bs^2a ，v：2bsh，总合 2bs^2a+2bsh</p></li><li><p>计算输出映射和一个 dropout，输出映射需要保存其输入，2bsh 字节，结合 dropout 3bsh</p></li></ul><p>综上所述，self-attention 块中间激活占用显存为 11bsh + 5bs^2a 字节</p><h4 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h4><p>计算公式为</p><script type="math/tex; mode=display">x = f_{gelu} (x_{out}W_1)W_2 + x_{out}</script><ul><li>linear_1：2bsh</li><li>激活函数：8bsh</li><li>linear_2：8bsh</li><li>dropout：bsh</li></ul><p>综上所述，MLP 块中间激活占用显存为 19bsh 字节</p><p>另外，self-attention 块和 MLP 块共对应两个 layer normalization，其输入合计为 2bsh，中间激活 4bsh</p><p>每个 transformer 层中间激活占用显存为 34bsh + 5bs^2a 字节</p><p>以 GPT3-175B 模型为例，模型配置如下</p><ul><li>layer（l）：96</li><li>hidden size（h）：12288</li><li>attention head（a）：96</li><li>sequence length（s）：2048</li></ul><p>假设采用混合精度训练，都采用 bf16 或 fp16 存储，每个元素占2个bytes，则模型参数占用显存为 350 GB</p><div class="table-container"><table><thead><tr><th>batch size（b）</th><th>中间激活显存 (34bsh + 5bs^2a)*l</th><th>中间激活显存/模型参数显存</th></tr></thead><tbody><tr><td>1</td><td>275 GB</td><td>79%</td></tr><tr><td>64</td><td>17.6 TB</td><td>5000%</td></tr><tr><td>128</td><td>35.3 TB</td><td>10100%</td></tr></tbody></table></div><p>可以看到中间激活占用显存，可以通过激活值 checkpoint 技术显著减少激活所需的内存，但代价是增加了 0.33 倍重计算</p><p>定义 c 为两个激活值 checkpoint 之间的 Transformer 块数量</p><p>大型模型如 Turing-NLG 17.2B 和 GPT-3 175B 都是使用该方法进行训练的。存储激活值 checkpoint 所需的内存近似估计为 2bsh * l/c</p><p>下图 a 第 7 列是存储激活值 checkpoint 所需的内存</p><p>假设 b：32，s：1024，c：1，虽然生成的激活值 checkpoint 比完整的中间激活小几个数量级，但在超过万亿参数时，GPU 内存仍然无法存储所有中间激活值</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/2.webp" width="800" /></p><h3 id="模型状态工作内存-MSWM"><a href="#模型状态工作内存-MSWM" class="headerlink" title="模型状态工作内存 (MSWM)"></a>模型状态工作内存 (MSWM)</h3><p>是在所有模型状态被 offload 至 CPU 或 NVMe 后，在模型中执行前向或 BWD 所需的最小 GPU 内存占用</p><p>从上图 (a) 倒数第 2 列得知，在超过 1k 亿个参数后，显存需求激增，在遇到 MLP 中的 linear 申请存储 8h^2+5h 个参数的连续内存时，没有足够的连续空间，下文提出了一种有别于 MP 的方法解决这个问题</p><h3 id="激活值工作内存-AWM"><a href="#激活值工作内存-AWM" class="headerlink" title="激活值工作内存 (AWM)"></a>激活值工作内存 (AWM)</h3><p>是在进行 BWD 之前，在 BWD 中重计算激活所需的内存。这取决于两个连续激活值 checkpoint 之间的激活大小</p><p>结合上图 (a) 最后 1 列，AWM 由几十个中间激活层参数组成，只要总的 AWM 占用满足显存，不会遇到连续空间不足的问题</p><h3 id="带宽需求"><a href="#带宽需求" class="headerlink" title="带宽需求"></a>带宽需求</h3><p>加载 CPU 和 NVMe 内存关键是有限的带宽是否会影响训练效率。文中描述了带宽对训练效率的影响。定义了效率 efficiency</p><p>假设没有计算通信重叠，执行工作负载，使用峰值计算吞吐量 penk_tp，数据移动带宽 bw，算术强度 ait 评估训练效率</p><p>ait 是总计算量与计算需要移动的数据之间的比率，描述了每次数据移动的计算量，更高的 ait 表示对带宽的要求更低</p><p>效率 efficiency 推导如下</p><script type="math/tex; mode=display">\begin{align}compute\_time & = \frac{total\_computation}{peak_{tp}} \\\\ait & = \frac{total\_computation}{total\_data\_mouement} \\\\communication\_time & = \frac{total\_data\_mouement}{bw} \\& = \frac{total\_computation}{ait \times bw} \\\\efficiency & = \frac{compute\_time}{compute\_time + communication\_time} \\& = \frac{ait \times bw}{ait \times bw + peak_{tp}}\end{align}</script><h3 id="Quantifying-AIT-in-DL-training"><a href="#Quantifying-AIT-in-DL-training" class="headerlink" title="Quantifying AIT in DL training"></a>Quantifying AIT in DL training</h3><p>模型状态和激活值 checkpoint 有不同的 ait 值，文中通过每次训练迭代中的总计算量，计算每个模型状态和激活值的数据移动量进行 ait 量化</p><ul><li>总计算量：每次迭代的总计算量主要取决于 Transformer 中的参数 p，序列长度 s 和 batch size，即 2bsp，BWD 的计算量大约是 FWD 的两倍。此外，激活值 checkpoint 在 BWD 期间需要进行额外的正向计算，所以每次迭代的总计算量为 8 bsp</li><li>参数和梯度：参数在 FWD 和 BWD 期间至少需要从 CPU 加载到 GPU 两次，存在 2p 的数据移动量，在使用激活值 checkpoint 时，参数可能需要在 BWD 期间进行额外的加载，增加了额外 1p 的数据移动量。此外，梯度必须至少从 GPU 存储到其目标位置一次，增加了最后1p 的数据移动量，假设参数和梯度存储在相同的目标位置，FWD 和 BWD 期间的总数据移动量为 4p，即 8p 字节。每次迭代的总计算量，因此 ait 与参数和梯度的关系为 bs</li><li>优化器状态：优化器状态至少需要读取和写入一次 GPU，总数据移动量为 2os，约为 2<em>16p 字节（结合上文的 adam，笔者认为是 2</em>20p 字节），因此在完整的训练迭代过程中，ait 与优化器状态关系为 bs/4<br>激活值 checkpoint：在 FWD 中，激活值 checkpoint 必须保存到目标位置，并在 BWD 期间检索。总数据移动量与激活值 checkpoint 的关系为 2 <em> bsh </em> l/c，ait 与激活值 checkpoint 的关系为 24hc</li></ul><h3 id="Bandwidth-Requirements"><a href="#Bandwidth-Requirements" class="headerlink" title="Bandwidth Requirements"></a>Bandwidth Requirements</h3><p>由于 ait 的变化，模型状态和激活 checkpoint 有非常不同的带宽要求。模型状态仅取决于 bs，激活 checkpoint 取决于 ch</p><p>由上面的公式得知，efficiency 的带宽需求还取决于 peak_tp，作者使用 NVIDIA V100 DGX-2 SuperPOD 集群进行了如下实验</p><p>实验假设了不在实验变量中的其他通信带宽无限，且 peak_tp 是理论峰值</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/3.webp" width="800" /></p><ul><li>a：参数和梯度的带宽超过70 GB/s，即使是最小的 batch size，也可以实现50%以上的效率，在这个带宽下，只要 bz 足够大，数据移动可以与计算完全重叠，以实现100%的效率</li><li>b：相比 a，优化器状态需要高出近 4 倍的带宽才能达到 50% 效率。此外，优化器状态在 FWD 和 BWD 结束时更新，并且不能与计算重叠</li><li>c：启用激活值 checkpoint 后，即使 h 为 2k，2GB/s 的带宽也能维持 50% 以上的效率。一旦 h 超过 8k，带宽需求将降至 1 GB/s 以下</li></ul><h2 id="ZeRO-Infinity-设计概览"><a href="#ZeRO-Infinity-设计概览" class="headerlink" title="ZeRO-Infinity 设计概览"></a>ZeRO-Infinity 设计概览</h2><p>GPU 集群在存储器方面具有高度异构性。除了GPU内存外，还有 CPU 内存以及无限大（Infinity）的 NVMe 磁盘空间。ZeRO-Infinity 通过利用这些异构存储器，突破了 GPU 内存壁垒。下图比较了 3D 并行和 ZeRO-Infinity 所能达到的最大模型规模，其支持每个 NVIDIA V100 DGX-2 节点 1 万亿个参数，相比 3D 并行增加了 50 倍</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/4.webp" width="500" /></p><p>示意图</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/5.webp" width="500" /></p><h3 id="Design-for-Unprecedented-Scale"><a href="#Design-for-Unprecedented-Scale" class="headerlink" title="Design for Unprecedented Scale"></a>Design for Unprecedented Scale</h3><ul><li>Infinity offload engine for model states：ZeRO Infinity 基于 ZeRO-3 设计，对所有模型状态进行分区，消除内存冗余，设计了 Infinity offload engine，将所有分区模型状态 offload 到 CPU 或 NVMe，或者根据内存需求保留在 GPU 上。请注意，上面图 2a 和图 2b 中，96 个节点（1536个GPU）的 DGX-2 集群的总 NVMe 内存足够存储 100  万亿参数模型的模型状态</li><li>CPU Offload for activations：将激活值内存 offload 到 CPU 内存中</li><li>Memory-centric tiling for working memory：ZeRO-Infinity 可以支持任意大小的算子，不依赖 MP，将大型算子分解为由原始算子的参数切片组成的数学上等价的较小线性算子序列，逐个获取和释放每个切片的参数和梯度</li></ul><h3 id="Design-for-Excellent-Training-Efficiency"><a href="#Design-for-Excellent-Training-Efficiency" class="headerlink" title="Design for Excellent Training Efficiency"></a>Design for Excellent Training Efficiency</h3><p>问题：带宽不匹配：GPU &gt; CPU &gt; NVMe</p><p>必要带宽：</p><ul><li>参数和梯度：70GB/s</li><li>优化器状态：1.5TB/s</li><li>激活值 checkpoint：1-4 GB/s</li></ul><h4 id="优化参数和梯度效率"><a href="#优化参数和梯度效率" class="headerlink" title="优化参数和梯度效率"></a>优化参数和梯度效率</h4><p>提出了优化两种方案</p><ul><li>基于带宽的划分策略（没明白，看懂代码后再来更）</li><li>重叠计算通信</li></ul><h4 id="提高优化器状态效率"><a href="#提高优化器状态效率" class="headerlink" title="提高优化器状态效率"></a>提高优化器状态效率</h4><p>基于 ZeRO-Offload ，可以利用聚合的 GPU 和 CPU 内存带宽以及聚合的 CPU 计算来进行优化器更新，不同是的，在 NVMe offload 时，需要将数据从 NVMe 传输到 CPU 内存，并以适合 CPU 内存的块大小进行优化器更新，一次一个块地进行，这会受到 NVMe → CPU 带宽的限制，可以多个节点聚合 NVMe 带宽</p><p>不足：可能会导致 CPU 内存碎片化（PyTorch 2.0 提出了新的内存碎片整理策略，后面有时间看看）</p><h4 id="优化激活值效率"><a href="#优化激活值效率" class="headerlink" title="优化激活值效率"></a>优化激活值效率</h4><p>每个 GPU 通过 PCIe 以 3GB/s 的速度并行读写数据到 CPU，可以超过 80% 的效率（对于隐藏大小大于8K），减少激活值 checkpoint 频率也可以提高效率</p><h3 id="Design-for-Ease-of-Use"><a href="#Design-for-Ease-of-Use" class="headerlink" title="Design for Ease of Use"></a>Design for Ease of Use</h3><p>不同于 MP，用户不需要在设计模型时考虑能否并行</p><p>提出了两个优化策略，后面内容有实现细节</p><ul><li>automated data movement：通过向 PyTorch 子模块注入 FWD/BWD hooks 触发参数的收集和分区操作。在 FWD/BWD 之前，进行 allgather 收集数据。而在 FWD/BWD 之后，通过 hooks 触发参数和梯度的分区，可选择将它们转移到 CPU 或 NVMe</li><li>automated model partitoining：通过包装所有模块类的构造函数，使每个子模块的参数在初始化过程中进行分区和移动</li></ul><h2 id="效率优化"><a href="#效率优化" class="headerlink" title="效率优化"></a>效率优化</h2><p>下面优化上一节提出的改进点</p><h3 id="Bandwidth-Centric-Partitioning"><a href="#Bandwidth-Centric-Partitioning" class="headerlink" title="Bandwidth-Centric Partitioning"></a>Bandwidth-Centric Partitioning</h3><p>ZeRO-Infinity 提出了新的数据映射和检索策略，以解决 NVMe 和 CPU 内存带宽限制的问题</p><p>与 ZeRO 和 ZeRO-Offload 不同的是，ZeRO-Infinity 将每个层的参数划分到所有的 DP 进程中，并且在访问参数时使用 allgather 而不是 broadcast，原因如下：</p><p>当数据位于 GPU 上时， broadcast 和 allgather 通信成本相同，</p><p>但当数据位于 NVMe 或 CPU 时，</p><ul><li>如果使用 braodcast，由于每个参数完全由一个 DP 进程拥有，参数必须先通过 PCIe 从其源位置传输到 GPU 内存，然后才能 broadcast，在此过程中只有一个 PCIe 可以处于活动状态，而连接到其他所有 GPU 的 PCIe 链路都处于闲置状态</li><li>如果使用 allgather，所有 PCIe 链路都同时处于活动状态，每个链路传输 1/dp 的参数，这样 CPU/NVMe 与 GPU 之间的通信带宽会随着 dp 线性增长</li></ul><p>在大规模训练时，该方法可以提供 Infinity 的异构内存带宽，例如在 64 个 DGX-2 节点上，ZeRO-Infinity 可以获得超过 3TB/s 的 CPU 内存带宽和超过 1.5TB/s 的 NVMe 带宽</p><h3 id="Overlap-Centric-Design"><a href="#Overlap-Centric-Design" class="headerlink" title="Overlap Centric Design"></a>Overlap Centric Design</h3><p>解决在单个 GPU 或单个节点中，带宽仍然可能成为瓶颈的问题。</p><p>即使是 GPU 之间的 allgather 通信在小 batch size （图3）下也会对效率产生重大影响</p><p>此外，访问 NVMe 内存需要进行三个步骤</p><ul><li>NVMe → CPU</li><li>CPU → GPU</li><li>GPU → GPU (allgather)</li></ul><p>如果顺序执行上面步骤，效率会很差。 ZeRO-Infinity 提出了 overlap engine，包括</p><p>dynamic prefetcher：在 FWD/BWD 时，要使用参数之前，并行执行参数的移动。实时跟踪 FWD/BWDW，构建每次迭代的算子序列的内部映射，，跟踪算子序列的位置，并行加载即将执行算子的参数，如在执行算子的计算时，对即将执行的 3 个算子依次并行执行 NVMe → CPU，CPU → GPU 和 GPU → GPU。当 FWD/BWD 发生变化时，也可以更新算子序列映射</p><p>communication and offload overlapping mechanism：并行执行 BWD 与梯度更新移动。类似上面，在计算 BWD 算子时，对已经执行的 2 个算子依次并行执行梯度更新与 GPU → CPU 梯度传输</p><h3 id="Infinity-Offload-Engine"><a href="#Infinity-Offload-Engine" class="headerlink" title="Infinity Offload Engine"></a>Infinity Offload Engine</h3><h4 id="DeepNVMe"><a href="#DeepNVMe" class="headerlink" title="DeepNVMe"></a>DeepNVMe</h4><p>C++ NVMe 读/写库，异步完成的批量读/写请求，，刷新正在进行的读/写的显式同步请求。允许将这些请求与 GPU → GPU 或 GPU → CPU 通信计算重叠</p><p>实现了接近峰值的 NVMe 顺序读写带宽，包括对 I/O 请求的并行化，智能工作调度，避免数据复制和内存固定</p><h4 id="Pinned-memory-management-layer"><a href="#Pinned-memory-management-layer" class="headerlink" title="Pinned memory management layer"></a>Pinned memory management layer</h4><p>确保 NVMe → CPU 读/写 高性能的数据，数据必须保存在固定内存缓冲区中。该层通过重复使用少量（数十GB）的内存来管理有限的固定内存供应，以将整个模型状态（数十TB） offload 到 CPU/NVMe。</p><p>内存缓冲区重复使用可防止 CPU/GPU 内存碎片化，还为 PyTorch 数据提供具有固定内存数据的功能，允许在原始地址计算，然后将其写入 NVMe，无需进一步复制以提高带宽</p><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><p>ZeRO Infinity 基于 PyTorch 代码实现，并且不用重构任何模型代码</p><h3 id="Automating-Data-Movement"><a href="#Automating-Data-Movement" class="headerlink" title="Automating Data Movement"></a>Automating Data Movement</h3><p>ZeRO-Infinity 需要协调模型参数、梯度和优化器状态数据移动，必须确保这些数据在被使用前移动到 GPU 内存中，并在使用之后重新分配位置</p><p>PyTorch 模型以层级模块的形式表示，代表着神经网络的各个层次。例如，Transformer 架构中包含了诸如自注意力和前馈网络等子模块。自注意力子模块又包含了线性层和其他子模块</p><p>ZeRO-Infinity 会递归地向模型的子模块中插入 hooks ，以自动化所需的数据移动。在子模块的 FWD 开始时，这些 hooks 会确保子模块的参数可用于计算，否则它们将执行 allgather 操作，并阻塞到参数可用</p><p>类似上文介绍的基于重叠的设计，在子模块的 FWD/BWD 结束时，再次对参数进行分区，并可选择将其转移，减少参数通信时的阻塞</p><h3 id="Auto-Registration-of-External-Parameters"><a href="#Auto-Registration-of-External-Parameters" class="headerlink" title="Auto Registration of External Parameters"></a>Auto Registration of External Parameters</h3><p>在理想情况下，一个子模块的参数和梯度只在自己的 FWD/BWD 中使用，这样可以很容易地识别并自动化数据移动，但某些模型架构例外，其中在一个子模块中定义和分配的参数在不同子模块的 FWD/BWD 中都被使用</p><p>例如，GPT 等模型在网络的开头和结尾都共享 embedding 层的权重</p><p>将上面这种跨模块边界使用的参数称为外部参数，这很难知道在一个子模块的 FWD/BWD 的开始时应该收集哪些参数</p><p>于是提出了将外部参数手动注册到 ZeRO-Infinity 中，以便在访问它们的子模块的 FWD/BWD 中进行 gather，注册后，外部参数将像其他参数一样，将被包含在 dynamic prefetcher 中</p><p>此外还提供了检测这些场景并自动注册外部参数机制，不用更改任何代码</p><ul><li>截获分区参数访问：PyTorch 模块将其数据参数存储在哈希表中。在初始化时，使用子类类型替换哈希表，覆盖数据的访问方法。当访问一个分区参数时，对该参数进行阻塞的 allgather，将其注册为外部参数，然后返回收集到的参数</li><li>激活信息检测：子模块的 FWD 可能会返回一个参数，供另一个子模块的 FWD/BWD 使用。例如，Megatron-LM 中线性层 FWD 后返回 bias，在父 Transformer 层模块中使用，再检查每个子模块 FWD 返回的激活值输出中是否包含分区参数，如果包含，则对其进行收集和注册，作为外部参数</li></ul><h3 id="Automatic-Model-Partitioning-during-Initialization"><a href="#Automatic-Model-Partitioning-during-Initialization" class="headerlink" title="Automatic Model Partitioning during Initialization"></a>Automatic Model Partitioning during Initialization</h3><p>需要在初始化时划分模型的每个层对应的参数，而不是在整个模型初始化之后再进行划分，以节省峰值内存</p><p>提供了一个 Python ZeRO-Infinity 上下文，用于修饰 torch.nn.Module 的 <strong>init</strong> 方法，在每个模块初始化之后，立即将其分配的参数划分在所在进程组中</p><p>只有单独的子模块在完全初始化之后才会被划分，整个模型不会在所有并行进程上复制。一个拥有 5 千亿个参数的模型只需要 1TB 的聚合 CPU 内存就可以在初始化期间完全划分</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>集群：V100 SXM3 32 GB GPU * 512</p><p>通信带宽：800 Gbps</p><p>no MP benchmark：torch DDP</p><p>MP benchmark：Megatron-LM</p><p>模型：GPT（s = 1024）</p><p>模型配置如下</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/6.webp" width="800" /></p><h3 id="Model-Size"><a href="#Model-Size" class="headerlink" title="Model Size"></a>Model Size</h3><p>Infinity：32 万亿参数</p><p>3D 并行：6500 亿参数</p><p>图5a，提升 50 倍</p><h3 id="Model-Speed"><a href="#Model-Speed" class="headerlink" title="Model Speed"></a>Model Speed</h3><p>Infinity 训练 20 万亿参数时，与 3D 并行有几乎相同的吞吐量</p><p>进一步增加模型大小，由于3D并行的内存限制，会 OOM，而 Infinity 吞吐量为 49 TFlops/GPU</p><p>在极限情况下，由于 CPU 内存有限，导致每个 GPU 的 batch size 非常小，图5a显示性能下降，可以通过增加 CPU 内存或者将激活值 checkpoint 转移到 NVMe 中改善</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/7.webp" width="800" /></p><h3 id="Superlinear-Scalability"><a href="#Superlinear-Scalability" class="headerlink" title="Superlinear Scalability"></a>Superlinear Scalability</h3><p>通过有效利用聚合 PCIe 和 NVMe 带宽的线性增加加速参数和优化器状态的 offload，并利用额外节点的 CPU 计算进行参数更新，实现超线性扩展（图5b）</p><p>即使规模较小时，ZeRO-Infinity 也只需 4 个节点就实现了超过 2.8 petaflops，44 Tflops/GPU，聚合 NVMe 不高的带宽足够用于 Infinity</p><h3 id="Democratizing-Large-Model-Training"><a href="#Democratizing-Large-Model-Training" class="headerlink" title="Democratizing Large Model Training"></a>Democratizing Large Model Training</h3><p>图5c 在单个节点，16个 GPU 上，使用 Infinity 可以训练超过1万亿参数，27 TFlops/GPU，3D并行和 ZeRO-Offload 无法训练超过 200 亿参数</p><h3 id="Impact-of-System-Features-on-Model-Scale"><a href="#Impact-of-System-Features-on-Model-Scale" class="headerlink" title="Impact of System Features on Model Scale"></a>Impact of System Features on Model Scale</h3><p>图6a 对比了不同策略，利用不同设备在单节点训练时能达到的最大参数量</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/8.webp" width="500" /></p><p>利用 CPU：70B</p><p>利用 CPU + NVMe：1T</p><p>通过将总 GPU 内存预先分割成 2GB 的连续块，使得所有大于2GB的内存分配请求都将失败，以此测试最大 hidden size</p><p>图6b memory-centric tiling 系数为 16 时可训练 64k 的最大 hidden size</p><h3 id="Impact-of-System-Features-on-Performance"><a href="#Impact-of-System-Features-on-Performance" class="headerlink" title="Impact of System Features on Performance"></a>Impact of System Features on Performance</h3><p>图6c 比较了使用 Infinity 和 Offload 将梯度卸载到 CPU 内存对 8B 模型的 BWD 时间的影响，Offload 受限于单个 PCIe 带宽，在 64 GPU时，只有 Infinity 50% 的速度</p><p>图6d 比较了重叠通信与否对速度的影响，bz 越大，重叠通信的影响越小</p><p>图6e 比较了 offload 不同规模的 hidden 层 checkpoint 到 CPU 的收益，hidden size 越大收益越小</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/9.webp" width="800" /></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>带宽是制约计算能力的一大关键因素，Infinity 在利用 CPU 和 NVMe 设备的同时，聚合所有通信设备的带宽提速</p><p><img src="/image/论文阅读-ZeRO-Infinity-Breaking-the-GPU-Memory-Wall-for-Extreme-Scale-Deep-Learning/10.webp" width="500" /></p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: ZeRO-Offload: Democratizing Billion-Scale Model Training</title>
      <link href="/2023/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/"/>
      <url>/2023/10/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>ZeRO-Offload 主要优化在于尽量减少数据在 GPU 与 CPU 之间的移动，并减少 CPU 计算时间，同时最大限度地节省 GPU 上的内存。</p><span id="more"></span><p>原文链接：<a href="https://arxiv.org/pdf/2101.06840.pdf">https://arxiv.org/pdf/2101.06840.pdf</a></p><p>开源代码：<a href="https://github.com/microsoft/deepspeed">https://github.com/microsoft/deepspeed</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>使用 Pytorch 训练内存不够，单个 V100 上性能为 30 TFlops</p><p>目前的分布式训练框架动辄需要超过 10w 刀的设备，难以在单卡上发挥作用</p><p>对于基于注意力机制的 LLM 训练，内存瓶颈主要在模型状态上</p><p>现有的异构训练方法主要利用 CPU 内存，而忽略了 CPU 的计算潜力</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>ZeRO-Offload 优化：尽量减少数据在 GPU 与 CPU 之间的移动，并减少 CPU 计算时间，同时最大限度地节省 GPU 上的内存</p><p>单个 V100 上性能提高到 40 TFlops，在 128 卡上实现接近线性加速，对比MP，模型规模增加了 4.5 倍</p><p>ZeRO-Offload 利用了CPU内存和计算资源进行 Offload，并与 ZeRO-DP 相结合</p><h3 id="Efficiency"><a href="#Efficiency" class="headerlink" title="Efficiency"></a>Efficiency</h3><p>论文提出了一种名为Efficiency的offload策略，通过分析确定了CPU和GPU设备之间的最佳计算和数据划分策略，以在三个关键方面达到最优化：</p><ul><li>在CPU上的计算量比GPU少多个数量级，防止CPU性能瓶颈；</li><li>最小化CPU和GPU之间的通信量，防止通信瓶颈；</li><li>在实现最小通信量的同时，可证明地最大化节约GPU内存。 </li></ul><p>分析表明，要在上述方面达到最优，必须将梯度、优化器状态和优化器计算卸载到CPU上，同时在 GPU 上保留参数、前向和反向计算。这种策略使模型规模增加了10倍，并且通信量最小。能够在一块 V100 GPU上训练130亿参数模型，性能为 40 TFLOPS，而没有 CPU offload 的情况下只能训练12亿参数模型，且性能只有30 TFLOPS。</p><p>offload 优化器计算要求CPU进行O(M)次计算，而GPU需进行O(MB)次计算，其中M和B分别为模型规模和 batch size 。在大多数情况下， batch size 较大，CPU计算量并不是瓶颈，但对于小 batch size，CPU计算量可能成为瓶颈。为了解决这个问题，采取了两种优化措施：</p><ul><li>高效的CPU优化器，其速度比现有技术快6倍；</li><li>延迟一步的参数更新，允许将CPU优化器步骤与GPU计算重叠，同时确保准确性。这两种措施共同保证了ZeRO-Offload在小 batch size 下也能保持效率。</li></ul><h3 id="Unique-Optimal-Offload-Strategy"><a href="#Unique-Optimal-Offload-Strategy" class="headerlink" title="Unique Optimal Offload Strategy"></a>Unique Optimal Offload Strategy</h3><p>为了确定最佳的下载策略，ZeRO-Offload将深度学习训练建模为数据流图，并使用基于第一原理的分析方法将该图分割为CPU和GPU设备之间的部分。</p><p>ZeRO-Offload 在以下三个关键方面优化了图的分割策略</p><ul><li>要求更少数量级的CPU计算，以防止CPU成为性能瓶颈；</li><li>确保最小化CPU和GPU内存之间的通信量；</li><li>可证明地实现最小通信量的同时最大化内存节省。</li></ul><p>训练的计算复杂度通常为O(MB)，其中M为模型大小，B为有效batch size。为避免CPU计算成为瓶颈，只有那些计算复杂度低于O(MB)的计算才能转移到CPU上</p><p>FWD 和 BWD 的计算复杂度都是O(MB)，必须在GPU上进行，而其余的计算，如范数计算、权重更新等，其复杂度为O(M)，可以转移到CPU上</p><p>基于此，数据流图中的前向传播和反向传播节点合并为一个 FWD-BWD Super Node 并分配到 GPU 上</p><p>还需要最小化 CPU 与 GPU 的通信带宽，如图中所示，最小通信量为 BWD后 GPU 发送到 CPU 的 2M 梯度与 CPU 发送到 GPU 的 2M 参数，只有将 fp32 模型状态（momentum 32、variance 32和p32），Param Update 和 float2half 计算放置在一起，为一个 CPU 上的 Update Super Node，才能达成最小通信量策略</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/1.webp" width="800"/></p><p>另外，上图中 2M 大小的 parameter 16 为什么分别到 FWD 与 BWD 有总和 4M 的数据传输，这个问题如有朋友知道，欢迎评论区讨论</p><p>实验依据如下</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/2.webp" width="500"/></p><p>总之，ZeRO-Offload 策略就是将所有的 fp32 模型状态和 fp16 梯度存储在 CPU 内存中，并在 CPU 上计算参数更新。而 fp16 参数则保留在GPU上，前向和反向计算在GPU上进行</p><h2 id="ZeRO-Offload-Schedule"><a href="#ZeRO-Offload-Schedule" class="headerlink" title="ZeRO-Offload Schedule"></a>ZeRO-Offload Schedule</h2><h3 id="单卡策略"><a href="#单卡策略" class="headerlink" title="单卡策略"></a>单卡策略</h3><p>ZeRO-Offload将数据进行分区，将fp16参数存储在GPU上，fp16梯度和所有优化器状态（如fp32动量、方差和参数）存储在CPU上。</p><p>在训练过程中，首先通过 FWD 计算损失。由于fp16参数已经位于GPU上，因此这部分计算不需要与CPU进行通信。</p><p>在 BWD 过程中，不同参数的梯度在后向调度的不同位置计算。ZeRO-Offload可以立即将这些梯度逐个或切分传输到 CPU 内存中。</p><p>因此，在将梯度传输到CPU内存之前，只需要在GPU内存中临时保存少量的梯度。此外，每个梯度传输可以与反向计算重叠，消除大部分通信成本。</p><p>在 BWD 之后，ZeRO-Offload在CPU上直接更新fp32参数和剩余的优化器状态（如动量和方差），并将更新后的 fp32 参数从 CPU 内存复制到 GPU 内存中的fp16参数中。</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/3.webp" width="800"/></p><p>上图展示了 ZeRO-Offload 每个步骤中的计算通信过程，下图伪代码展示了具体的调度过程</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/4.webp" width="500"/></p><h3 id="多卡策略"><a href="#多卡策略" class="headerlink" title="多卡策略"></a>多卡策略</h3><p>ZeRO-Offload 将梯度和优化器状态在不同的 GPU 之间进行 partition，并且每个 GPU 将自己的 part offload 到 CPU 内存中，存储持续整个训练过程</p><p>BWD 过程中，在 GPU 上 reduce-scatter 计算梯度并平均，每个 GPU 仅将属于其 part 的平均梯度 offload 到 CPU 内存中</p><p>一旦梯度在 CPU 上可用，优化器状态 part 对应的每个 DP 进程直接在 CPU 上并行更新对应的参数 part</p><p>更新完成后，参数 part 发送到 GPU，在 GPU 上对参数进行类似 ZeRO-2 的 all-gather 操作</p><p>下面是 ZeRO-Offload 的多卡数据布局示意图</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/5.webp" width="800"/></p><p>此外，ZeRO-Offload 还可以与 MP 和 Megatron-LM 同时使用，通过 offload MP 进程对应的梯度、优化器状态和优化器计算实现</p><h2 id="Optimized-CPU-Execution"><a href="#Optimized-CPU-Execution" class="headerlink" title="Optimized CPU Execution"></a>Optimized CPU Execution</h2><h3 id="Implementing-the-CPU-Optimizer"><a href="#Implementing-the-CPU-Optimizer" class="headerlink" title="Implementing the CPU Optimizer"></a>Implementing the CPU Optimizer</h3><p>优化 CPU Optimizer 性能</p><ul><li>使用SIMD向量指令</li><li>循环展开</li><li>OMP 多线程并行计算</li><li>混合精度训练</li></ul><p>在混合精度训练模式下，内存中的参数有两个版本</p><ul><li>FP32 版本，用于优化器在 CPU 上进行更新</li><li>浮点转换 FP32  后用于在 FWD 中计算激活的 FP16，用于GPU</li></ul><p>此外，梯度的动量和方差也以 FP32 保存在 CPU 上，防止在更新参数时出现精度丢失</p><p>论文中的 adam 实现使用了 SIMD，通过将数据读入向量寄存器并使用多个融合乘加操作（FMA）构成主执行流水线，SIMD 矢量宽度由 simd_width 指定，并结合循环展开，自动调优的 unroll_width 为 8</p><p>同时，对计算的数据按 tile_width 分区，将 CPU上计算好的 FP32 数据 part 转换为 FP16 拷贝到 GPU 中，同时在 CPU 上计算下一个 数据 part，实现通信计算 overlap</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/6.webp" width="600"/></p><h3 id="One-Step-Delayed-Parameter-Update（DPU）"><a href="#One-Step-Delayed-Parameter-Update（DPU）" class="headerlink" title="One-Step Delayed Parameter Update（DPU）"></a>One-Step Delayed Parameter Update（DPU）</h3><p>在 DPU 训练过程中，首先在前 N-1 个步骤中进行训练，避免在训练的早期阶段梯度变化较快时破坏训练的稳定性。</p><p>在第N步，从 GPU 获取梯度，但跳过 CPU 优化器步骤，也不更新 GPU 上的 fp16 参数。</p><p>在第 N+1 步，使用第N步的梯度在CPU上计算参数更新，在GPU上并行计算前向和后向传播，使用第 N-1 步更新的参数。</p><p>从这一步开始，每个步骤的模型将使用从 i-1 步更新的参数进行训练，实现了 CPU 与 GPU 的计算重叠</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/7.webp" width="1000"/></p><p>作者通过对多个训练工作评估发现，如果在几十次迭代之后引入 DPU，不会对收敛产生影响</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h3><p>硬件配置</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/8.webp" width="500"/></p><p>实验模型：GPT-2、BERT</p><p>实验数据集：SQuAD</p><p>对比框架：PyTorch DDP、Megatron、L2L、ZeRO</p><p>模型配置</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/9.webp" width="500"/></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>图7 在单个GPU，4个GPU和16个GPU（一个DGX-2节点）上可训练的最大模型参数量对比</p><p>图8 在 batch size 为 512 的单个GPU上使用 PyTorch，L2L 和 ZeRO-Offload 训练的 TFLOPS 对比</p><p>图9 使用 DPU 和不使用 DPU 的情况下，与 GPT-2 训练的 TFLOPS（batch_size = 8） 对比</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/10.webp" width="800"/></p><p>图10 针对不同模型大小，使用PyTorch、ZeRO-2、MegatronLM、ZeRO-Offload（无 MP）以及ZeRO-Offload（有 MP）训练的 TFLOPS 对比</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/11.webp" width="600"/></p><p>图11 针对不同 GPU 数量，ZeRO-2 和 ZeRO-Offload 的 TFLOPS 对比，从 64 卡开始 ZeRO-2 优于 ZeRO-Offload 是因为 ZeRO-2 没有 CPU-GPU 通信的额外开销</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/12.webp" width="600"/></p><p>本文优化过的 CPU-Adam 与 PT-GPU 性能差距并不大，不会成为性能瓶颈</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/13.webp" width="600"/></p><p>实验表明 DPU 策略在训练与微调上不会影响精度，最终的 F1 分数都是 0.92</p><p><img src="/image/论文阅读-ZeRO-Offload-Democratizing-Billion-Scale-Model-Training/14.webp" width="600"/></p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记录第n次修复USB设备无法识别挂载</title>
      <link href="/2023/10/11/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E4%BF%AE%E5%A4%8DUSB%E8%AE%BE%E5%A4%87%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E6%8C%82%E8%BD%BD/"/>
      <url>/2023/10/11/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E4%BF%AE%E5%A4%8DUSB%E8%AE%BE%E5%A4%87%E6%97%A0%E6%B3%95%E8%AF%86%E5%88%AB%E6%8C%82%E8%BD%BD/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>archlinux 滚着滚着就不能识别 USB 设备了，记录一下修复过程</p><span id="more"></span><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>一个U盘</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>使用 <code>lsusb</code> 可以识别到<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Bus 004 Device 006: ID 0dd8:2004 Netac Technology Co., Ltd OnlyDisk</span><br></pre></td></tr></table></figure></p><p><code>sudo dmesg | grep USB</code> 显示如下，可以识别到</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[418536.174864] usb 4-2: New USB device found, idVendor=0dd8, idProduct=2004, bcdDevice= 1.10</span><br><span class="line">[418536.174868] usb 4-2: New USB device strings: Mfr=2, Product=3, SerialNumber=4</span><br></pre></td></tr></table></figure><p><code>lsblk</code> 显示如下，没有USB设备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS</span><br><span class="line">nvme0n1     259:0    0 931.5G  0 disk </span><br><span class="line">├─nvme0n1p1 259:1    0   260M  0 part /boot</span><br><span class="line">└─nvme0n1p2 259:2    0 931.3G  0 part /</span><br></pre></td></tr></table></figure><p><code>sudo fdisk -l</code>  显示如下，没有USB设备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Disk /dev/nvme0n1：931.51 GiB，1000204886016 字节，1953525168 个扇区磁盘型号：CT1000P2SSD8                            </span><br><span class="line">单元：扇区 / 1 * 512 = 512 字节扇区大小(逻辑/物理)：512 字节 / 512 字节I/O 大小(最小/最佳)：512 字节 / 512 字节磁盘标签类型：gpt</span><br><span class="line">磁盘标识符：C23A898B-ABD0-4208-9AF0-6BB0F443BD75</span><br><span class="line"></span><br><span class="line">设备             起点       末尾       扇区   大小 类型/dev/nvme0n1p1   2048     534527     532480   260M EFI 系统/dev/nvme0n1p2 534528 1953525134 1952990607 931.3G Linux 文件系统</span><br></pre></td></tr></table></figure><p><code>df -h</code> 显示如下，没有usb设备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dev             6.8G     0  6.8G    0% /dev</span><br><span class="line">run             6.8G  1.8M  6.8G    1% /run</span><br><span class="line">/dev/nvme0n1p2  931G  433G  499G   47% /</span><br><span class="line">tmpfs           6.8G  144M  6.7G    3% /dev/shm</span><br><span class="line">tmpfs           6.8G   31M  6.8G    1% /tmp</span><br><span class="line">/dev/nvme0n1p1  256M  255M  1.7M  100% /boot</span><br><span class="line">tmpfs           1.4G  5.8M  1.4G    1% /run/user/1000</span><br></pre></td></tr></table></figure><p>总结 <code>lsusb</code>，<code>sudo dmesg | grep USB</code> 可以识别，<code>lsblk</code>，<code>fdisk -l</code>，<code>df -h</code> 不能识别到，推测是因为系统没有自动挂载</p><h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>安装 <code>udisks2</code> 和 <code>udevil</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pacman -S udisks2 udevil</span><br></pre></td></tr></table></figure><p>启用<code>devmon</code>服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo systemctl enable devmon@$(whoami).service</span><br></pre></td></tr></table></figure><p><code>reboot</code> 后即可生效</p>]]></content>
      
      
      <categories>
          
          <category> archlinux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> USB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</title>
      <link href="/2023/09/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/"/>
      <url>/2023/09/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>ZeRO 是一种用于大规模深度学习模型的内存优化解决方案，通过消除数据和模型并行训练中的内存冗余，同时保持了低通信量和高计算粒度。</p><span id="more"></span><p>paper: <a href="https://arxiv.org/abs/1910.02054">https://arxiv.org/abs/1910.02054</a></p><p>code: <a href="https://github.com/microsoft/DeepSpeed">https://github.com/microsoft/DeepSpeed</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>模型训练的挑战：</p><p>传统的 DP 方法不能减少每个设备的内存使用量，导致在当时 32G 显存的 GPU 上无法训练超过 14 亿个参数的模型</p><p>现有的方法：流水线并行，模型并行， CPU offloading，在功能性，内存使用，计算通信效率之间做出了取舍</p><p>作者提到模型并行最有潜力，需要将模型在垂直方向上进行切分，并将每个层的计算和参数在多个设备之间进行划分，当时的 11B 参数的 T5，8.3B 的 Megatron-LM 都使用了模型并行。但是这需要大量的层间通信，虽然在单个节点内表现良好，作者在两个 DGX-2 节点上使用 Megatron-LM 测试了 40B 的模型，每个 V100 GPU 的计算性能仅为 5 TFLOPS</p><p>作者发现内存消耗主要集中在</p><ul><li>模型状态：优化器，梯度，参数</li><li>residual states：激活值，buffer，内存碎片</li></ul><p>针对这两个部分提出了 ZeRO（Zero Redundancy Optimizer） </p><h2 id="引论"><a href="#引论" class="headerlink" title="引论"></a>引论</h2><h3 id="优化模型状态内存"><a href="#优化模型状态内存" class="headerlink" title="优化模型状态内存"></a>优化模型状态内存</h3><p>数据并行 （DP）不足：不需要频繁地通信，但需要在每个进程中复制整个模型状态，浪费内存</p><p>模型并行（MP）不足：对模型状态分区以高效率使用内存，但会导致过细的计算粒度，需要频繁通信</p><p>整体来看，上述方法需要在整个训练过程中静态地维护所有模型状态，但在训练时不总是需要所有的模型状态</p><p>改进：</p><p>提出了  ZeRO-DP，ZeRO-powered  DP ，通过分区模型状态而不是复制来消除 DP 时的内存冗余，并使用动态通信调度策略优化计算通信效率，提出了三个优化阶段</p><ul><li>优化器分区（Pos）：内存减少4倍，与DP具有相同的通信量</li><li>梯度分区（Pos+g）：内存减少8倍，与DP具有相同的通信量</li><li>模型参数分区（Pos+g+p）：内存减少量与 DP 的份数成线性关系。如在 64 个 GPU 上拆分将减少64倍内存。通信仅增加 50%</li></ul><p>图中 Ψ 表示模型参数量，K 表示优化器状态的内存倍数，Nd 表示 DP 份数。在该示例中，假设基于 Adam 优化器的混合精度训练，Ψ=7.5B，Nd=64，K=12，其中的详细计算方法后面会介绍</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/1.webp" width="800" /></p><p>作者在这里计算了通过上述优化在 1024 张 GPU 上训练万亿参数模型，使用 fp16，需要 16TB 空间存储优化器状态，梯度和参数，每张卡占用 16GB 显存</p><h3 id="优化-residual-states-内存"><a href="#优化-residual-states-内存" class="headerlink" title="优化 residual states 内存"></a>优化 residual states 内存</h3><p>为了优化激活值，buffer，内存碎片的占用，提出了 ZeRO-R，有以下几个优化点</p><ul><li>使用对激活值的 checkpointing 来节省内存，还对激活值切片，根据需要将激活数据转移至 CPU 来优化激活值的内存占用</li><li>ZeRO-R 定义了适当的临时缓冲区大小，使内存和计算效率平衡</li><li>根据不同 tensor 的生命周期，主动管理内存，预防内存碎片化</li></ul><p>综上，ZeRO 主要是由 ZeRO-DP 和 ZeRO-R 两种优化结合</p><h2 id="ZeRO-搭配模型并行（MP）"><a href="#ZeRO-搭配模型并行（MP）" class="headerlink" title="ZeRO 搭配模型并行（MP）"></a>ZeRO 搭配模型并行（MP）</h2><p>虽然使用 ZeRO 的策略后，MP 的策略变得不那么重要了，MP 在使用时还需要修改模型，相比 DP，有诸多限制</p><p>但在激活内存占用非常大时，这时 ZeRO-R 的策略也不能满足训练优化，可以搭配 MP 减少激活内存占用。并且，在结合 ZeRO 和 MP 时，理论上可以优化到 Nd * Nm（MP 份数） 倍的内存占用</p><p>在小模型的情况下，单独使用 DP 会导致 batch size 过大可能无法收敛，使用 MP 可以在加速的同时减小 batch size 到合适的值，帮助模型收敛</p><h3 id="Implementation-amp-Evaluation"><a href="#Implementation-amp-Evaluation" class="headerlink" title="Implementation &amp; Evaluation"></a>Implementation &amp; Evaluation</h3><p>作者进行了一些上述工作的实验，得出如下结论</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/2.webp" width="800" /></p><ul><li><p>模型大小：将 Megatron 与 MP 相结合，ZeRO-100B 可以高效地运行 1700 亿参数的模型，而单独使用 Megatron 等现有系统在 40 亿参数以上的规模上无法高效扩展。相比于SOTA，模型大小增加了8倍以上</p></li><li><p>训练速度：改进的内存效率提高了吞吐量和训练速度。通过在 400 台 Nvidia V100 GPU 上运行，ZeRO 可以以每个 GPU 38 TFlops 的速度训练 1000 亿参数的模型，总性能超过 15 Petaflops。与 SOTA 相比，对于相同模型大小，训练速度提高了 10 倍以上</p></li><li><p>可扩展性：当使用64-400个GPU时，性能呈现超线性加速，即当GPU数量加倍时，性能增加了一倍以上。这是 ZeRO-DP 的特性，它随着DP度数的增加减少了模型状态的内存占用，使得每个GPU能够承载更大的 batch_size ，从而提高性能</p></li><li><p>大规模训练可行性：ZeRO-100B 使得 130亿参数的模型只需重构即可训练。相比之下，现有系统（如PyTorch Distributed Data Parallel）在 14 亿参数的规模上就会出现内存不足的情况</p></li><li><p>SOTA：ZeRO 支持拥有 170 亿参数的 Turing-NLG 模型，并取得了 SOTA 的成绩</p></li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="DP-TP-and-PP"><a href="#DP-TP-and-PP" class="headerlink" title="DP, TP and PP"></a>DP, TP and PP</h3><p>先了解一下 3 种并行的概念</p><h4 id="DP（数据并行）"><a href="#DP（数据并行）" class="headerlink" title="DP（数据并行）"></a>DP（数据并行）</h4><p>将每批输入的训练数据都在 DP 的 worker 之间进行平分。反向传播之后，需要进行通信来规约梯度，以保证优化器在各个 worker 上可以得到相同的更新</p><p>优势，计算效率高，工程上易于实现</p><p>不足：会在所有 worker 之间复制模型和优化器，因此显存效率不高。且随着并行度的提高，每个 worker 执行的计算量是恒定的。 DP 可以在小规模上实现近乎线性扩展。但是，因为在 worker 之间规约梯度的通信成本跟模型大小成正相关，所以当模型很大或通信带宽很低时，计算效率会受到限制</p><p>改进：梯度累积可以增加 batch size，在本地使用 micro-batch 进行多次正向和反向传播，在进行优化器更新之前再规约梯度，从而分摊通信成本</p><h4 id="TP（模型并行）"><a href="#TP（模型并行）" class="headerlink" title="TP（模型并行）"></a>TP（模型并行）</h4><p>在多个 worker 之间划分模型的各个层。模型并行的计算和通信因模型结构而异，因此需要很大的工作量来实现。DeepSpeed 利用了 Megatron-LM 来构建基于 Transformer的大规模模型并行语言模型</p><p>优势：会根据 worker 数量成比例地减少显存使用，这是这三种并行模式中显存效率最高的。且可以通过在模型并行 worker 之间划分激活显存，减少显存占用</p><p>不足：每次前向和反向传播中都需要额外通信来传递激活，模型并行的计算效率很低。模型并行需要高通信带宽，并且不能很好地扩展到通信带宽受限的单个节点之外。此外，每个模型并行worker都会减少每个通信阶段之间执行的计算量，从而影响计算效率</p><h4 id="PP（流水线并行）"><a href="#PP（流水线并行）" class="headerlink" title="PP（流水线并行）"></a>PP（流水线并行）</h4><p>将模型的各层划分为可以并行处理的阶段。当一个阶段完成一个 micro-batch 的正向传播时，激活内存将被发送给流水线的下一个阶段。类似地，当下一阶段完成反向传播时，将通过流水线把梯度反向传递回来。为了确保流水线的各个阶段能并行计算，必须同时计算多个 micro-batch</p><p>在 PipeDream 的实现中，通过保留多份旧参数来隐藏流水线泡沫，而且不会过多增加 batch size，本文通过梯度累积来实现并行的方法，在相同的 batch size下可以达到与传统 DP 和模型并行相同的训练效果</p><p>优势：流水线并行减少的显存与流水线的阶段数成正比，这使模型的大小可以随 worker 的数量线性扩展，并且通过 micro-batch 可以有效减少 bubble。此外，流水线的通信量只和阶段边界的各层的激活值大小成正比，所以流水线并行的通信量最低</p><p>不足：每个 worker 必须同时存储并运行的各个 micro-batch 的激活值，导致流水线第一阶段的激活内存与单个 mirco-batch 的总激活内存大致相同。不断增加流水线大小会减少每个流水线阶段的计算量，降低计算通信效率。流水线并行还对每个阶段的负载均衡有很高的要求。此外，由于水平拆分和 micro-batch，如tied-weight 和 batch-normalization 等功能难以实现</p><h3 id="非并行化方法节省内存"><a href="#非并行化方法节省内存" class="headerlink" title="非并行化方法节省内存"></a>非并行化方法节省内存</h3><h4 id="减少激活内存"><a href="#减少激活内存" class="headerlink" title="减少激活内存"></a>减少激活内存</h4><p>作者引用了几篇文献举例如何优化激活内存</p><h5 id="压缩内存"><a href="#压缩内存" class="headerlink" title="压缩内存"></a>压缩内存</h5><p>论文主要研究了在深度神经网络（DNNs）的训练中，GPU主内存限制导致训练更深层次的网络时出现瓶颈的问题。研究发现，主要的内存占用问题来自于中间层的输出（特征图）。为了解决这个问题，论文提出了一种DNN层特定的优化框架，通过对特定层（如卷积、ReLU、池化）进行编码和解码，以显著降低GPU主内存的压力。其核心方法是在两个时间点之间存储特征图的编码表示，并在反向传播时解码，而在前向传播时使用完整的特征图。作者还介绍了名为Gist的系统，它采用两类层特定的编码方案（无损和有损），利用DNN训练中现有的数值冗余，显著减少了目标特征图的内存消耗。例如，通过充分利用从池化到ReLU层的反向传播的计算特性，可以将中间特征图的存储精简至每个值仅使用1位而不是32位。通过在一流的DNN框架（CNTK）中部署这些机制，Gist在5个最先进的图像分类DNN上将内存占用降低了最多2倍，平均降低了1.8倍，仅带来4%的性能开销。此外，论文还表明，进一步的软件（例如CuDNN）和硬件（例如动态分配）优化可以进一步降低内存占用，最多可减少4.1倍</p><h5 id="激活内存-checkpoint"><a href="#激活内存-checkpoint" class="headerlink" title="激活内存 checkpoint"></a>激活内存 checkpoint</h5><p>论文提出了一种系统性方法，旨在减少深度神经网络训练的内存消耗。主要关注点是减少存储中间结果（特征图）和梯度所需的内存成本，因为与许多常见的深度架构中的中间特征图相比，参数的大小相对较小。论文使用计算图分析进行自动原地操作和内存共享优化。论文还提出了一种新的方法来以计算为代价来换取内存，用于特征图训练n层网络，成本为O(√n)的内存，仅需双倍的前向传播计算成本。在极端情况下，可以使用仅O(logn)的内存来训练n层网络的特征图</p><p>作者介绍了他们开发的系统 Checkmate，该系统旨在解决张量重新生成调度的最佳化问题。Checkmate 可以在合理的时间内（不到一小时）使用现成的MILP（Mixed Integer Linear Programming）求解器或使用近似算法找到接近最优的调度方案。这些调度方案可以用于加速数百万次的训练迭代。除了减少训练成本外，Checkmate还可以使现实世界中的网络能够使用比以前大约多5.1倍的输入尺寸进行训练</p><h5 id="实时内存管理"><a href="#实时内存管理" class="headerlink" title="实时内存管理"></a>实时内存管理</h5><p>作者提出了SuperNeurons 动态的GPU内存调度运行时策略，包括三种内存优化技术：Liveness Analysis（存活性分析）、Unified Tensor Pool（统一张量池）和 Cost-Aware Recomputation（成本感知的重计算）。这些技术共同有效地将网络整体的内存峰值使用量降低到各层中的最大内存使用量。此外，SuperNeurons还解决了这些内存节省技术中的性能问题。鉴于有限的GPU DRAM，SuperNeurons不仅提供了训练所需的内存，还动态分配内存用于卷积工作空间，以实现高性能的训练。SuperNeurons 能够在一个12GB的 K40c GPU上训练包含104个基本网络层的 ResNet2500</p><p>本文的 ZeRO-R 同时使用了压缩内存和 checkpoint 技术</p><h3 id="CPU-Offload"><a href="#CPU-Offload" class="headerlink" title="CPU Offload"></a>CPU Offload</h3><p>这里引用了其它两种内存卸载方法</p><h4 id="优化执行算法"><a href="#优化执行算法" class="headerlink" title="优化执行算法"></a>优化执行算法</h4><p>论文介绍了一种名为”L2L”的新型执行算法，它采用一种中继式执行技术。在任何给定时刻，设备内存主要仅填充了正在执行的层的占用空间。模型驻留在DRAM内存中，连接到CPU或FPGA，作为一种称为”eager param-server（EPS）”的实体。为了解决将参数传递到EPS的带宽问题，该模型采用了一种逐层执行的方式，而不是传统的小批量执行整个模型的方法。这意味着模型以多个微批次的方式执行，而不是传统的小批量方式。这种方法可以显著减少内存占用，并提高吞吐量</p><h4 id="虚拟内存-Offload"><a href="#虚拟内存-Offload" class="headerlink" title="虚拟内存 Offload"></a>虚拟内存 Offload</h4><p>论文提出了一种名为 vDNN 的虚拟化DNN，主要思想是保守地分配GPU内存，以立即使用给定层的计算，从而大幅减少最大和平均内存使用。vDNN 利用分配的数据结构的数据依赖性，特别是占内存使用量大部分的中间特征映射，在 GPU 和 CPU 内存之间释放或移动这些中间数据。具体来说，如果没有进一步的重用，就积极地从 GPU 内存中释放这些特征映射，如果存在进一步的重用，但不是立即需要，就从CPU内存卸载/预取。通过 DNN 络的层间内存访问和重用模式，内存管理器智能地将正常的 DNN 计算与卸载/预取/释放 操作重叠，几乎没有性能损失</p><p>上面的文献利用计算节点的异构性，分别通过算法设计或虚拟化内存将模型状态转移到CPU内存。但是这导致有50%的时间被浪费在GPU-CPU-GPU传输。ZeRO的不同之处在于，它显著降低了内存消耗，而无需将模型状态存储到CPU内存中。在极少数情况下，ZeRO-R可能只针对非常大的模型才 offload 激活 checkpoint，以提高性能</p><h3 id="Memory-Efficient-Optimizer"><a href="#Memory-Efficient-Optimizer" class="headerlink" title="Memory Efficient Optimizer"></a>Memory Efficient Optimizer</h3><p>自适应梯度优化器如 Adagrad 和 Adam 方法在 NLP 任务中取得了不错的性能，然而这些方法为每个参数维护了二阶统计信息，因此引入了显著的内存开销，限制了可使用的模型大小以及每个小批量中的示例数量。下面是过去提出的优化内存的文献</p><h4 id="自适应学习率"><a href="#自适应学习率" class="headerlink" title="自适应学习率"></a>自适应学习率</h4><p>论文提出了一种名为 Adafactor 的自适应学习率优化算法，为了减少内存占用，仅维护每行和每列的移动平均和二阶矩的和，然后基于这些和来估计每个参数的二阶矩。这种方法在实验中表现出与传统方法相似的结果。最后，作者还提出了根据参数本身的规模来调整参数更新的方法</p><h4 id="自适应优化器"><a href="#自适应优化器" class="headerlink" title="自适应优化器"></a>自适应优化器</h4><p>作者提出了一种自适应优化算法 SM3，该算法通过使用参数的覆盖集合来减少内存需求，其中每个集合都对应一个变量，通过维护一组覆盖集合，并对每个集合的最大方差进行求和，确定每个梯度条目的学习率。通过减少内存需求实现了两倍的速度提升</p><p>上面的文献通过获取模型参数或梯度的粗粒度统计数据来减少自适应优化方法的内存消耗，这可能会对模型收敛保证产生影响。ZeRO与这些工作不同，它的优化不会改变模型优化方法或影响模型收敛，但会有效地减少每个设备的优化器状态和梯度的内存占用</p><h4 id="训练优化器"><a href="#训练优化器" class="headerlink" title="训练优化器"></a>训练优化器</h4><p>对于大型模型，自适应优化（Adaptive）方法对于达到 SOTA 性能和精度至关重要。与 SGD 相比，它以显著的内存占用为代价，维护每个模型参数和梯度的细粒度一阶和二阶统计信息。ZeRO可以将这些优化器的内存占用减少几个数量级，使这些复杂的优化方法在训练大模型时非常有效。它还允许开发和使用更复杂、内存消耗更大、收敛性更好的优化器</p><h2 id="优化内存占用"><a href="#优化内存占用" class="headerlink" title="优化内存占用"></a>优化内存占用</h2><p>前面提到内存消耗主要集中在</p><ul><li>模型状态：优化器，梯度，参数</li><li>residual states：激活值，buffer，内存碎片</li></ul><p>下面展开讨论为何会这样</p><h3 id="模型状态：优化器状态，梯度与参数"><a href="#模型状态：优化器状态，梯度与参数" class="headerlink" title="模型状态：优化器状态，梯度与参数"></a>模型状态：优化器状态，梯度与参数</h3><h4 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h4><p>在使用adam优化器训练时，Adam使用指数移动平均来计算梯度，这需要保存梯度的拷贝，以稳定更新参数。Adam还使用了自适应学习率机制，会为每个参数自动调整学习率。学习率的自适应性依赖于每个参数的梯度方差。为了计算梯度方差，就需要保存梯度的平方的移动平均值，以便在参数更新时更好地适应局部梯度的特性</p><p>在使用混合精度训练时，将参数和梯度存储为 fp16，并在前反向传播时都使用 fp16 更新，但是为了反向传播结束后保证计算的精确，需要保存参数和优化器状态的 fp32 副本，以Adam优化器为例，使用Adam对具有Ψ个参数的模型进行混合精度训练需要足够的内存来存储参数和梯度的fp16副本，内存需求分别为 2Ψ 和 2Ψ 字节。此外，还需要存储优化器状态：参数、动量和方差的 fp32 副本，内存需求分别为 4Ψ、4Ψ 和 4Ψ 字节。文中使用 K 来表示优化器状态的内存乘数，即存储它们所需的额外内存为 KΨ 字节。混合精度 Adam 的 K 值为 12。这导致了 2+2+12=16 Ψ 字节的内存需求。对于像 GPT-2 这样有15亿参数的模型，至少需要24GB的内存，远远高于 3GB 内存来存储 fp16 参数的需求</p><h3 id="Residual-内存消耗"><a href="#Residual-内存消耗" class="headerlink" title="Residual 内存消耗"></a>Residual 内存消耗</h3><p>在训练过程中，使用的激活函数会占用大量内存。以 GPT-2 模型为例，当序列长度为 1K，batch_size 为 32时，1.5B 参数的模型需要大约 60GB 的内存</p><p>计算公式：激活值内存 = Transformer 层数 × hidden_dim × batch_size × seq_len × Transformer 层数</p><p>使用激活值 checkpoint 方法可以减少激活函数内存的消耗，会增加 33% 的 recompute 开销，但可以将激活函数内存消耗降低到约 8GB 但对于更大的模型，激活函数的内存消耗仍然可能非常大。如一个拥有 1000 亿参数的 GPT-like 模型，在 batch_size 为32 时，即使使用了激活值 checkpoint 仍需60GB的内存。此外，对于大型模型，用于存储中间结果的临时 buffer 也会占用相当大的内存。例如，对梯度 all_reduce 或梯度计算时会将所有梯度融合到一个 flattened buffer 中，尽管梯度可以以 fp16 存储，但 buffer 可能还是 fp32。对于一个具有 15 亿参数的模型，一个 flattened fp32 buffer 要占用6GB的内存</p><p>此外，内存碎片的问题也要注意，在极端情况下，内存碎片可浪费 30% 的内存</p><h2 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO"></a>ZeRO</h2><p>ZeRO 提出了两组优化：</p><ul><li>ZeRO-DP：优化模型状态内存消耗</li><li>ZeRO-R：优化 Residual 内存消耗</li></ul><h3 id="ZeRO-DP"><a href="#ZeRO-DP" class="headerlink" title="ZeRO-DP"></a>ZeRO-DP</h3><p>DP：优点：计算粒度高，通信低；不足： DP 进程之间冗余存储</p><p>MP：优点：通过分区模型利用内存；不足：计算粒度降低</p><p>ZeRO-DP 通过分区模型状态并使用动态的通信调度，同时有 DP 和 MP 的优点</p><h4 id="Optimizer-State-Partitioning"><a href="#Optimizer-State-Partitioning" class="headerlink" title="Optimizer State Partitioning"></a>Optimizer State Partitioning</h4><p>在DP中，通过将优化器的状态分成N个分区，使得每个DP进程只更新对应的分区的优化器状态，也就是1/N的总优化器状态参数量</p><h4 id="Gradient-Partitioning"><a href="#Gradient-Partitioning" class="headerlink" title="Gradient Partitioning"></a>Gradient Partitioning</h4><p>梯度的计算被分为不同的分区，每个 DP 进程只处理和更新对应参数分区的梯度。文中还采用了一种 ucketization<br>策略，将同一参数分区的梯度进行分组，并一次性对整个组进行归约操作。类似于NVIDIA的 AMP 优化器中将全局梯度计算进行 bucketization 以重叠计算通信。通过在最后一个分区进行 all-reduce，以减少内存占用，实现计算通信重叠</p><h4 id="Parameter-Partitioning"><a href="#Parameter-Partitioning" class="headerlink" title="Parameter Partitioning"></a>Parameter Partitioning</h4><p>参数分区是在 DP 训练中减少内存消耗的一种方式。每个进程只存储与其分区相对应的参数，当需要使用到其他分区的参数进行前向和反向传播时，通过 broadcast 从相应的 DP 进程接收这些参数</p><h4 id="Implication-on-Model-Size"><a href="#Implication-on-Model-Size" class="headerlink" title="Implication on Model Size"></a>Implication on Model Size</h4><p>在 1024 DP 的情况下，搭配 Pos+g+p，可以实现 1.5 万亿参数的训练，如果只使用 DP 训练，仅能训练 1.5 Billion 参数量</p><h3 id="ZeRO-R"><a href="#ZeRO-R" class="headerlink" title="ZeRO-R"></a>ZeRO-R</h3><p>将使用的内存分为两类：</p><ul><li>长期存在：前向传播时的激活值 checkpoint，反向传播时的参数梯度</li><li>短期存在：前向传播时的 recompute，反向传播时的激活值梯度</li></ul><p>ZeRO-R 通过将激活值 checkpoint 和 梯度 移动到预先分配的连续 buffer 中，进行实时内存碎片整理，还减少了查找空闲连续内存的时间</p><h4 id="Partitioned-Activation-Checkpointing"><a href="#Partitioned-Activation-Checkpointing" class="headerlink" title="Partitioned Activation Checkpointing"></a>Partitioned Activation Checkpointing</h4><p>通过激活值 checkpoint 分区，消除了 MP 中的内存冗余，只有在计算中需要使用激活时，才会将激活值复制</p><p>在模型中一层完成前向完成时，会将激活值分区到所有并行进程上，如果在反向传播过程中被使用时，则使用 all-gather 重新创建激活值的 copy</p><h4 id="Constant-Size-Buffers"><a href="#Constant-Size-Buffers" class="headerlink" title="Constant Size Buffers"></a>Constant Size Buffers</h4><p>通过保持足够大的常量 buffer ，在计算之前将所有参数融合到这个单独的 buffer 中，可以加速内存读写效率</p><h4 id="Memory-Defragmentation"><a href="#Memory-Defragmentation" class="headerlink" title="Memory Defragmentation"></a>Memory Defragmentation</h4><p>前向传播过程中通过激活值 checkpoint 只保留了部分激活值，其余的激活值会被丢弃，因为它们在后向传播时可以重新计算。同样，在后向传播过程中，参数梯度是长期存在的，而激活梯度和其他用于计算参数梯度的缓冲区是短期存在的。这种长期和短期内存的交织导致了内存碎片化的问题</p><p>文中提出了 In-place Activation Reuse 的方法，在反向传播过程中可以重复使用激活值的内存，而无需每次都重新分配内存。将不再需要的激活值内存标记为可重用，并在下一次需要相同大小内存的地方重用它们。减少了内存分配和释放的次数，减少了碎片化</p><h2 id="ZeRO-DP-通信分析"><a href="#ZeRO-DP-通信分析" class="headerlink" title="ZeRO-DP 通信分析"></a>ZeRO-DP 通信分析</h2><h3 id="DP-通信量"><a href="#DP-通信量" class="headerlink" title="DP 通信量"></a>DP 通信量</h3><p>在每次反向后执行 all-reduce 平均，这种 all-reduce 会在大模型上会完全依赖通信带宽，现在的 all-reduce 通过流水线执行 reduce-scatter 再 all-gather 达到 all-reduce 的效果，会有两倍的数据通信量</p><h3 id="ZeRO-DP-通信量"><a href="#ZeRO-DP-通信量" class="headerlink" title="ZeRO-DP 通信量"></a>ZeRO-DP 通信量</h3><h4 id="Pos-g"><a href="#Pos-g" class="headerlink" title="Pos+g"></a>Pos+g</h4><p>因为每个进程只保存其分区的梯度，对梯度进程进行 scatter-reduce，再执行 all-gather，通信量与 DP 相同，为 2Ψ</p><h4 id="Pos-g-p"><a href="#Pos-g-p" class="headerlink" title="Pos+g+p"></a>Pos+g+p</h4><p>通过参数分区，每个进程只存储更新的参数。在计算前向传播结果前，每个分区的进程将权重 braodcast 到所有进程，在前向传播时，通过流水线执行 all-gather 接受其他分区的参数，以减少内存占用，前向传播后，丢弃权重。在反向传播时需要再次 all-gather，因此，总通信量为 (Ψ * N) / N = Ψ</p><p>综上，总通信量为 3Ψ，为 DP 的 1.5 倍</p><h2 id="ZeRO-R-通信分析"><a href="#ZeRO-R-通信分析" class="headerlink" title="ZeRO-R  通信分析"></a>ZeRO-R  通信分析</h2><p>下面通过分析分区激活值 checkpoint（Pa）与MP，DP通信量决定使用Pa还是Pa+cpu</p><p>Pa 的通信量权衡取决于模型大小、checkpoint 策略和 MP 策略。论文使用 Megatron-LM 实现的模型背景下进行分析。 在带有激活值 checkpoint 的 Megatron-LM 中，每个transformer在正向传播中执行两个大小为 batch × seq_length × hidden_dim 的 all-reduce 操作用于正向传播时的重计算，另外两个 all-reduce 操作用于反向传播。每个块的总通信量为 12 × seq length × hidden dim，因为 all-reduce 的通信量为 2 × message_size。 当 ZeRO-R 对激活值 checkpoint 进行分区时，在每个激活值 checkpoint 上的反向传播的正向重新计算之前需要额外的 all-gather 操作。会检查每个transformer块的输入激活，需要一个 all-gather，因此，Pa 的通信量为 seq_length ∗ hidden_dim。因为 all-gather 的通信量为 message_size，计算 Pa 的总通信量小于 MP 原始通信量的 10%。当MP与DP结合使用时，Pa可用于将Pa通信量减少一个数量级，而模型并行通信量增加10%，并在 DP 通信成为性能瓶颈时显著提高效率。另外，Pa将激活内存消耗降低了 MP 并行度，从而允许按比例增加 batch_size。由于 Pa 导致 batch_size 增加一个数量级可能导致 DP 通信量减少一个数量级。 如果采用 Pa+cpu，分区激活值 checkpoint 将卸载到cpu，就不再需要激活内存了，与Pa相比，cpu内存之间增加了2倍的数据移动。在极端情况下，DP 通信量是主要瓶颈，因为即使使用Pa，batch_size 也很小，在小batch_size的情况下，只要 cpu 数据传输开销小于DP通信量开销，Pa+cpu 就可以通过增加 batch_size 来提高效率。在给定模型和硬件特性的情况下，可以利用上述分析来决定是否以及何时使用Pa还是Pa+cpu</p><h2 id="万亿参数"><a href="#万亿参数" class="headerlink" title="万亿参数"></a>万亿参数</h2><p>仅使用 DP，ZeRO能够在1024个GPU上容纳超过1万亿参数的模型。此外，如下表所示，当与 MP 结合使用时，每个DGX2节点内使用16路MP，跨节点使用64路DP，ZeRO能够在1024个GPU上运行超过1万亿参数的模型，但训练时长会超过一年，期待未来算力提升</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/3.webp" width="800" /></p><h2 id="实验评估"><a href="#实验评估" class="headerlink" title="实验评估"></a>实验评估</h2><p>实施：基于 PyTorch 的 ZeRO-100B，包括Pos+g 和 ZeRO-R 中的全部优化点</p><p>硬件：由 400 个 V100 GPU（25个DGX-2节点）组成的集群，节点间通信带宽为800 Gbps</p><p>Baseline：没有MP的实验使用了 torch 的 DDP，MP 的实验使用 Megatron-LM 的 MP</p><p>ZeRO：没有MP的实验使用了 ZeRO-100B 中基于 ZeRO 的 DP 实现。MP 的实验将 ZeRO-100B 中的 ZeRO-powered DP 与 Megatron-LM 的 MP 相结合</p><p>模型：基于GPT-2的 transformer 模型，下表是参数配置</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/5.webp" width="800" /></p><h3 id="Speed-and-Model-size"><a href="#Speed-and-Model-size" class="headerlink" title="Speed and Model size"></a>Speed and Model size</h3><p>Baseline 的 Megatron MP 在模型规模增大时性能会快速下降，因为 MP 在 GPU 之间产生了高额通信量，而在超过单个节点以适应更大的模型时，每条链路（NVSwitch）的通信带宽从300GB/秒下降为12.5GB/秒，导致性能严重下降。对比之下，ZeRO-100B 会有 10 倍的训练速度提升</p><h3 id="Super-Linear-Scalability"><a href="#Super-Linear-Scalability" class="headerlink" title="Super-Linear Scalability"></a>Super-Linear Scalability</h3><p>如下图所示，使用ZeRO-100B可以实现超线性可扩展性，并通过增加 DP 并行度来提高每个GPU的吞吐量，从而适应更大的 batch_size</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/6.webp" width="800" /></p><h3 id="Democratizing-Large-Model-Training"><a href="#Democratizing-Large-Model-Training" class="headerlink" title="Democratizing Large Model Training"></a>Democratizing Large Model Training</h3><p>下图使用128个GPU，ZeRO-100B 可以训练13B参数的模型，平均每个GPU吞吐量超过40 TFlops。相比之下，没有使用ZeRO，仅使用DP的最大可训练模型仅有 1.4B 参数，每个 GPU 的吞吐量不到 20 TFlops。此外，由于没有 MP 带来的通信开销，这些模型可以在具有较低端计算节点上进行训练，无需 NVLINK 或 NVSwitch 这种高速互联方式</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/7.webp" width="500" /></p><h3 id="Memory-and-Performance-Analysis"><a href="#Memory-and-Performance-Analysis" class="headerlink" title="Memory and Performance Analysis"></a>Memory and Performance Analysis</h3><p>作者讨论了不同优化方法对最大模型大小、内存消耗和性能的影响。作者将这些优化方法分为配置1到5（C1-C5），如下表所示，通过使用固定 batch_size 和 MP 为16，观察启用不同ZeRO优化的可训练模型的最大尺寸</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/8.webp" width="400" /></p><p>在最大模型尺寸方面，如 Figure 6 所示，通过使用 Pa 优化，模型大小从40B增加到了60B。而通过使用Pos+g优化，在C2的基础上，模型大小增加到了140B，这是因为与C2相比，该优化使模型状态的内存需求减半。使用C5进一步减少了激活内存，将分区激活值 checkpoint 转移到CPU内存，使模型大小增加到150B</p><p>对于每个训练迭代中 PyTorch 缓存的最大内存，如 Figure 7 所示，作者观察了40B和100B模型的情况。从C1到C2，缓存的内存大小如期减少。C2到C3的内存消耗差异取决于模型状态与激活内存的大小关系，当激活内存较大时，差异可能增加，当模型状态较大时，差异可能减小。值得注意的是，在40B模型中，从C4到C5时，缓存的内存大小没有减少，但在100B模型中有减少。这是因为100B的激活内存较大，减少不明显。作者指出，当我们处理非常大的模型时，Pa+cpu优化可用于适应更大的 batch_size</p><p>对于不同优化设置的最佳性能，如 Figure 8 所示，性能提升与内存消耗的减少相对应。较低的内存消耗可以实现更大的 batch_size，从而提高性能。唯一的例外是60B参数模型在C4和C5之间的性能下降。尽管内存消耗较低，但C5会导致激活在CPU之间移动，这通常会导致性能下降，除非模型非常大以至于无法在没有C5的情况下运行，或者可以在没有C5的情况下运行的 batch_size 很小（例如在 Figure 8 中具有170B参数的模型）。在训练过程中，Pa+cpu优化只在有益时才启用</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/9.webp" width="800" /></p><h3 id="Turing-NLG-the-SOTA-language-model-with-17B-parameters"><a href="#Turing-NLG-the-SOTA-language-model-with-17B-parameters" class="headerlink" title="Turing-NLG, the SOTA language model with 17B parameters"></a>Turing-NLG, the SOTA language model with 17B parameters</h3><p>下图展示了在300K次迭代中与之前最先进的Megatron-LM 8.3B参数模型的验证 Perplexity 对比，使用 ZeRO 训练得到的模型指标优于 Megatron-LM，此外 ZeRO100B 还实现了持续的 41.4 TFlops/GPU的吞吐量</p><p><img src="/image/论文阅读-ZeRO-Memory-Optimizations-Toward-Training-Trillion-Parameter-Models/10.webp" width="400" /></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>ZeRO 牛逼</p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: PyTorch Distributed: Experiences on Accelerating Data Parallel Training</title>
      <link href="/2023/09/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/"/>
      <url>/2023/09/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文提出的 DistributedDataParallel 在优化器运行之前进行梯度平均，用相同的梯度集更新所有模型副本，这样在数学上和本地训练完全等价，而且可以实现异步，比参数平均更加高效。</p><span id="more"></span><p>paper: <a href="https://arxiv.org/abs/2006.15704">https://arxiv.org/abs/2006.15704</a></p><p>code: <a href="https://github.com/pytorch/pytorch/">https://github.com/pytorch/pytorch/</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><h3 id="PyTorch-训练流程"><a href="#PyTorch-训练流程" class="headerlink" title="PyTorch 训练流程"></a>PyTorch 训练流程</h3><ul><li>Forward pass：计算损失 </li><li>Backward pass：计算梯度 </li><li>Optimizer step：更新参数</li><li>数据并行 </li></ul><p>PyTorch 有以下方式进行分布式训练，如 </p><ul><li>DataParallel：单机多卡进行单进程多线程数据并行训练 </li><li>DistributedDataParallel：多机多卡进行多进程数据并行训练 </li><li>RPC（e.g. 参数服务器）：分布式模型并行训练 </li></ul><p>另一种方案：参数平均计算所有模型权重的均值，但是当优化器中有依赖过去局部梯度的值（如 Adam 中的动量），优化器的状态可能会逐渐偏离，最终导致训练效果下降。另外，向后传递和参数平均不能重叠运行，浪费了性能</p><p>本文提出的 DistributedDataParallel 在优化器运行之前进行梯度平均，用相同的梯度集更新所有模型副本，这样在数学上和本地训练完全等价，而且可以实现异步，比参数平均更加高效 </p><h2 id="AllReduce"><a href="#AllReduce" class="headerlink" title="AllReduce"></a>AllReduce</h2><p>次级实现</p><p>每个进程将其输入张量广播给所有对等进程，然后独立地应用算术运算，然而效率不高 </p><p>于是 Nvidia 在 NCCL 后端中提出了基于环的 AllReduce 和基于树的 AllReduce </p><p>AllReduce 操作需要等待所有进程就绪后才能运行，是同步操作，而参数服务器中使用 P2P 通信 </p><h2 id="系统设计"><a href="#系统设计" class="headerlink" title="系统设计"></a>系统设计</h2><p>API 实现</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/1.webp" width="300" /></p><p>分布式训练时只需要修改少部分训练脚本</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/3.webp" width="500" /></p><p>暴露更多接口以实现拦截信号和触发操作的机制进行优化 </p><h2 id="梯度规约"><a href="#梯度规约" class="headerlink" title="梯度规约"></a>梯度规约</h2><h3 id="次级实现（All-Reduce）"><a href="#次级实现（All-Reduce）" class="headerlink" title="次级实现（All Reduce）"></a>次级实现（All Reduce）</h3><p>DDP控制所有的训练进程</p><p>在模型初始化时广播网络状态，让所有模型以相同状态开始训练<br>每次迭代时在局部 backward() 之后和优化器 step() 之前执行梯度，以相同的梯度更新权重 </p><p>DDP 可以注册 autograd hooks，每次 backward() 后触发以下操作</p><ol><li>hooks 扫描所有局部模型参数</li><li>从每个参数中检索梯度张量</li><li>再使用 AllReduce 集体通信计算所有进程中参数的平均梯度</li><li>将结果返回梯度张量</li></ol><p>但是有两个性能问题</p><ul><li>在小 tensor 上的集合通信效率非常低</li><li>把梯度计算和同步分开之后，不能让它们通信重叠 </li></ul><h3 id="梯度桶"><a href="#梯度桶" class="headerlink" title="梯度桶"></a>梯度桶</h3><p>下图显示了不同的参数规模在执行 All Reduce 时的效率，可以看到 tensor 越大，效率越高</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/2.webp" width="500" /></p><p>梯度桶为了解决 All Reduce 在小 tensor 上的性能问题，将多个小 tensor 收集为一个桶，在达到一定规模后进行 All Reduce，这将 All Reduce 的操作转变为了异步</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/4.webp" width="500" /></p><h3 id="通信和计算重叠"><a href="#通信和计算重叠" class="headerlink" title="通信和计算重叠"></a>通信和计算重叠</h3><p>在没有使用梯度桶机制之前，通信和梯度计算可以重叠。 </p><p>在梯度桶机制下，需要等在同一个桶内的所有梯度都计算完成后，才能进行通信。为了重叠，PyTorch 引入 hook 机制，在反向传播计算完成后，调用自定义函数</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/5.webp" width="500" /></p><p>每当一个梯度计算完成后，对应的 hook 将会被触发，当在同一个bucket中的梯度的hook都被fire后，就调用AllReduce对该bucket进行通信 </p><p>但是这种策略会导致两个问题</p><ul><li><p>每个进程都是独立的，不能保证所有进程处理桶的顺序一致，如下图 (a) 所示 </p><p>解决方法：将模型参数的反序作为桶的顺序，反向顺序可以近似表示反向传递中的梯度计算顺序</p></li><li><p>在某些阶段，网络中的一些层的梯度可以不需要使用，如 Dropout 等，这样这个层对应梯度的 hook 永远不会被触发，如下图 (b) 所示</p><p>解决方法：在前向传播结束后从输出开始遍历计算图，使用 bitmap（用于表示二进制数据的数据结构） 记录哪些参数参与计算，哪些参数没有参与计算，对于没有参与计算的参数，标记为 ready </p></li></ul><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/6.webp" width="500" /></p><h3 id="梯度累积"><a href="#梯度累积" class="headerlink" title="梯度累积"></a>梯度累积</h3><p>传统梯度传播频率是在每次训练完成后传播，PyTorch 通过在在全局同步梯度之前进行 n 次局部训练迭代，减少梯度通信的时间，这种思路还可以解决大 batch size 占用资源过大的问题，可以将一个batch切分为多个micro<br>batch，在最后一个micro batch训练完成后，进行梯度更新</p><p>但是这种策略会导致某些迭代中没有参与计算的梯度（如 Dropout）和正常计算的梯度混合后，会导致有部分梯度在某些迭代中被累加，而在其他迭代中被清零，PyTorch也无法判断哪些梯度计算完成后立即进行同步，还是等<br>累加若干个迭代之后再进行同步</p><p>解决方法：提出了 “no_sync”上下文。当进行梯度累积并使用 “no_sync”上下文时，会有一些参数在某些迭代中未被使用，但在其他迭代中被使用。为了确保这些未使用参数的梯度不会在下一次迭代中被误用，使用 “bitmap”<br>来记录这些未使用参数的信息。在进入”no_sync”上下文之后，所有的 DDP hook 都被禁用，这意味着参数的梯度将被累积而不会在该上下文中进行通信。同时，全局未使用参数的信息也会被记录在 “bitmap” 中。这样，在下<br>一次通信时，PyTorch会使用 “bitmap” 来指导梯度通信，以确保未使用的参数不会被传输，从而保持梯度的正确性。</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/11.webp" width="500" /></p><h2 id="集合通信"><a href="#集合通信" class="headerlink" title="集合通信"></a>集合通信</h2><p>PyTorch DDP 支持三种通讯库：NCCL，Gloo 和 MPI。这些库被包装到同一个 ProcessGroup API 中，在运行多个 ProcessGroup 时，会使用轮询调度将集体通信分派给各个 ProcessGroup 实例，获得更高的带宽利用率 </p><h2 id="工程实现"><a href="#工程实现" class="headerlink" title="工程实现"></a>工程实现</h2><h3 id="python-前后端"><a href="#python-前后端" class="headerlink" title="python 前后端"></a>python 前后端</h3><p>可配置参数</p><ul><li>Process Group：指定进程组运行 AllReduce</li><li>bucket_cap_mb：调整桶大小优化训练速度 </li><li><p>find_unused_parameters：控制 DDP 是否应该通过遍历计算图来检测未使用的参数 </p></li><li><p>Model Buffers：在某些层中，例如BatchNorm层，需要维护一些状态，比如running variance（运行方差）和running mean（运行均值）。这些状态需要在训练过程中持续更新和使用，以确保模型在训练过程中的稳定性和性能。为了正确处理Model Buffers的同步和广播。DDP通过指定 rank 0 进程来处理。在启用no sync模式时，相应地调整缓冲区的广播，确保所有进程在进行本地计算之前，都拥有最新的Model Buffers的值 </p></li></ul><h3 id="梯度规约的重点实现"><a href="#梯度规约的重点实现" class="headerlink" title="梯度规约的重点实现"></a>梯度规约的重点实现</h3><p>参数与桶的映射：确保同一 bucket 中的 parameter 都来自同一个device</p><p>Autograd Hook：通过为每个桶添加值为梯度数量的递减计数器来判断当前 backward 到了第几层，从而在合适的时候 AllReduce，在下一次前向传播时，重置计数器</p><p>桶规约：默认 bucket size 为 25M，实践中需要实验得到最佳大小 </p><p>全局未使用参数：在 CPU 上创建 bitmap 来保存本地没有使用的参数信息，并通过一个额外的allreduce得到 global bitmap</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/7.webp" width="300" /></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>通过 Latency Breakdown（对训练过程中不同阶段的延迟进行细分和分析）比较了不同模型、使用不同 backend、有无通信和训练的 overlap，得到反向传播是最耗时是阶段，AllReduce 就是在这一阶段中，仍然需要优化通信效率 </p><p>下图实验了不同 bucket size</p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/9.webp" width="800" /></p><p>DDP通过使用多个round-robin（轮询调度）进程组从而充分利用带宽。下图实验比较了使用不同数量进程组对 latency 的影响 </p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/10.webp" width="800" /></p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><ul><li><p>通信后端：NCCL 优于 GLOO </p><p><img src="/image/论文阅读-PyTorch-Distributed-Experiences-on-Accelerating-Data-Parallel-Training/8.webp" width="300" /></p></li><li><p>bucket size：大小随着模型的增大而增大 </p></li><li><p>资源分配：用 NCCL 时建议把同一台机器上的所有进程都放到同一个进程组中 </p></li></ul><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul><li>梯度顺序预测：使用 autograd hook 记录 backward 的顺序，并相应地更新 bucket mapping 中的对应参数</li><li>Layer dropping：在forward的过程中随机 drop 掉几层网络，加速训练的同时避免过拟合，与此同时相应修改 parameter-to-bucket mapping，或从 bucket 层面 drop 网络层</li><li>梯度压缩：只通信需要高精度的梯度</li></ul>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: Horovod: fast and easy distributed deep learning in TensorFlow</title>
      <link href="/2023/09/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/"/>
      <url>/2023/09/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近忙实习去了，断更大半年。整理了一下论文，有十几篇没读，后面有时间慢慢更。</p><p>这篇论文主要工作是对 TensorFlow 框架 API 的重写，使用 ring-allreduce 和 broadcast 方法，进行数据并行。</p><span id="more"></span><p>paper: <a href="https://arxiv.org/abs/1802.05799">https://arxiv.org/abs/1802.05799</a></p><p>code: <a href="https://github.com/uber/horovod">https://github.com/uber/horovod</a></p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>训练时的 GPU 通信耗时过大 </p><p>当时分布式训练加速需要修改大量代码，成本过高 </p><h3 id="TensorFlow-不足"><a href="#TensorFlow-不足" class="headerlink" title="TensorFlow 不足"></a>TensorFlow 不足</h3><p>TensorFlow 的分布式引入了许多复杂的概念和 API，使用成本变高，很难发现修复隐藏的 bug<br>使用 128 张 GPU 进行大规模训练时，通信与计算占比相当，可扩展性差 </p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/1.webp" width="800" /></p><p>Facebook 首次提出数据并行，在多个节点上并行拆分数据来训练。不同批次数据的梯度在每个节点上单独计算，之后收集梯度求平均后广播到每个节点上，以保证每个节点中的模型副本参数一致 </p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/2.webp" width="800" /></p><h3 id="Leveraging-a-different-type-of-algorithm"><a href="#Leveraging-a-different-type-of-algorithm" class="headerlink" title="Leveraging a different type of algorithm"></a>Leveraging a different type of algorithm</h3><p>TensorFlow 通过指定一个进程为 worker（工作进程） 或 parameter servers（参数服务器）</p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/3.webp" width="800" /></p><p>worker 负责前反向传播，发送计算得到的梯度，接收平均后的梯度和数据</p><p>有两个难点：</p><ul><li>worker 和 parameter servers 比例难以确定，太少的 parameter servers 会导致计算饱和，太多的 parameter servers 会导致通信饱和 </li><li>学习曲线陡峭，大量代码重构 </li></ul><p>ring allreduce 的提出解决了这个问题 </p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/4.webp" width="800" /> </p><p>N个节点中的每个节点与其两个对等节点进行通信，共进行了2 * (N - 1)次通信，节点发送和接收数据缓冲区的块。在第一次（N - 1）次迭代时，接收到的值会被添加到节点缓冲区中的值中。在第二次（N - 1）次迭代时，接<br>收到的值将替换节点缓冲区中的值 </p><h2 id="Horovod"><a href="#Horovod" class="headerlink" title="Horovod"></a>Horovod</h2><p>做了如下工作 </p><ul><li>将百度的 TensorFlow ring-allreduce 算法的实现转化为一个独立的 Python 包，命名为 Horovod </li><li>使用 NCCL 库实现了 TensorFlow ring-allreduce，并优化了性能 </li><li>添加了对单机多卡的支持 </li><li>改进了 API，添加 broadcast 操作，仅需 4 步即可使用 Horovod </li></ul><h3 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h3><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/5.webp" width="800" /></p><p>下面代码使用 mpirun 命令在 4 张卡的 4 个 server 上运行 train.py，支持 TensorFlow 和 Keras </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mpirun -np 16 -H server1:4,server2:4,server3:4,server4:4 python train.py </span><br></pre></td></tr></table></figure><h3 id="Horovod-Timeline"><a href="#Horovod-Timeline" class="headerlink" title="Horovod Timeline"></a>Horovod Timeline</h3><p>TensorFlow 和 CUDA Profiler 不足：只能显示单个 server 的 Profiler，需要用户手动交叉比对 </p><p>改进：如下图所示，设置环境变量，就可以查看训练时每个节点在每个时间步骤中的具体操作，并且兼容 chrome://tracing </p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/6.webp" width="800" /></p><h3 id="Tensor-Fusion"><a href="#Tensor-Fusion" class="headerlink" title="Tensor Fusion"></a>Tensor Fusion</h3><p>观察 Profiler 时发现不足：大 Tensor 可以很好地利用带宽，但是小 Tensor 的 ring-allreduce 会影响效率 </p><p>改进：</p><ul><li>整理要 allreduce 的 Tensor，分配到匹配的同数据类型的 buffer，如果没能成功分配，就创建一个 fusion buffer，默认为 64 MB； </li><li>将 Tensor 拷贝到 buffer，对 fusion buffer 执行 broadcast，再将 Tensor 拷贝出来。重复上述步骤；</li></ul><p>对比得到 65% 的性能提升 </p><h2 id="Horovod-Benchmarks"><a href="#Horovod-Benchmarks" class="headerlink" title="Horovod Benchmarks"></a>Horovod Benchmarks</h2><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/7.webp" width="800" /> </p><p>上图所示，使用 Inception V3和ResNet-101模型进行实验，Horovod 相比 TensorFlow 性能提升了 88% </p><p>使用 RDMA 与 TCP 进行基准测试。在 Inception V3和 ResNet-101 模型上，RDMA 并没有显著提高性能，只比 TCP 网络多了 3% 到 4% 的增长</p><p><img src="/image/论文阅读-Horovod-fast-and-easy-distributed-deep-learning-in-TensorFlow/8.webp" width="800" /></p><p>但是在 VGG-16 模型上，使用RDMA网络性能提高了30%。可以解释为 VGG-16 模型具有大量的参数，结合其较少的层数，使得通信成为关键路径，使得网络成为瓶颈</p><h2 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h2><ul><li>使 MPI 安装使用更方便 </li><li>着重研究如何调整模型的超参以提高准确率 </li><li>开发更多大模型训练示例</li></ul>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布式训练框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023MCM-ICM美赛C题第二问思路</title>
      <link href="/2023/02/23/2023MCM-ICM%E7%BE%8E%E8%B5%9BC%E9%A2%98%E7%AC%AC%E4%BA%8C%E9%A2%98%E6%80%9D%E8%B7%AF/"/>
      <url>/2023/02/23/2023MCM-ICM%E7%BE%8E%E8%B5%9BC%E9%A2%98%E7%AC%AC%E4%BA%8C%E9%A2%98%E6%80%9D%E8%B7%AF/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在睡了十一个小时后，蹦出的思路，目前网上没有看到相同的方法（不知道比赛刚结束就发思路会不会出事），第一题和第三题大家大同小异，就不讲解了。</p><span id="more"></span><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>对于给定的未来解决方案单词，在未来的日期，开发一个模型，使您能够预测报告结果的分布。换句话说，预测未来日期（1，2，3，4，5，6，X）的相关百分比。你的模型和预测有哪些不确定性？举一个具体的例子，说明你对2023年3月1日<code>EERIE</code>一词的预测。你对模型的预测有多自信？</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>首先是单词特征，笔者在这里选择了 6 个特征</p><ul><li>词频（</li><li>词性（名词，动词 ……）</li><li>音节数</li><li>词的情感分类（贬义，中性，褒义）</li><li>重复字母数</li><li>当天是否是假期</li></ul><p>要预测的 7 个百分比是很明显呈正态分布，可以使用正态分布曲线拟合，可视化如下</p><p><img src="/image/2023MCM-ICM美赛C题第二题思路/std.png" alt=""></p><p>这样就可以将 7 个要预测的特征转化为 2 个特征（std，mean）</p><p>再搭建神经网络预测即可，由于数据量较少，笔者使用多层 Dropout 的方法，p 相继下降来防止过拟合，提高泛化。网络结构如下所示，超参在文末代码中</p><p><img src="/image/2023MCM-ICM美赛C题第二题思路/net.webp" alt=""></p><p>最后预测的<code>EERIE</code>百分比如下图所示</p><p><img src="/image/2023MCM-ICM美赛C题第二题思路/eerie.webp" alt=""></p><p>代码我放在了 github 仓库</p><p><a href="https://github.com/aeeeeeep/2023MCM-C-Task2">https://github.com/aeeeeeep/2023MCM-C-Task2</a></p><p>欢迎给个 star ⭐⭐⭐～</p>]]></content>
      
      
      <categories>
          
          <category> 数学建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HGame_2023_Week3_3ctu4_card_problem</title>
      <link href="/2023/01/31/HGame-2023-Week3-3ctu4-card-problem/"/>
      <url>/2023/01/31/HGame-2023-Week3-3ctu4-card-problem/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>干扰特征非常多的二分类问题，解题步骤如下</p><span id="more"></span><ul><li>首先使用高斯模糊对原图片降噪，kernel 为 (9,9) ，再二值化图片，阈值为 220，使用形态学腐蚀来填充孔洞，kernel 为 (5,5)，得到掩码图像</li><li>获取图像中最大轮廓的旋转包围盒（中心点，宽高，旋转角度）以及顶点，生成透视变换矩阵，对掩码图像对应的部分进行透视变换，得到卡片图像，得到的卡片不分上下方向</li><li>经过分析得 YGO 类别的卡片边缘特征比较统一，遮盖中心无用特征，将卡片图像 [50:355,22:280] 的区域设为白色，留下边缘特征，从遮盖后的图像中找向上和向下方向各两张图片作为模板图片保存到当前目录下 <code>temp1.webp</code>和 <code>temp2.webp</code></li><li>每张图对两个模板分别使用 ORB 算法进行相似度检测，如果与两个模板相似度之和 &gt; 0.04，就判定为 YGO，否则为 PTCG</li></ul><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> pwn <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Img_Outline</span>(<span class="params">original_img</span>):</span><br><span class="line">    gray_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">    blurred = cv2.GaussianBlur(gray_img, (<span class="number">9</span>, <span class="number">9</span>), <span class="number">0</span>)                    </span><br><span class="line">    _, RedThresh = cv2.threshold(blurred, <span class="number">220</span>, <span class="number">255</span>, cv2.THRESH_BINARY)  </span><br><span class="line">    h, w = original_img.shape[:<span class="number">2</span>]</span><br><span class="line">    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="number">5</span>,<span class="number">5</span>))         </span><br><span class="line">    marker = np.zeros_like(gray_img)</span><br><span class="line">    marker[<span class="number">0</span>, :] = <span class="number">255</span></span><br><span class="line">    marker[-<span class="number">1</span>, :] = <span class="number">255</span></span><br><span class="line">    marker[:, <span class="number">0</span>] = <span class="number">255</span></span><br><span class="line">    marker[:, -<span class="number">1</span>] = <span class="number">255</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        marker_pre = marker</span><br><span class="line">        dilation = cv2.dilate(marker, kernel=kernel)</span><br><span class="line">        marker = np.<span class="built_in">min</span>((dilation, RedThresh), axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> (marker_pre == marker).<span class="built_in">all</span>():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    marker = cv2.morphologyEx(marker, cv2.MORPH_OPEN, kernel)</span><br><span class="line">    marker[<span class="number">0</span>:<span class="number">5</span>,:] = <span class="number">255</span></span><br><span class="line">    marker[-<span class="number">5</span>:,:] = <span class="number">255</span></span><br><span class="line">    marker[:,<span class="number">0</span>:<span class="number">5</span>] = <span class="number">255</span></span><br><span class="line">    marker[:,-<span class="number">5</span>:] = <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> original_img, gray_img, marker</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findContours_img</span>(<span class="params">original_img, marker</span>):</span><br><span class="line">    contours, hierarchy = cv2.findContours(marker, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">    c = <span class="built_in">sorted</span>(contours, key=cv2.contourArea, reverse=<span class="literal">True</span>)[<span class="number">1</span>]  </span><br><span class="line">    rect = cv2.minAreaRect(c)                                  </span><br><span class="line">    box = np.int0(cv2.boxPoints(rect))                          </span><br><span class="line">    draw_img = cv2.drawContours(original_img.copy(), [box], -<span class="number">1</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> box,draw_img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Perspective_transform</span>(<span class="params">box,original_img</span>):</span><br><span class="line">    orignal_W = math.ceil(np.sqrt((box[<span class="number">3</span>][<span class="number">1</span>] - box[<span class="number">2</span>][<span class="number">1</span>])**<span class="number">2</span> + (box[<span class="number">3</span>][<span class="number">0</span>] - box[<span class="number">2</span>][<span class="number">0</span>])**<span class="number">2</span>))</span><br><span class="line">    orignal_H= math.ceil(np.sqrt((box[<span class="number">3</span>][<span class="number">1</span>] - box[<span class="number">0</span>][<span class="number">1</span>])**<span class="number">2</span> + (box[<span class="number">3</span>][<span class="number">0</span>] - box[<span class="number">0</span>][<span class="number">0</span>])**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    pts1 = np.float32([box[<span class="number">0</span>], box[<span class="number">1</span>], box[<span class="number">2</span>], box[<span class="number">3</span>]])</span><br><span class="line">    pts2 = np.float32([[<span class="built_in">int</span>(orignal_W+<span class="number">1</span>),<span class="built_in">int</span>(orignal_H+<span class="number">1</span>)], [<span class="number">0</span>, <span class="built_in">int</span>(orignal_H+<span class="number">1</span>)], [<span class="number">0</span>, <span class="number">0</span>], [<span class="built_in">int</span>(orignal_W+<span class="number">1</span>), <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">    M = cv2.getPerspectiveTransform(pts1, pts2)</span><br><span class="line">    result_img = cv2.warpPerspective(original_img, M, (<span class="built_in">int</span>(orignal_W+<span class="number">3</span>),<span class="built_in">int</span>(orignal_H+<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">if</span> orignal_H &lt; orignal_W:</span><br><span class="line">        result_img = cv2.flip(cv2.transpose(result_img), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result_img</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img_similarity</span>(<span class="params">img1,img2</span>):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line"></span><br><span class="line">        orb = cv2.ORB_create()</span><br><span class="line">        kp1, des1 = orb.detectAndCompute(img1, <span class="literal">None</span>)</span><br><span class="line">        kp2, des2 = orb.detectAndCompute(img2, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        bf = cv2.BFMatcher(cv2.NORM_HAMMING)</span><br><span class="line"></span><br><span class="line">        matches = bf.knnMatch(des1, trainDescriptors=des2, k=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        good = [m <span class="keyword">for</span> (m, n) <span class="keyword">in</span> matches <span class="keyword">if</span> m.distance &lt; <span class="number">0.75</span> * n.distance]</span><br><span class="line">        similary = <span class="built_in">len</span>(good) / <span class="built_in">len</span>(matches)</span><br><span class="line">        <span class="keyword">return</span> similary</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;无法计算两张图片相似度&#x27;</span>)</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;./result&#x27;</span>):</span><br><span class="line">        os.makedirs(<span class="string">&#x27;result&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    final = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    temp1 = cv2.imread(<span class="string">&#x27;./temp1.webp&#x27;</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">    temp2 = cv2.imread(<span class="string">&#x27;./temp2.webp&#x27;</span>, cv2.IMREAD_GRAYSCALE)</span><br><span class="line"></span><br><span class="line">    r = remote(<span class="string">&#x27;week-3.hgame.lwsec.cn&#x27;</span>, <span class="number">30802</span>)</span><br><span class="line">    r.recvuntil(<span class="string">&#x27;...&#x27;</span>)</span><br><span class="line">    r.send(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    b64 = r.recvline()[:-<span class="number">1</span>]</span><br><span class="line">    out = <span class="built_in">open</span>(<span class="string">&#x27;row.zip&#x27;</span>, <span class="string">&quot;wb&quot;</span>)</span><br><span class="line">    base64.decode(io.BytesIO(b64), out)</span><br><span class="line">    out.close()</span><br><span class="line"></span><br><span class="line">    zipfile_path = <span class="string">&#x27;./row.zip&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(zipfile_path, mode=<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> zfile:</span><br><span class="line">        nWaitTime = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> tqdm(zfile.namelist()):</span><br><span class="line">            <span class="keyword">if</span> <span class="string">&#x27;.webp&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> name:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> zfile.<span class="built_in">open</span>(name,mode=<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> image_file:</span><br><span class="line">                content = image_file.read() </span><br><span class="line">                image = np.asarray(<span class="built_in">bytearray</span>(content), dtype=<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">                image = cv2.imdecode(image, cv2.IMREAD_ANYCOLOR)</span><br><span class="line"></span><br><span class="line">                original_img, gray_img, RedThresh = Img_Outline(image)</span><br><span class="line">                box, draw_img = findContours_img(original_img, RedThresh)</span><br><span class="line">                result_img = Perspective_transform(box,original_img)</span><br><span class="line">                result_img[<span class="number">50</span>:<span class="number">355</span>,<span class="number">22</span>:<span class="number">280</span>] = <span class="number">255.0</span></span><br><span class="line">                cv2.imwrite(<span class="string">f&quot;./result/<span class="subst">&#123;name&#125;</span>&quot;</span>, result_img)</span><br><span class="line">                result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">                similarity1 = img_similarity(temp1, result_img)</span><br><span class="line">                similarity2 = img_similarity(temp2, result_img)</span><br><span class="line">                <span class="keyword">if</span> (similarity1 + similarity2) &gt; <span class="number">0.04</span>:</span><br><span class="line">                    final += <span class="string">&#x27;1&#x27;</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    final += <span class="string">&#x27;0&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(final)</span><br><span class="line">    r.sendline(final)</span><br><span class="line">    r.interactive()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> misc </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>linux下中文路径命名规范化</title>
      <link href="/2023/01/29/linux%E4%B8%8B%E4%B8%AD%E6%96%87%E8%B7%AF%E5%BE%84%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%E5%8C%96/"/>
      <url>/2023/01/29/linux%E4%B8%8B%E4%B8%AD%E6%96%87%E8%B7%AF%E5%BE%84%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>shell 中无法正常访问带空格或某些字符的目录，写了个脚本规范一下中文路径命名</p><span id="more"></span><p>将指定目录下所有子目录和文件命名中的<code>、，。()！、？：“”‘’【】《》（）</code>全部替换为<code>_</code>，去除首尾的<code>_</code>。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_chinese_path</span>(<span class="params">directory</span>):</span><br><span class="line">    <span class="comment"># 遍历目录</span></span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(directory):</span><br><span class="line">        <span class="keyword">for</span> dirname <span class="keyword">in</span> dirs:</span><br><span class="line">            <span class="comment"># 利用正则表达式，将指定字符全部替换为_</span></span><br><span class="line">            new_dirname = re.sub(<span class="string">r&#x27;[\s、，。()！、？：“”‘’【】《》（）]+&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, dirname)</span><br><span class="line">            <span class="comment"># 去除首尾的_</span></span><br><span class="line">            new_dirname = new_dirname.strip(<span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">            <span class="comment"># 新目录名和旧目录名不同时才进行重命名</span></span><br><span class="line">            <span class="keyword">if</span> new_dirname != dirname:</span><br><span class="line">                old_path = os.path.join(root, dirname)</span><br><span class="line">                new_path = os.path.join(root, new_dirname)</span><br><span class="line">                os.rename(old_path, new_path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> filename <span class="keyword">in</span> files:</span><br><span class="line">            <span class="comment"># 利用正则表达式，将指定字符全部替换为_</span></span><br><span class="line">            new_filename = re.sub(<span class="string">r&#x27;[\s、，。()！？：“”‘’【】《》（）]+&#x27;</span>, <span class="string">&#x27;_&#x27;</span>, filename)</span><br><span class="line">            <span class="comment"># 去除首尾的_</span></span><br><span class="line">            new_filename, suffix = os.path.splitext(new_filename)</span><br><span class="line">            new_filename = new_filename.strip(<span class="string">&#x27;_&#x27;</span>)</span><br><span class="line">            <span class="comment"># 新文件名和旧文件名不同时才进行重命名</span></span><br><span class="line">            <span class="keyword">if</span> new_filename != filename:</span><br><span class="line">                old_path = os.path.join(root, filename)</span><br><span class="line">                new_path = os.path.join(root, new_filename + suffix)</span><br><span class="line">                os.rename(old_path, new_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">normalize_chinese_path(<span class="string">&#x27;./03 golang微服务实战/&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新年愿望</title>
      <link href="/2023/01/21/%E6%96%B0%E5%B9%B4%E6%84%BF%E6%9C%9B/"/>
      <url>/2023/01/21/%E6%96%B0%E5%B9%B4%E6%84%BF%E6%9C%9B/</url>
      
        <content type="html"><![CDATA[<p>写于壬寅年除夕夜</p><span id="more"></span><p>❎ 通过英语六级 (2023-03-12 听力没涂…)</p><p>✅ <del>通过某高校人工智能学院研究生初试</del> 去国内某GPU大厂实习</p><p>✅ go 微服务部署并行编程</p><p>✅ 购入 <del>高</del> 低性能工作站</p><p>  配置清单</p><ul><li>CPU：i5 10400</li><li>主板：华擎B460M-ITX/AC</li><li>显卡：微星 2060 万图师 12G</li><li>内存：金士顿 DDR4 16G 2666MHz</li><li>固态：硬盘 金士顿 SNV2S 500G</li><li>机械：硬盘 希捷 酷鱼SATA3 1T(7200转)</li><li>机箱：金河田N1 mini-itx</li><li>电源：航嘉wd500k</li><li>系统：Ubuntu 20.04</li></ul>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>caj文档批量转换为pdf-shell脚本多线程</title>
      <link href="/2023/01/10/caj%E6%96%87%E6%A1%A3%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BApdf-shell%E8%84%9A%E6%9C%AC%E5%A4%9A%E7%BA%BF%E7%A8%8B/"/>
      <url>/2023/01/10/caj%E6%96%87%E6%A1%A3%E6%89%B9%E9%87%8F%E8%BD%AC%E6%8D%A2%E4%B8%BApdf-shell%E8%84%9A%E6%9C%AC%E5%A4%9A%E7%BA%BF%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近看的论文有些多，但知网上的 caj 文档又卡又难做笔记，遂写了一个 caj 文档批量转换为 pdf 的 shell 脚本。</p><span id="more"></span><h2 id="caj2pdf"><a href="#caj2pdf" class="headerlink" title="caj2pdf"></a>caj2pdf</h2><p>安装 caj2pdf 命令行工具</p><p>github 源码: [</p><p>aur 用户</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yay -S caj2pdf-git</span><br></pre></td></tr></table></figure><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><p>当前目录下所有 caj 文件批量转换为pdf，并删除原 caj 文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">for file in ./*.caj; do</span><br><span class="line">    caj2pdf convert &quot;$file&quot; -o &quot;$&#123;file%.*&#125;.pdf&quot; &amp;&amp; rm &quot;$file&quot; &amp;</span><br><span class="line">done</span><br><span class="line">wait</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA编程性能分析工具 nvprof/ncu --metrics参数含义</title>
      <link href="/2023/01/07/CUDA%E7%BC%96%E7%A8%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7-metrics%E5%8F%82%E6%95%B0%E5%90%AB%E4%B9%89/"/>
      <url>/2023/01/07/CUDA%E7%BC%96%E7%A8%8B%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7-metrics%E5%8F%82%E6%95%B0%E5%90%AB%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在网上没有比较全的中文 ncu —metrics 参数含义，于是自己整理了一下官方和外国友人的笔记。</p><span id="more"></span><h2 id="nvprof-和-ncu"><a href="#nvprof-和-ncu" class="headerlink" title="nvprof 和 ncu"></a>nvprof 和 ncu</h2><p>nvprof 是过去比较常用的命令行工具，但在终端直接输入<code>nvprof ./*.o</code>会得到以下 Warning</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.</span><br><span class="line">                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.</span><br><span class="line">                  Refer  for more details.</span><br></pre></td></tr></table></figure><p>目前主流的 CUDA 驱动不再支持<code>nvprof</code>命令，但我们仍可以在 NVIDIA Nsight Systems 中使用，在终端输入 <code>nsys nvprof ./*.o</code>就可以看到CUDA 程序执行的具体内容。</p><p>另外，<code>nvprof --metrics</code> 命令的功能被转换到了 <code>ncu --metrics</code> 命令中，下面就对 <code>nvprof/ncu --metrics</code>命令的参数作详细解释，nsys 和 ncu 工具都有可视化版本，这里只讨论命令行版本。</p><h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><ul><li><code>inst_per_warp</code>: 每个 warp 执行的平均指令数</li><li><code>branch_efficiency</code>: 非发散分支与总分支的比率</li><li><code>warp_execution_efficiency</code>: 每个 warp 的平均活动线程数与 SM 支持的每个 warp 的最大线程数之比</li><li><code>warp_nonpred_execution_efficiency</code>: 执行非谓词指令的每个 warp 的平均活动线程数与 SM 支持的每个 warp 的最大线程数之比</li><li><code>inst_replay_overhead</code>: 每条指令执行的平均重放次数</li><li><code>shared_load_transactions_per_request</code>: 每次共享内存加载时执行的平均共享内存加载事务数</li><li><code>shared_store_transactions_per_request</code>: 每次共享内存加载时执行的平均共享内存写入事务数</li><li><code>local_load_transactions_per_request</code>: 每次本地内存加载执行的本地内存加载事务平均数</li><li><code>local_store_transactions_per_request</code>: 为每个本地内存存储执行的本地内存存储交易的平均数量</li><li><code>gld_transactions_per_request</code>: 为每个全局内存加载执行的全局内存加载事务的平均数。</li><li><code>gst_transactions_per_request</code>: 为每个全局内存存储执行的平均全局内存存储事务数</li><li><code>shared_store_transactions</code>: 共享内存存储事务数</li><li><code>shared_load_transactions</code>: 共享内存加载事务数</li><li><code>local_load_transactions</code>: 本地内存加载事务数</li><li><code>local_store_transactions</code>: 本地内存存储事务数</li><li><code>gld_transactions</code>: 全局内存加载事务数</li><li><code>gst_transactions</code>: 全局内存存储事务数</li><li><code>sysmem_read_transactions</code>: 系统内存读取事务数</li><li><code>sysmem_write_transactions</code>: 系统内存写入事务数</li><li><code>l2_read_transactions</code>: 所有读取请求在 L2 缓存中接收到的内存读取事务</li><li><code>l2_write_transactions</code>: 所有写入请求在 L2 缓存中接收到的内存写入事务</li><li><code>dram_read_transactions</code>: 设备内存读取事务</li><li><code>dram_write_transactions</code>: 设备内存写入事务</li><li><code>global_hit_rate</code>: 统一 L1/tex 缓存中全局加载的命中率</li><li><code>local_hit_rate</code>: 本地加载和存储的命中率</li><li><code>gld_requested_throughput</code>: 请求的全局内存负载吞吐量</li><li><code>gst_requested_throughput</code>: 请求的全局内存存储吞吐量</li><li><code>gld_throughput</code>: 全局内存负载吞吐量</li><li><code>gst_throughput</code>: 全局内存存储吞吐量</li><li><code>local_memory_overhead</code>: 本地内存流量占 L1 和 L2 缓存之间总内存流量之比</li><li><code>tex_cache_hit_rate</code>: 统一缓存命中率</li><li><code>l2_tex_read_hit_rate</code>: 来自纹理缓存的所有读取请求在 L2 缓存中的命中率</li><li><code>l2_tex_write_hit_rate</code>: 来自纹理缓存的所有写入请求在 L2 缓存中的命中率</li><li><code>dram_read_throughput</code>: 设备内存读取吞吐量</li><li><code>dram_write_throughput</code>: 设备内存写入吞吐量</li><li><code>tex_cache_throughput</code>: 统一缓存吞吐量</li><li><code>l2_tex_read_throughput</code>: 在 L2 缓存中接收到的来自纹理缓存的内存读取吞吐量</li><li><code>l2_tex_write_throughput</code>: 在 L2 缓存中接收到的来自纹理缓存的内存写入吞吐量</li><li><code>l2_read_throughput</code>: 在 L2 缓存中接收到的所有内存读取吞吐量</li><li><code>l2_write_throughput</code>: 在 L2 缓存中接收到的所有内存写入吞吐量</li><li><code>sysmem_read_throughput</code>: 系统内存读取吞吐量</li><li><code>sysmem_write_throughput</code>: 系统内存写入吞吐量</li><li><code>local_load_throughput</code>: 本地内存加载吞吐量</li><li><code>local_store_throughput</code>: 本地内存存储吞吐量</li><li><code>shared_load_throughput</code>: 共享内存负载吞吐量</li><li><code>shared_store_throughput</code>: 共享内存存储吞吐量</li><li><code>gld_efficiency</code>: 请求的全局内存负载吞吐量与所需的全局内存负载吞吐量的比率</li><li><code>gst_efficiency</code>: 请求的全局内存存储吞吐量与所需的全局内存存储吞吐量的比率</li><li><code>tex_cache_transactions</code>: 统一缓存读取事务</li><li><code>flop_count_dp</code>: 非谓词线程执行的双精度浮点运算数（加法、乘法和乘法累加）。每个乘法累加运算对计数贡献 2。</li><li><code>flop_count_dp_add</code>: 非断言线程执行的双精度浮点加法运算次数</li><li><code>flop_count_dp_fma</code>: 非谓词线程执行的双精度浮点乘累加运算次数，每个乘法累加运算使计数加一</li><li><code>flop_count_dp_mul</code>: 非谓词线程执行的双精度浮点乘法运算次数</li><li><code>flop_count_sp</code>: 非谓词线程执行的单精度浮点运算数（加法、乘法和乘法累加），每个乘法累加运算使计数加二（不包括特殊操作）</li><li><code>flop_count_sp_add</code>: 非断言线程执行的单精度浮点加法运算次数</li><li><code>flop_count_sp_fma</code>: 非谓词线程执行的单精度浮点乘累加运算次数。每个乘法累加运算使计数加一</li><li><code>flop_count_sp_mul</code>: 非谓词线程执行的单精度浮点乘法运算次数</li><li><code>flop_count_sp_special</code>: 非谓词线程执行的单精度浮点特殊操作数</li><li><code>inst_executed</code>: 执行的指令数</li><li><code>inst_issued</code>: 发出的指令数</li><li><p><code>dram_utilization</code>: 设备内存利用率相对于理论峰值利用率的级别，范围为 0 到 10</p></li><li><p><code>sysmem_utilization</code>: 系统内存利用率相对于理论峰值利用率的级别</p></li><li><code>stall_inst_fetch</code>: 由于尚未获取下一条汇编指令而发生的停顿百分比</li><li><code>stall_exec_dependency</code>: 由于指令所需的输入尚不可用而发生的停顿百分比</li><li><code>stall_memory_dependency</code>: 由于所需资源不可用或未完全利用而无法执行内存操作，或者由于给定类型的太多请求未完成而导致的停顿百分比</li><li><code>stall_texture</code>: 由于纹理子系统被充分利用或有太多未完成的请求而发生的停顿百分比</li><li><code>stall_sync</code>: 由于 warp 在 __syncthreads() 调用时被阻塞而发生的停顿百分比</li><li><code>stall_other</code>: 由于各种原因发生的停顿百分比</li><li><code>stall_constant_memory_dependency</code>: 由于立即常量高速缓存未命中而发生的停顿百分比</li><li><code>stall_pipe_busy</code>: 由于计算管道繁忙而无法执行计算操作而发生的停顿百分比</li><li><code>shared_efficiency</code>: 请求的共享内存吞吐量与所需共享内存吞吐量的比率</li><li><code>inst_fp_32</code>: 非谓词线程（算术、比较等）执行的单精度浮点指令数</li><li><code>inst_fp_64</code>: 非谓词线程（算术、比较等）执行的双精度浮点指令数</li><li><code>inst_integer</code>: 非谓词线程执行的整数指令数</li><li><code>inst_bit_convert</code>: 非谓词线程执行的位转换指令数</li><li><code>inst_control</code>: 非谓词线程（跳转、分支等）执行的控制流指令数</li><li><code>inst_compute_ld_st</code>: 非谓词线程执行的计算加载/存储指令数</li><li><code>inst_misc</code>: 非谓词线程执行的杂项指令数</li><li><code>inst_inter_thread_communication</code>: 非谓词线程执行的线程间通信指令数</li><li><code>issue_slots</code>: 使用的问题槽数</li><li><code>cf_issued</code>: 发出的控制流指令数</li><li><code>cf_executed</code>: 执行的控制流指令数</li><li><code>ldst_issued</code>: 发出的本地、全局、共享和纹理内存加载和存储指令的数量</li><li><code>ldst_executed</code>: 执行的本地、全局、共享和纹理内存加载和存储指令的数量</li><li><code>atomic_transactions</code>: 全局内存原子和减少事务</li><li><code>atomic_transactions_per_request</code>: 为每个原子和归约指令执行的全局内存原子和归约事务的平均数量</li><li><code>l2_atomic_throughput</code>: 在 L2 缓存中接收到的原子和减少请求的内存读取吞吐量</li><li><code>l2_atomic_transactions</code>: 在 L2 缓存中接收到的内存读取事务，用于原子请求和缩减请求</li><li><code>l2_tex_read_transactions</code>: 在 L2 缓存中接收到的内存读取事务，用于来自纹理缓存的读取请求</li><li><code>stall_memory_throttle</code>: 由于内存节流而发生的停顿百分比</li><li><code>stall_not_selected</code>: 由于未选择 warp 而发生的停顿百分比</li><li><code>l2_tex_write_transactions</code>: 在 L2 缓存中接收到的内存写入事务，用于来自纹理缓存的写入请求</li><li><code>flop_count_hp</code>: 非谓词线程执行的半精度浮点运算数（加法、乘法和乘法累加），每个乘法累加运算使计数加二</li><li><code>flop_count_hp_add</code>: 非断言线程执行的半精度浮点加法运算的次数</li><li><code>flop_count_hp_mul</code>: 非谓词线程执行的半精度浮点乘法运算次数</li><li><code>flop_count_hp_fma</code>: 非谓词线程执行的半精度浮点乘累加运算次数。每个乘法累加运算使计数加一</li><li><code>inst_fp_16</code>: 非谓词线程（算术、比较等）执行的半精度浮点指令数</li><li><code>ipc</code>: 每个周期执行的指令</li><li><code>issued_ipc</code>: 每个周期发出的指令</li><li><code>issue_slot_utilization</code>: 发出至少一条指令的发布槽的百分比，在所有周期中取平均值</li><li><code>sm_efficiency</code>: 至少一个 warp 在特定 SM 上处于活动状态的时间百分比</li><li><code>achieved_occupancy</code>: 每个活动周期的平均活动 warp 与 SM 支持的最大 warp 数之比</li><li><code>eligible_warps_per_cycle</code>: 每个活动周期有资格发布的平均 warp 数</li><li><code>shared_utilization</code>: 共享内存相对于理论峰值利用率的利用率级别</li><li><p><code>l2_utilization</code>: L2 缓存利用率相对于理论峰值利用率的级别，范围为 0 到 10</p></li><li><p><code>tex_utilization</code>: 统一缓存利用率相对于理论峰值利用率的级别</p></li><li><code>ldst_fu_utilization</code>: 执行共享加载、共享存储和恒定加载指令的 SM 的利用率级别</li><li><p><code>cf_fu_utilization</code>: 执行控制流指令的 SM 的利用率级别，范围为 0 到 10</p></li><li><p><code>tex_fu_utilization</code>: 执行全局、局部和纹理内存指令的 SM 的利用率级别，范围为 0 到 10</p></li><li><p><code>special_fu_utilization</code>: 执行 sin、cos、ex2、popc、flo 和类似指令的 SM 的利用率级别，范围为 0 到 10</p></li><li><p><code>half_precision_fu_utilization</code>: 执行 16 位浮点指令和整数指令的 SM 的利用率级别，范围为 0到10</p></li><li><p><code>single_precision_fu_utilization</code>: 执行单精度浮点指令和整数指令的 SM 的利用率级别</p></li><li><code>double_precision_fu_utilization</code>: 执行双精度浮点指令的 SM 的利用率级别</li><li><code>flop_hp_efficiency</code>: 实现的半精度浮点运算与理论峰值的比值</li><li><code>flop_sp_efficiency</code>: 实现的单精度浮点运算与理论峰值的比值</li><li><code>flop_dp_efficiency</code>: 实现的双精度浮点运算与理论峰值的比值</li><li><p><code>sysmem_read_utilization</code>: 系统内存的读取利用率相对于理论峰值利用率的级别，范围为 0 到 10</p></li><li><p><code>sysmem_write_utilization</code>: 系统内存的写入利用率相对于理论峰值利用率的级别，范围为 0 到 10</p></li></ul><h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><div class="table-container"><table><thead><tr><th>nvprof —metrics</th><th>ncu —metrics (&gt;= SM 7.0)</th></tr></thead><tbody><tr><td><code>achieved_occupancy</code></td><td><code>sm__warps_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>atomic_transactions</code></td><td><code>l1tex__t_set_accesses_pipe_lsu_mem_global_op_atom.sum + l1tex__t_set_accesses_pipe_lsu_mem_global_op_red.sum</code></td></tr><tr><td><code>atomic_transactions_per_request</code></td><td><code>(l1tex__t_sectors_pipe_lsu_mem_global_op_atom.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_red.sum) / (l1tex__t_requests_pipe_lsu_mem_global_op_atom.sum + l1tex__t_requests_pipe_lsu_mem_global_op_red.sum)</code></td></tr><tr><td><code>branch_efficiency</code></td><td><code>smsp__sass_average_branch_targets_threads_uniform.pct</code></td></tr><tr><td><code>cf_executed</code></td><td><code>smsp__inst_executed_pipe_cbu.sum + smsp__inst_executed_pipe_adu.sum</code></td></tr><tr><td><code>cf_fu_utilization</code></td><td><code>n/a</code></td></tr><tr><td><code>cf_issued</code></td><td><code>n/a</code></td></tr><tr><td><code>double_precision_fu_utilization</code></td><td><code>smsp__inst_executed_pipe_fp64.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>dram_read_bytes</code></td><td><code>dram__bytes_read.sum</code></td></tr><tr><td><code>dram_read_throughput</code></td><td><code>dram__bytes_read.sum.per_second</code></td></tr><tr><td><code>dram_read_transactions</code></td><td><code>dram__sectors_read.sum</code></td></tr><tr><td><code>dram_utilization</code></td><td><code>dram__throughput.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>dram_write_bytes</code></td><td><code>dram__bytes_write.sum</code></td></tr><tr><td><code>dram_write_throughput</code></td><td><code>dram__bytes_write.sum.per_second</code></td></tr><tr><td><code>dram_write_transactions</code></td><td><code>dram__sectors_write.sum</code></td></tr><tr><td><code>eligible_warps_per_cycle</code></td><td><code>smsp__warps_eligible.sum.per_cycle_active</code></td></tr><tr><td><code>flop_count_dp</code></td><td><code>smsp__sass_thread_inst_executed_op_dadd_pred_on.sum + smsp__sass_thread_inst_executed_op_dmul_pred_on.sum + smsp__sass_thread_inst_executed_op_dfma_pred_on.sum * 2</code></td></tr><tr><td><code>flop_count_dp_add</code></td><td><code>smsp__sass_thread_inst_executed_op_dadd_pred_on.sum</code></td></tr><tr><td><code>flop_count_dp_fma</code></td><td><code>smsp__sass_thread_inst_executed_op_dfma_pred_on.sum</code></td></tr><tr><td><code>flop_count_dp_mul</code></td><td><code>smsp__sass_thread_inst_executed_op_dmul_pred_on.sum</code></td></tr><tr><td><code>flop_count_hp</code></td><td><code>smsp__sass_thread_inst_executed_op_hadd_pred_on.sum + smsp__sass_thread_inst_executed_op_hmul_pred_on.sum + smsp__sass_thread_inst_executed_op_hfma_pred_on.sum * 2</code></td></tr><tr><td><code>flop_count_hp_add</code></td><td><code>smsp__sass_thread_inst_executed_op_hadd_pred_on.sum</code></td></tr><tr><td><code>flop_count_hp_fma</code></td><td><code>smsp__sass_thread_inst_executed_op_hfma_pred_on.sum</code></td></tr><tr><td><code>flop_count_hp_mul</code></td><td><code>smsp__sass_thread_inst_executed_op_hmul_pred_on.sum</code></td></tr><tr><td><code>flop_count_sp</code></td><td><code>smsp__sass_thread_inst_executed_op_fadd_pred_on.sum + smsp__sass_thread_inst_executed_op_fmul_pred_on.sum + smsp__sass_thread_inst_executed_op_ffma_pred_on.sum * 2</code></td></tr><tr><td><code>flop_count_sp_add</code></td><td><code>smsp__sass_thread_inst_executed_op_fadd_pred_on.sum</code></td></tr><tr><td><code>flop_count_sp_fma</code></td><td><code>smsp__sass_thread_inst_executed_op_ffma_pred_on.sum</code></td></tr><tr><td><code>flop_count_sp_mul</code></td><td><code>smsp__sass_thread_inst_executed_op_fmul_pred_on.sum</code></td></tr><tr><td><code>flop_count_sp_special</code></td><td><code>n/a</code></td></tr><tr><td><code>flop_dp_efficiency</code></td><td><code>smsp__sass_thread_inst_executed_ops_dadd_dmul_dfma_pred_on.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>flop_hp_efficiency</code></td><td><code>smsp__sass_thread_inst_executed_ops_hadd_hmul_hfma_pred_on.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>flop_sp_efficiency</code></td><td><code>smsp__sass_thread_inst_executed_ops_fadd_fmul_ffma_pred_on.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>gld_efficiency</code></td><td><code>smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct</code></td></tr><tr><td><code>gld_requested_throughput</code></td><td><code>n/a</code></td></tr><tr><td><code>gld_throughput</code></td><td><code>l1tex__t_bytes_pipe_lsu_mem_global_op_ld.sum.per_second</code></td></tr><tr><td><code>gld_transactions</code></td><td><code>l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum</code></td></tr><tr><td><code>gld_transactions_per_request</code></td><td><code>l1tex__average_t_sectors_per_request_pipe_lsu_mem_global_op_ld.ratio</code></td></tr><tr><td><code>global_atomic_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_global_op_atom.sum</code></td></tr><tr><td><code>global_hit_rate</code></td><td><code>(l1tex__t_sectors_pipe_lsu_mem_global_op_ld_lookup_hit.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_st_lookup_hit.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_red_lookup_hit.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_atom_lookup_hit.sum) / (l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_red.sum + l1tex__t_sectors_pipe_lsu_mem_global_op_atom.sum)</code></td></tr><tr><td><code>global_load_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_global_op_ld.sum</code></td></tr><tr><td><code>global_reduction_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_global_op_red.sum</code></td></tr><tr><td><code>global_store_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_global_op_st.sum</code></td></tr><tr><td><code>gst_efficiency</code></td><td><code>smsp__sass_average_data_bytes_per_sector_mem_global_op_st.pct</code></td></tr><tr><td><code>gst_requested_throughput</code></td><td><code>n/a</code></td></tr><tr><td><code>gst_throughput</code></td><td><code>l1tex__t_bytes_pipe_lsu_mem_global_op_st.sum.per_second</code></td></tr><tr><td><code>gst_transactions</code></td><td><code>l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum</code></td></tr><tr><td><code>gst_transactions_per_request</code></td><td><code>l1tex__average_t_sectors_per_request_pipe_lsu_mem_global_op_st.ratio</code></td></tr><tr><td><code>half_precision_fu_utilization</code></td><td><code>smsp__inst_executed_pipe_fp16.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>inst_bit_convert</code></td><td><code>smsp__sass_thread_inst_executed_op_conversion_pred_on.sum</code></td></tr><tr><td><code>inst_compute_ld_st</code></td><td><code>smsp__sass_thread_inst_executed_op_memory_pred_on.sum</code></td></tr><tr><td><code>inst_control</code></td><td><code>smsp__sass_thread_inst_executed_op_control_pred_on.sum</code></td></tr><tr><td><code>inst_executed</code></td><td><code>smsp__inst_executed.sum</code></td></tr><tr><td><code>inst_executed_global_atomics</code></td><td><code>smsp__sass_inst_executed_op_global_atom.sum</code></td></tr><tr><td><code>inst_executed_global_loads</code></td><td><code>smsp__inst_executed_op_global_ld.sum</code></td></tr><tr><td><code>inst_executed_global_reductions</code></td><td><code>smsp__inst_executed_op_global_red.sum</code></td></tr><tr><td><code>inst_executed_global_stores</code></td><td><code>smsp__inst_executed_op_global_st.sum</code></td></tr><tr><td><code>inst_executed_local_loads</code></td><td><code>smsp__inst_executed_op_local_ld.sum</code></td></tr><tr><td><code>inst_executed_local_stores</code></td><td><code>smsp__inst_executed_op_local_st.sum</code></td></tr><tr><td><code>inst_executed_shared_atomics</code></td><td><code>smsp__inst_executed_op_shared_atom.sum + smsp__inst_executed_op_shared_atom_dot_alu.sum + smsp__inst_executed_op_shared_atom_dot_cas.sum</code></td></tr><tr><td><code>inst_executed_shared_loads</code></td><td><code>smsp__inst_executed_op_shared_ld.sum</code></td></tr><tr><td><code>inst_executed_shared_stores</code></td><td><code>smsp__inst_executed_op_shared_st.sum</code></td></tr><tr><td><code>inst_executed_surface_atomics</code></td><td><code>smsp__inst_executed_op_surface_atom.sum</code></td></tr><tr><td><code>inst_executed_surface_loads</code></td><td><code>smsp__inst_executed_op_surface_ld.sum + smsp__inst_executed_op_shared_atom_dot_alu.sum + smsp__inst_executed_op_shared_atom_dot_cas.sum</code></td></tr><tr><td><code>inst_executed_surface_reductions</code></td><td><code>smsp__inst_executed_op_surface_red.sum</code></td></tr><tr><td><code>inst_executed_surface_stores</code></td><td><code>smsp__inst_executed_op_surface_st.sum</code></td></tr><tr><td><code>inst_executed_tex_ops</code></td><td><code>smsp__inst_executed_op_texture.sum</code></td></tr><tr><td><code>inst_fp_16</code></td><td><code>smsp__sass_thread_inst_executed_op_fp16_pred_on.sum</code></td></tr><tr><td><code>inst_fp_32</code></td><td><code>smsp__sass_thread_inst_executed_op_fp32_pred_on.sum</code></td></tr><tr><td><code>inst_fp_64</code></td><td><code>smsp__sass_thread_inst_executed_op_fp64_pred_on.sum</code></td></tr><tr><td><code>inst_integer</code></td><td><code>smsp__sass_thread_inst_executed_op_integer_pred_on.sum</code></td></tr><tr><td><code>inst_inter_thread_communication</code></td><td><code>smsp__sass_thread_inst_executed_op_inter_thread_communication_pred_on.sum</code></td></tr><tr><td><code>inst_issued</code></td><td><code>smsp__inst_issued.sum</code></td></tr><tr><td><code>inst_misc</code></td><td><code>smsp__sass_thread_inst_executed_op_misc_pred_on.sum</code></td></tr><tr><td><code>inst_per_warp</code></td><td><code>smsp__average_inst_executed_per_warp.ratio</code></td></tr><tr><td><code>inst_replay_overhead</code></td><td><code>n/a</code></td></tr><tr><td><code>ipc</code></td><td><code>smsp__inst_executed.avg.per_cycle_active</code></td></tr><tr><td><code>issue_slot_utilization</code></td><td><code>smsp__issue_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>issue_slots</code></td><td><code>smsp__inst_issued.sum</code></td></tr><tr><td><code>issued_ipc</code></td><td><code>smsp__inst_issued.avg.per_cycle_active</code></td></tr><tr><td><code>l1_sm_lg_utilization</code></td><td><code>l1tex__lsu_writeback_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>l2_atomic_throughput</code></td><td><code>2 * ( lts__t_sectors_op_atom.sum.per_second + lts__t_sectors_op_red.sum.per_second )</code></td></tr><tr><td><code>l2_atomic_transactions</code></td><td><code>2 * ( lts__t_sectors_op_atom.sum + lts__t_sectors_op_red.sum )</code></td></tr><tr><td><code>l2_global_atomic_store_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_global_op_atom.sum</code></td></tr><tr><td><code>l2_global_load_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_global_op_ld.sum</code></td></tr><tr><td><code>l2_local_global_store_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_local_op_st.sum + lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_global_op_st.sum</code></td></tr><tr><td><code>l2_local_load_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_lsu_mem_local_op_ld.sum</code></td></tr><tr><td><code>l2_read_throughput</code></td><td><code>lts__t_sectors_op_read.sum.per_second + lts__t_sectors_op_atom.sum.per_second + lts__t_sectors_op_red.sum.per_second</code></td></tr><tr><td><code>l2_read_transactions</code></td><td><code>lts__t_sectors_op_read.sum + lts__t_sectors_op_atom.sum + lts__t_sectors_op_red.sum</code></td></tr><tr><td><code>l2_surface_load_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_tex_mem_surface_op_ld.sum</code></td></tr><tr><td><code>l2_surface_store_bytes</code></td><td><code>lts__t_bytes_equiv_l1sectormiss_pipe_tex_mem_surface_op_st.sum</code></td></tr><tr><td><code>l2_tex_hit_rate</code></td><td><code>lts__t_sector_hit_rate.pct</code></td></tr><tr><td><code>l2_tex_read_hit_rate</code></td><td><code>lts__t_sector_op_read_hit_rate.pct</code></td></tr><tr><td><code>l2_tex_read_throughput</code></td><td><code>lts__t_sectors_srcunit_tex_op_read.sum.per_second</code></td></tr><tr><td><code>l2_tex_read_transactions</code></td><td><code>lts__t_sectors_srcunit_tex_op_read.sum</code></td></tr><tr><td><code>l2_tex_write_hit_rate</code></td><td><code>lts__t_sector_op_write_hit_rate.pct</code></td></tr><tr><td><code>l2_tex_write_throughput</code></td><td><code>lts__t_sectors_srcunit_tex_op_write.sum.per_second</code></td></tr><tr><td><code>l2_tex_write_transactions</code></td><td><code>lts__t_sectors_srcunit_tex_op_write.sum</code></td></tr><tr><td><code>l2_utilization</code></td><td><code>lts__t_sectors.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>l2_write_throughput</code></td><td><code>lts__t_sectors_op_write.sum.per_second + lts__t_sectors_op_atom.sum.per_second + lts__t_sectors_op_red.sum.per_second</code></td></tr><tr><td><code>l2_write_transactions</code></td><td><code>lts__t_sectors_op_write.sum + lts__t_sectors_op_atom.sum + lts__t_sectors_op_red.sum</code></td></tr><tr><td><code>ldst_executed</code></td><td><code>n/a</code></td></tr><tr><td><code>ldst_fu_utilization</code></td><td><code>smsp__inst_executed_pipe_lsu.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>ldst_issued</code></td><td><code>n/a</code></td></tr><tr><td><code>local_hit_rate</code></td><td><code>n/a</code></td></tr><tr><td><code>local_load_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_local_op_ld.sum</code></td></tr><tr><td><code>local_load_throughput</code></td><td><code>l1tex__t_bytes_pipe_lsu_mem_local_op_ld.sum.per_second</code></td></tr><tr><td><code>local_load_transactions</code></td><td><code>l1tex__t_sectors_pipe_lsu_mem_local_op_ld.sum</code></td></tr><tr><td><code>local_load_transactions_per_request</code></td><td><code>l1tex__average_t_sectors_per_request_pipe_lsu_mem_local_op_ld.ratio</code></td></tr><tr><td><code>local_memory_overhead</code></td><td><code>n/a</code></td></tr><tr><td><code>local_store_requests</code></td><td><code>l1tex__t_requests_pipe_lsu_mem_local_op_st.sum</code></td></tr><tr><td><code>local_store_throughput</code></td><td><code>l1tex__t_sectors_pipe_lsu_mem_local_op_st.sum.per_second</code></td></tr><tr><td><code>local_store_transactions</code></td><td><code>l1tex__t_sectors_pipe_lsu_mem_local_op_st.sum</code></td></tr><tr><td><code>local_store_transactions_per_request</code></td><td><code>l1tex__average_t_sectors_per_request_pipe_lsu_mem_local_op_st.ratio</code></td></tr><tr><td><code>nvlink_data_receive_efficiency</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_data_transmission_efficiency</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_overhead_data_received</code></td><td><code>(nvlrx__bytes_data_protocol.sum / nvlrx__bytes.sum) * 100</code></td></tr><tr><td><code>nvlink_overhead_data_transmitted</code></td><td><code>(nvltx__bytes_data_protocol.sum / nvltx__bytes.sum) * 100</code></td></tr><tr><td><code>nvlink_receive_throughput</code></td><td><code>nvlrx__bytes.sum.per_second</code></td></tr><tr><td><code>nvlink_total_data_received</code></td><td><code>nvlrx__bytes.sum</code></td></tr><tr><td><code>nvlink_total_data_transmitted</code></td><td><code>nvltx__bytes.sum</code></td></tr><tr><td><code>nvlink_total_nratom_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_total_ratom_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_total_response_data_received</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_total_write_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_transmit_throughput</code></td><td><code>nvltx__bytes.sum.per_second</code></td></tr><tr><td><code>nvlink_user_data_received</code></td><td><code>nvlrx__bytes_data_user.sum</code></td></tr><tr><td><code>nvlink_user_data_transmitted</code></td><td><code>nvltx__bytes_data_user.sum</code></td></tr><tr><td><code>nvlink_user_nratom_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_user_ratom_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_user_response_data_received</code></td><td><code>n/a</code></td></tr><tr><td><code>nvlink_user_write_data_transmitted</code></td><td><code>n/a</code></td></tr><tr><td><code>pcie_total_data_received</code></td><td><code>pcie__read_bytes.sum</code></td></tr><tr><td><code>pcie_total_data_transmitted</code></td><td><code>pcie__write_bytes.sum</code></td></tr><tr><td><code>shared_efficiency</code></td><td><code>smsp__sass_average_data_bytes_per_wavefront_mem_shared.pct</code></td></tr><tr><td><code>shared_load_throughput</code></td><td><code>l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum.per_second</code></td></tr><tr><td><code>shared_load_transactions</code></td><td><code>l1tex__data_pipe_lsu_wavefronts_mem_shared_op_ld.sum</code></td></tr><tr><td><code>shared_load_transactions_per_request</code></td><td><code>n/a</code></td></tr><tr><td><code>shared_store_throughput</code></td><td><code>l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum.per_second</code></td></tr><tr><td><code>shared_store_transactions</code></td><td><code>l1tex__data_pipe_lsu_wavefronts_mem_shared_op_st.sum</code></td></tr><tr><td><code>shared_store_transactions_per_request</code></td><td><code>n/a</code></td></tr><tr><td><code>shared_utilization</code></td><td><code>l1tex__data_pipe_lsu_wavefronts_mem_shared.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>single_precision_fu_utilization</code></td><td><code>smsp__pipe_fma_cycles_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>sm_efficiency</code></td><td><code>smsp__cycles_active.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>sm_tex_utilization</code></td><td><code>l1tex__texin_sm2tex_req_cycles_active.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>special_fu_utilization</code></td><td><code>smsp__inst_executed_pipe_xu.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>stall_constant_memory_dependency</code></td><td><code>smsp__warp_issue_stalled_imc_miss_per_warp_active.pct</code></td></tr><tr><td><code>stall_exec_dependency</code></td><td><code>smsp__warp_issue_stalled_short_scoreboard_per_warp_active.pct + smsp__warp_issue_stalled_wait_per_warp_active.pct</code></td></tr><tr><td><code>stall_inst_fetch</code></td><td><code>smsp__warp_issue_stalled_no_instruction_per_warp_active.pct</code></td></tr><tr><td><code>stall_memory_dependency</code></td><td><code>smsp__warp_issue_stalled_long_scoreboard_per_warp_active.pct</code></td></tr><tr><td><code>stall_memory_throttle</code></td><td><code>smsp__warp_issue_stalled_drain_per_warp_active.pct + smsp__warp_issue_stalled_lg_throttle_per_warp_active.pct</code></td></tr><tr><td><code>stall_not_selected</code></td><td><code>smsp__warp_issue_stalled_not_selected_per_warp_active.pct</code></td></tr><tr><td><code>stall_other</code></td><td><code>smsp__warp_issue_stalled_dispatch_stall_per_warp_active.pct + smsp__warp_issue_stalled_misc_per_warp_active.pct</code></td></tr><tr><td><code>stall_pipe_busy</code></td><td><code>smsp__warp_issue_stalled_math_pipe_throttle_per_warp_active.pct + smsp__warp_issue_stalled_mio_throttle_per_warp_active.pct</code></td></tr><tr><td><code>stall_sleeping</code></td><td><code>smsp__warp_issue_stalled_sleeping_per_warp_active.pct</code></td></tr><tr><td><code>stall_sync</code></td><td><code>smsp__warp_issue_stalled_barrier_per_warp_active.pct + smsp__warp_issue_stalled_membar_per_warp_active.pct</code></td></tr><tr><td><code>stall_texture</code></td><td><code>smsp__warp_issue_stalled_tex_throttle_per_warp_active.pct</code></td></tr><tr><td><code>surface_atomic_requests</code></td><td><code>l1tex__t_requests_pipe_tex_mem_surface_op_atom.sum</code></td></tr><tr><td><code>surface_load_requests</code></td><td><code>l1tex__t_requests_pipe_tex_mem_surface_op_ld.sum</code></td></tr><tr><td><code>surface_reduction_requests</code></td><td><code>l1tex__t_requests_pipe_tex_mem_surface_op_red.sum</code></td></tr><tr><td><code>surface_store_requests</code></td><td><code>l1tex__t_requests_pipe_tex_mem_surface_op_st.sum</code></td></tr><tr><td><code>sysmem_read_bytes</code></td><td><code>lts__t_sectors_aperture_sysmem_op_read * 32</code></td></tr><tr><td><code>sysmem_read_throughput</code></td><td><code>lts__t_sectors_aperture_sysmem_op_read.sum.per_second</code></td></tr><tr><td><code>sysmem_read_transactions</code></td><td><code>lts__t_sectors_aperture_sysmem_op_read.sum</code></td></tr><tr><td><code>sysmem_read_utilization</code></td><td><code>n/a</code></td></tr><tr><td><code>sysmem_utilization</code></td><td><code>n/a</code></td></tr><tr><td><code>sysmem_write_bytes</code></td><td><code>lts__t_sectors_aperture_sysmem_op_write * 32</code></td></tr><tr><td><code>sysmem_write_throughput</code></td><td><code>lts__t_sectors_aperture_sysmem_op_write.sum.per_second</code></td></tr><tr><td><code>sysmem_write_transactions</code></td><td><code>lts__t_sectors_aperture_sysmem_op_write.sum</code></td></tr><tr><td><code>sysmem_write_utilization</code></td><td><code>n/a</code></td></tr><tr><td><code>tensor_precision_fu_utilization</code></td><td><code>sm__pipe_tensor_op_hmma_cycles_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>tensor_precision_int_utilization</code></td><td><code>sm__pipe_tensor_op_imma_cycles_active.avg.pct_of_peak_sustained_active (SM 7.2+)</code></td></tr><tr><td><code>tex_cache_hit_rate</code></td><td><code>l1tex__t_sector_hit_rate.pct</code></td></tr><tr><td><code>tex_cache_throughput</code></td><td><code>n/a</code></td></tr><tr><td><code>tex_cache_transactions</code></td><td><code>l1tex__lsu_writeback_active.avg.pct_of_peak_sustained_active + l1tex__tex_writeback_active.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>tex_fu_utilization</code></td><td><code>smsp__inst_executed_pipe_tex.avg.pct_of_peak_sustained_active</code></td></tr><tr><td><code>tex_sm_tex_utilization</code></td><td><code>l1tex__f_tex2sm_cycles_active.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>tex_sm_utilization</code></td><td><code>sm__mio2rf_writeback_active.avg.pct_of_peak_sustained_elapsed</code></td></tr><tr><td><code>tex_utilization</code></td><td><code>n/a</code></td></tr><tr><td><code>texture_load_requests</code></td><td><code>l1tex__t_requests_pipe_tex_mem_texture.sum</code></td></tr><tr><td><code>warp_execution_efficiency</code></td><td><code>smsp__thread_inst_executed_per_inst_executed.ratio</code></td></tr><tr><td><code>warp_nonpred_execution_efficiency</code></td><td><code>smsp__thread_inst_executed_per_inst_executed.pct</code></td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> CUDA 编程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CT-Windowing医学CT图像增强</title>
      <link href="/2022/10/06/CT-Windowing%E5%8C%BB%E5%AD%A6CT%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/"/>
      <url>/2022/10/06/CT-Windowing%E5%8C%BB%E5%AD%A6CT%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>CT winding 是CT扫描的一项图像处理任务，它有助于突出关键的组织结构，通过修改 HU (Hounsfield Units) 参数，使图像更易于分析。</p><span id="more"></span><p><img src="/image/CT-Windowing医学CT图像增强/0.webp" style="zoom:50%;" /></p><h2 id="Hounsfield-Unit-HU"><a href="#Hounsfield-Unit-HU" class="headerlink" title="Hounsfield Unit (HU)"></a>Hounsfield Unit (HU)</h2><p>亨氏量表（Hounsfield scale）阻射率的数量尺度单位，命名自X射线电脑断层扫描的发明人高弗雷·豪斯费尔德（Godfrey Newbold Hounsfield）。亨氏单位常用于X射线电脑断层扫描，因此又称为CT值（CT number）。</p><p>HU 值与组织的组成和性质有关，因此代表各种组织的密度，HU 值越高，材料越致密，反之亦然。</p><p>以下为各组织在 HU 量表上的一些值</p><p><img src="/image/CT-Windowing医学CT图像增强/1.webp" style="zoom:80%;" /></p><h2 id="Window-Width-amp-Window-Level"><a href="#Window-Width-amp-Window-Level" class="headerlink" title="Window Width &amp; Window Level"></a>Window Width &amp; Window Level</h2><h3 id="Window-Width-WW"><a href="#Window-Width-WW" class="headerlink" title="Window Width (WW)"></a>Window Width (WW)</h3><p>是 CT 图像包含的 HU 值范围的度量，任何低于 WW 下限值的 HU 值将在扫描中显示为黑色，而高于 WW 上限值的 HU 值将显示为白色。</p><p>随着 WW 的增加，将需要更大的密度变化来改变代表某个 HU 单位的灰色阴影。这会导致对比度损失，因为随着 HU 值的增加，更多的结构将看起来相似（尽管具有不同的密度）。</p><p>随着 WW 的减小，较小的密度变化将导致 CT 图像上的颜色发生变化，导致密度接近的结构将分配给不同的灰度，这将增加对比度。</p><p><img src="/image/CT-Windowing医学CT图像增强/3.webp" style="zoom: 67%;" /></p><h3 id="Window-Level-WL"><a href="#Window-Level-WL" class="headerlink" title="Window Level (WL)"></a>Window Level (WL)</h3><p>表示窗口中心或中点的 HU 值，为图像设置的 WL 越低，整个图像将变得越亮。</p><p>随着 WL 的降低，较低的 HU 值就能将组织表示为白色，这将允许更多的白色通过，从而使图像更亮。因此，WL 影响图像的亮度。</p><p><img src="/image/CT-Windowing医学CT图像增强/4.webp" style="zoom:67%;" /></p><h3 id="WW-amp-WL-计算"><a href="#WW-amp-WL-计算" class="headerlink" title="WW &amp; WL 计算"></a>WW &amp; WL 计算</h3><p>灰度上限：WL + (WW / 2)<br>灰度下限：WL - (WW / 2)</p><p><img src="/image/CT-Windowing医学CT图像增强/2.webp" style="zoom:80%;" /></p><h3 id="WW-amp-WL-示例"><a href="#WW-amp-WL-示例" class="headerlink" title="WW &amp; WL 示例"></a>WW &amp; WL 示例</h3><p>在 CT 图像查看软件中，通常有各种标准 WL 和 WW。</p><div class="table-container"><table><thead><tr><th>Tissue</th><th>WW</th><th>WL</th></tr></thead><tbody><tr><td>骨头</td><td>2000</td><td>500</td></tr><tr><td>肺</td><td>1600</td><td>-600</td></tr><tr><td>腹</td><td>400</td><td>40</td></tr><tr><td>脑</td><td>70</td><td>30</td></tr><tr><td>软组织</td><td>350</td><td>50</td></tr><tr><td>肝</td><td>160</td><td>60</td></tr><tr><td>纵隔膜（胸腔）</td><td>500</td><td>50</td></tr><tr><td>中风（低密度脑成像）</td><td>30</td><td>30</td></tr><tr><td>血管成像</td><td>600</td><td>170</td></tr></tbody></table></div><h3 id="WW-amp-WL-选择"><a href="#WW-amp-WL-选择" class="headerlink" title="WW &amp; WL  选择"></a>WW &amp; WL  选择</h3><p>一、根据所需部位的 HU 值（对于CT图像而言）分布范围选取，若是增强 CT 的话 HU 值会有一些差别，可以观察直方图，自定义 WW 和 WL</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">transform_ctdata</span>(<span class="params">self, windowWidth, windowLevel, normal=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        注意，这个函数的self.image一定得是float类型的，否则就无效！</span></span><br><span class="line"><span class="string">        return: trucated image according to window level and window width</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        minWindow = <span class="built_in">float</span>(windowLevel) - <span class="number">0.5</span>*<span class="built_in">float</span>(windowWidth)</span><br><span class="line">        newimg = (self.image - minWindow) / <span class="built_in">float</span>(windowWidth)</span><br><span class="line">        newimg[newimg &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        newimg[newimg &gt; <span class="number">1</span>] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> normal:</span><br><span class="line">            newimg = (newimg * <span class="number">255</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> newimg</span><br></pre></td></tr></table></figure><p>二、根据图像的统计信息，例如图像均值作为窗口中心，$\pm \delta$ 的方差作为 WW</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> SimpleITK <span class="keyword">as</span> sitk</span><br><span class="line"><span class="keyword">import</span> torchvision <span class="keyword">as</span> tv</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StatisticalNormalization</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Normalize an image by mapping intensity with intensity distribution</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sigma</span>):</span><br><span class="line">        self.name = <span class="string">&#x27;StatisticalNormalization&#x27;</span></span><br><span class="line">        <span class="keyword">assert</span> <span class="built_in">isinstance</span>(sigma, <span class="built_in">float</span>)</span><br><span class="line">        self.sigma = sigma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, sample</span>):</span><br><span class="line">        image, label = sample[<span class="string">&#x27;image&#x27;</span>], sample[<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line">        statisticsFilter = sitk.StatisticsImageFilter()</span><br><span class="line">        statisticsFilter.Execute(image)</span><br><span class="line"></span><br><span class="line">        intensityWindowingFilter = v.IntensityWindowingImageFilter()</span><br><span class="line">        intensityWindowingFilter.SetOutputMaximum(<span class="number">255</span>)</span><br><span class="line">        intensityWindowingFilter.SetOutputMinimum(<span class="number">0</span>)</span><br><span class="line">        intensityWindowingFilter.SetWindowMaximum(</span><br><span class="line">            statisticsFilter.GetMean() + self.sigma * statisticsFilter.GetSigma())</span><br><span class="line">        intensityWindowingFilter.SetWindowMinimum(</span><br><span class="line">            statisticsFilter.GetMean() - self.sigma * statisticsFilter.GetSigma())</span><br><span class="line"></span><br><span class="line">        image = intensityWindowingFilter.Execute(image)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">&#x27;image&#x27;</span>: image, <span class="string">&#x27;label&#x27;</span>: label&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GroupFormer简要学习笔记</title>
      <link href="/2022/08/31/GroupFormer%E7%AE%80%E8%A6%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/08/31/GroupFormer%E7%AE%80%E8%A6%81%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>快速记录下 GroupFormer 网络的核心思想。</p><span id="more"></span><p>paper: </p><p>code: </p><h2 id="主要结构"><a href="#主要结构" class="headerlink" title="主要结构"></a>主要结构</h2><p>![figure1](</p><ol><li>提取视频clip的特征表示的<strong>CNN主干</strong>;</li><li>用于个体和场景特征初始化的<strong>群体表示生成器 (Group Representation Generator，GRG)</strong>;</li><li>用于建模时空关系，细化群体表示和个体表示的<strong>聚类时空Transformer (CSTT)</strong>;</li></ol><h2 id="特征提取器"><a href="#特征提取器" class="headerlink" title="特征提取器"></a>特征提取器</h2><p>Kinetics预训练的3D网络 (I3D) 作为Backbone</p><h2 id="群体表示生成器"><a href="#群体表示生成器" class="headerlink" title="群体表示生成器"></a>群体表示生成器</h2><p>是一个在模型中用于初始化群体表示的预处理组件，将场景特征和个体特征分别转换为视觉token，将它们聚合以生成群体表示</p><h2 id="聚类时空Transformer"><a href="#聚类时空Transformer" class="headerlink" title="聚类时空Transformer"></a>聚类时空Transformer</h2><h3 id="时空Transformer"><a href="#时空Transformer" class="headerlink" title="时空Transformer"></a>时空Transformer</h3><p>为群体活动识别而设计的时空Transformer(STT)增强了个体表征和群体表征。它包括两个并行的编码器（一个空间编码器和时间编码器 ），分别生成空间和时间特征。并引入交叉的个体解码器 来解码时空上下文信息。最后，用一个群体解码器 来增强群体的表示。</p><h4 id="Encoders"><a href="#Encoders" class="headerlink" title="Encoders"></a>Encoders</h4><p>采用了两个并行编码器来embed上下文特征。在一个分支中，作者采用了一个基于Transformer的空间编码器 来学习个体的上下文信息。将时间维度视为Batch维度，并应用一个编码器来建模所有帧的空间上下文。</p><p>另一种并行时间编码器 利用时间动态线索增强输入特征，通过突出每个个体沿时间维度的信息特征来丰富时间上下文。时间编码器遵循空间编码器的操作。与上述空间编码器的不同之处在于，时间编码器将空间维度视为一个Batch维度。</p><h4 id="Individual-Decoders"><a href="#Individual-Decoders" class="headerlink" title="Individual Decoders"></a>Individual Decoders</h4><p>作者提出了个体解码器 来综合考虑空间和时空上下文信息。个体解码器遵循标准Transformer的解码器的设计，来互补利用时空上下文。</p><p>对于空间解码器 ，空间编码器的输出结果被视为 actor query，时间编码器输出结果的embedding被视为key和value。actor query和key、value进行cross-attention，捕获时间动态，并输出更新后的上下文特征。</p><p>同样的，对于时间解码器 ，空间编码器的输出结果将时间维度与空间维度进行转换，可以看作是解码器使用的key和value。解码器将时间上下文视为time query，然后进行cross-attention的过程，时间解码器有助于查找视频中感兴趣的帧。</p><p>最后，将这两个交叉解码器的输出embedding进行融合，生成增强的个体表示。这两种交叉解码器是利用了基于空间上下文和时间上下文的语义关联来增强个体表征。</p><h4 id="Group-Decoder"><a href="#Group-Decoder" class="headerlink" title="Group Decoder"></a>Group Decoder</h4><p>引入了一个群体解码器 （Group Decoder）来通过个体表示来增强群体表示。群体解码器也遵循Transformer的解码器设计。与原Transformer的区别在于，群体解码器只包含多头交叉注意机制和一个前馈网络，不包含Self-Attention。</p><h3 id="Clustered-Attention-Mechanism"><a href="#Clustered-Attention-Mechanism" class="headerlink" title="Clustered Attention Mechanism"></a>Clustered Attention Mechanism</h3><p>虽然基于全连接注意机制的时空Transformer(STT)能够建模个体的关系，但它包含了许多不相关个体的关系。为了使模型能够关注关键的群体关系，作者将全连接的注意力替换为聚类的注意力，并将整个模块称之为聚类时空Transformer(Clustered Spatial-Temporal Transformer，CSTT) 。它可以对个体进行分组，并利用组内和组间的关系来捕获全局活动上下文。</p><p>首先将个体分组为C个聚类，然后计算一下两种类型的注意：</p><ol><li>组内注意（intra-group attention） ：只有来自同一个聚类内的query和key才会被考虑。</li><li>组间注意（inter-group attention） ：考虑了聚类之间成对的加权连接。</li></ol><h2 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h2><p>本文提出的CSTT以端到端的方式进行训练。在CSTT中，可以直接从群体表示生成群体活动分数。同样，采用另一个分类器，使用CSTT生成的个体表示来预测个体的动作得分。对于这两个任务，作者都选择了交叉熵损失函数来指导优化过程：</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在本文中，作者提出了一种新的基于Transformer的结构，称为GroupFormer，它联合建模了时空上下文表示来推断群体活动。此外，作者还引入了聚类注意机制来对个体进行分组，并利用组内和组间的关系获得更好的群体特征表示。作者在两个数据集上进行了广泛的实验。结果表明，GroupFormer的表现超过了大多数目前的SOTA方法。</p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 群体行为识别 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯函数与高斯核函数</title>
      <link href="/2022/08/07/%E9%AB%98%E6%96%AF%E5%87%BD%E6%95%B0%E4%B8%8E%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0/"/>
      <url>/2022/08/07/%E9%AB%98%E6%96%AF%E5%87%BD%E6%95%B0%E4%B8%8E%E9%AB%98%E6%96%AF%E6%A0%B8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>图像处理领域中，高斯核函数常用于图像低通滤波，记录下原理，同时用代码实现下。</p><span id="more"></span><h2 id="高斯函数"><a href="#高斯函数" class="headerlink" title="高斯函数"></a>高斯函数</h2><h3 id="一维高斯函数"><a href="#一维高斯函数" class="headerlink" title="一维高斯函数"></a>一维高斯函数</h3><script type="math/tex; mode=display">f(x) = ae^{\frac{-(x-b)^2}{2c^2}}</script><p>$a, b, c$ 为实数常数， 且 $a&gt;0$</p><p>$a$ 为曲线尖峰高度，$b$ 为尖峰的中心位置， $c$ 为钟的宽度</p><p><img src="" style="zoom: 40%;" /></p><p>由高斯积分公式，得</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}e^{-x^2}dx = \sqrt{\pi}</script><p>令 $y=x-b$, 得 $dx = dy$</p><script type="math/tex; mode=display">a \int_{-\infty}^{\infty}e^{-\frac{y^2}{2c^2}}dx</script><p>令 $z= \frac{y}{\sqrt{2}c}$, 得 $dy = \sqrt{2}cdz$</p><script type="math/tex; mode=display">\begin{aligned}  & a\int_{-\infty}^{\infty}e^{-z^2}\sqrt{2}cdz \\= & \sqrt{2}ac \int_{-\infty}^{\infty}e^{-z^2}dz \\= & \sqrt{2\pi}\cdot ac\end{aligned}</script><p>得</p><script type="math/tex; mode=display">\int_{-\infty}^{\infty}ae^{\frac{-(x-b)^2}{2c^2}}dx = \sqrt{2\pi}\cdot ac</script><p>令 $f(x)=1$，使宽度范围内所有概率为$ 1$，得</p><script type="math/tex; mode=display">a = \frac{1}{\sqrt{2\pi}\cdot c}</script><p>令期望 $\mu$ 为 $b$，标准差 $\sigma$ 为 $c$，得满足正态分布的高斯函数</p><script type="math/tex; mode=display">g(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^{2}}</script><h3 id="二维高斯函数（高斯分布、正态分布）"><a href="#二维高斯函数（高斯分布、正态分布）" class="headerlink" title="二维高斯函数（高斯分布、正态分布）"></a>二维高斯函数（高斯分布、正态分布）</h3><script type="math/tex; mode=display">G(x,y) = \frac{1}{2\pi\sigma^2}e^{-(x^2+y^2)/2\sigma^2}</script><p>$\mu=0$，即原点为中心点</p><p>在实际编程应用中，高斯函数中的参数如下</p><ul><li>ksize 核大小</li><li>sigma 方差</li><li>center 尖峰中心点坐标</li><li>bias 尖峰中心点的偏移量，用于控制截断高斯函数</li></ul><p>以下程序递增高斯函数的方差，并将结果图保存为 gif 图像</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">item = <span class="number">10</span></span><br><span class="line">dt = <span class="number">1</span></span><br><span class="line">ksize = <span class="number">20</span></span><br><span class="line">sigma = <span class="number">2</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">plt.ion()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(item)):</span><br><span class="line">    center = <span class="built_in">round</span>(ksize/<span class="number">2</span>)</span><br><span class="line">    bias = ksize *<span class="number">10</span>/<span class="number">10</span></span><br><span class="line">    ksigma = np.multiply(cv2.getGaussianKernel(ksize, sigma),</span><br><span class="line">            (cv2.getGaussianKernel(ksize,sigma)).T)</span><br><span class="line">    [m, n] = ksigma.shape</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(m)):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">list</span>(<span class="built_in">range</span>(n)):</span><br><span class="line">            <span class="keyword">if</span>((x&lt;center-bias) <span class="keyword">or</span> (x&gt;center+bias) <span class="keyword">or</span> (y&lt;center-bias) <span class="keyword">or</span></span><br><span class="line">                    (y&gt;center+bias)):</span><br><span class="line">                ksigma[x, y] = <span class="number">0</span></span><br><span class="line">    sigma = sigma + dt</span><br><span class="line"></span><br><span class="line">    ax3 = plt.axes(projection=<span class="string">&#x27;3d&#x27;</span>)</span><br><span class="line">    ax3.set_zlim3d(<span class="number">0</span>,<span class="number">8e-3</span>)</span><br><span class="line">    x = <span class="built_in">list</span>(<span class="built_in">range</span>(ksize))</span><br><span class="line">    y = x</span><br><span class="line">    ax3.plot_surface(x,y,ksigma,cmap=<span class="string">&#x27;rainbow&#x27;</span>)</span><br><span class="line">    plt.draw()</span><br><span class="line">    plt.pause(<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>图片如下</p><p><img src="/image/高斯函数与高斯核函数/gaussian_curve.gif" style="zoom: 80%;" /></p><p>随着方差的增大，整个高斯函数的尖峰逐渐减小，整体也变的更加平缓，则对图像的平滑效果越来越明显</p><p>保持参数不变，对上述高斯函数进行截断，bias 的大小为 ksize *3/10，则结果如下</p><p><img src="/image/高斯函数与高斯核函数/gaussian_curve2.gif" style="zoom: 80%;" /></p><p>bias 的作用主要是对超过一定区域的原始图像信息不再考虑，这就保证在更加合理的利用靠近高斯函数中心点的周围像素，同时还可以改变高斯函数的中心坐标</p><h2 id="高斯核函数卷积"><a href="#高斯核函数卷积" class="headerlink" title="高斯核函数卷积"></a>高斯核函数卷积</h2><p>径向基函数（Radial Basis Function）， 就是某种沿径向对称的标量函数。 通常定义为空间中任一点 $x_1$ 到某一中心 $x_2$ 之间欧氏距离的单调函数，其作用往往是局部的 , 即当 $x_1$ 远离 $x_2$ 时函数取值很小</p><p>最常用的一个核函数为高斯核函数，形式为</p><script type="math/tex; mode=display">k(\|x_1-x_2\|)=e^{-\|x_1-x_2\|^2/2\sigma^2}</script><p>也称为<strong>径向基函数</strong></p><p>高斯核函数的代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gaussian_kernel</span>(<span class="params">x1, x2, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Easy to understand but inefficient.&quot;&quot;&quot;</span></span><br><span class="line">    m, n = x1.shape[<span class="number">0</span>], x2.shape[<span class="number">0</span>]</span><br><span class="line">    dist_matrix = np.zeros((m, n), dtype=<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            dist_matrix[i][j] = np.<span class="built_in">sum</span>((x1[i] - x2[j]) ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> sigma_f ** <span class="number">2</span> * np.exp(- <span class="number">0.5</span> / l ** <span class="number">2</span> * dist_matrix)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gaussian_kernel_vectorization</span>(<span class="params">x1, x2, l=<span class="number">1.0</span>, sigma_f=<span class="number">1.0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;More efficient approach.&quot;&quot;&quot;</span></span><br><span class="line">    dist_matrix = np.<span class="built_in">sum</span>(x1**<span class="number">2</span>, <span class="number">1</span>).reshape(-<span class="number">1</span>, <span class="number">1</span>) + np.<span class="built_in">sum</span>(x2**<span class="number">2</span>, <span class="number">1</span>) - <span class="number">2</span> * np.dot(x1, x2.T)</span><br><span class="line">    <span class="keyword">return</span> sigma_f ** <span class="number">2</span> * np.exp(-<span class="number">0.5</span> / l ** <span class="number">2</span> * dist_matrix)</span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">700</span>, <span class="number">800</span>, <span class="number">1029</span>]).reshape(-<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(gaussian_kernel_vectorization(x, x, l=<span class="number">500</span>, sigma=<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">100.</span>          <span class="number">98.01986733</span>  <span class="number">80.5347031</span> ]</span><br><span class="line"> [ <span class="number">98.01986733</span> <span class="number">100.</span>          <span class="number">90.04307671</span>]</span><br><span class="line"> [ <span class="number">80.5347031</span>   <span class="number">90.04307671</span> <span class="number">100.</span>        ]]</span><br></pre></td></tr></table></figure><h2 id="高斯函数性质"><a href="#高斯函数性质" class="headerlink" title="高斯函数性质"></a>高斯函数性质</h2><p>高斯函数具有五个重要的性质，这些性质使得它在早期图像处理中特别有用。这些性质表明，高斯平滑滤波器无论在空间域还是在频率域都是十分有效的低通滤波器，且在实际图像处理中得到了有效使用。高斯函数具有五个十分重要的性质</p><ol><li><p>二维高斯函数具有旋转对称性，即滤波器在各个方向上的平滑程度是相同的。一般来说，一幅图像的边缘方向是事先不知道的，因此，在滤波前是无法确定一个方向上比另一方向上需要更多的平滑。旋转对称性意味着高斯平滑滤波器在后续边缘检测中不会偏向任一方向。</p></li><li><p>高斯函数是单值函数。这表明，高斯滤波器用像素邻域的加权均值来代替该点的像素值，而每一邻域像素点权值是随该点与中心点的距离单调增减的。边缘是一种图像局部特征，如果平滑运算对离算子中心很远的像素点仍然有很大作用，则平滑运算会使图像失真。</p></li><li><p>高斯函数的傅立叶变换频谱是单瓣的。正如下面所示，这一性质是高斯函数傅立叶变换等于高斯函数本身这一事实的直接推论。图像常被不希望的高频信号所污染（噪声和细纹理）。而所希望的图像特征（如边缘），既含有低频分量，又含有高频分量。高斯函数傅立叶变换的单瓣意味着平滑图像不会被不需要的高频信号所污染，同时保留了大部分所需信号。</p></li><li><p>高斯滤波器宽度(决定着平滑程度)是由参数 $\sigma$ 表征的，而且 $\sigma$ 和平滑程度的关系是非常简单的。 $\sigma$ 越大，高斯滤波器的频带就越宽，平滑程度就越好。通过调节平滑程度参数 $\sigma$ ，可在图像特征过分模糊(过平滑)与平滑图像中由于噪声和细纹理所引起的过多的不希望突变量（欠平滑）之间取得折衷。</p></li><li><p>由于高斯函数的可分离性，大高斯滤波器可以得以有效地实现。二维高斯函数卷积可以分两步来进行，首先将图像与一维高斯函数进行卷积，然后将卷积结果与方向垂直的相同一维高斯函数卷积。因此，二维高斯滤波的计算量随滤波模板宽度成线性增长而不是成平方增长。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 空间滤波 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的2022暑假独居生活</title>
      <link href="/2022/06/12/%E6%88%91%E7%9A%842022%E6%9A%91%E5%81%87%E7%8B%AC%E5%B1%85%E7%94%9F%E6%B4%BB/"/>
      <url>/2022/06/12/%E6%88%91%E7%9A%842022%E6%9A%91%E5%81%87%E7%8B%AC%E5%B1%85%E7%94%9F%E6%B4%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>一张小方桌，有一荤一素</p><span id="more"></span><h2 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h2><h3 id="2022年6月10日"><a href="#2022年6月10日" class="headerlink" title="2022年6月10日"></a>2022年6月10日</h3><ul><li>青椒土豆丝</li><li>腊肠蒸蛋</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-10.webp" style="zoom: 50%;" /></p><h3 id="2022年6月11日"><a href="#2022年6月11日" class="headerlink" title="2022年6月11日"></a>2022年6月11日</h3><ul><li>青椒土豆丝</li><li>红烧带鱼</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-11.webp" style="zoom: 50%;" /></p><h3 id="2022年6月12日"><a href="#2022年6月12日" class="headerlink" title="2022年6月12日"></a>2022年6月12日</h3><ul><li>土豆鸡腿</li><li>炒青菜</li><li>紫菜蛋汤</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-12.webp" style="zoom: 50%;" /></p><h3 id="2022年6月13日"><a href="#2022年6月13日" class="headerlink" title="2022年6月13日"></a>2022年6月13日</h3><ul><li>番茄鸡蛋面</li><li>包菜炒肉</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-13.webp" style="zoom: 50%;" /></p><h3 id="2022年6月14日"><a href="#2022年6月14日" class="headerlink" title="2022年6月14日"></a>2022年6月14日</h3><ul><li>番茄鸡蛋面</li><li>包菜炒肉</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-14.webp" style="zoom: 50%;" /></p><h3 id="2022年6月16日"><a href="#2022年6月16日" class="headerlink" title="2022年6月16日"></a>2022年6月16日</h3><ul><li>蛋黄粽</li><li>红枣粽</li><li>虎皮青椒</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-16.webp" style="zoom: 50%;" /></p><h3 id="2022年6月17日"><a href="#2022年6月17日" class="headerlink" title="2022年6月17日"></a>2022年6月17日</h3><ul><li>红烧鳊鱼</li><li>土豆洋葱炒腊肠</li><li>紫菜蛋汤</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-17.webp" style="zoom: 50%;" /></p><h3 id="2022年6月23日-30-日"><a href="#2022年6月23日-30-日" class="headerlink" title="2022年6月23日 - 30 日"></a>2022年6月23日 - 30 日</h3><ul><li>去学校天天吃大餐</li></ul><p><img src="/image/我的2022暑假独居生活/2022-06-24.webp" style="zoom: 50%;" /></p><p><img src="/image/我的2022暑假独居生活/2022-06-30.webp" style="zoom: 50%;" /></p><h3 id="2022年07月02日"><a href="#2022年07月02日" class="headerlink" title="2022年07月02日"></a>2022年07月02日</h3><ul><li>蒸贝贝南瓜</li><li>蒜蓉茄子</li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-02.webp" style="zoom: 50%;" /></p><h3 id="2022年07月03日"><a href="#2022年07月03日" class="headerlink" title="2022年07月03日"></a>2022年07月03日</h3><ul><li>金色传说煎饺</li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-03.webp" style="zoom: 50%;" /></p><h3 id="2022年07月04日"><a href="#2022年07月04日" class="headerlink" title="2022年07月04日"></a>2022年07月04日</h3><ul><li>空气炸锅奥尔良鸡腿</li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-04.webp" style="zoom: 50%;" /></p><h3 id="2022年07月07日"><a href="#2022年07月07日" class="headerlink" title="2022年07月07日"></a>2022年07月07日</h3><ul><li><p>番茄马铃薯排骨</p></li><li><p>炒豇豆</p></li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-07.webp" style="zoom: 50%;" /></p><h3 id="2022年07月14日"><a href="#2022年07月14日" class="headerlink" title="2022年07月14日"></a>2022年07月14日</h3><ul><li>奥尔良鸡腿</li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-14.webp" style="zoom: 50%;" /></p><h3 id="2022年07月19日"><a href="#2022年07月19日" class="headerlink" title="2022年07月19日"></a>2022年07月19日</h3><ul><li>糖果煮蛋</li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-19.webp" style="zoom: 50%;" /></p><h3 id="2022年07月20日"><a href="#2022年07月20日" class="headerlink" title="2022年07月20日"></a>2022年07月20日</h3><ul><li><p>空气炸锅鸡米花</p></li><li><p>丝瓜炒蛋</p></li><li><p>番茄鸡蛋汤</p></li><li><p>青椒土豆丝</p></li></ul><p><img src="/image/我的2022暑假独居生活/2022-07-20.webp" style="zoom: 50%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>第二届网刃杯misc所见非所见wp</title>
      <link href="/2022/04/25/%E7%AC%AC%E4%BA%8C%E5%B1%8A%E7%BD%91%E5%88%83%E6%9D%AFmisc%E6%89%80%E8%A7%81%E9%9D%9E%E6%89%80%E8%A7%81wp/"/>
      <url>/2022/04/25/%E7%AC%AC%E4%BA%8C%E5%B1%8A%E7%BD%91%E5%88%83%E6%9D%AFmisc%E6%89%80%E8%A7%81%E9%9D%9E%E6%89%80%E8%A7%81wp/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>记录一下这道深度学习入门的wp</p><span id="more"></span><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>隐藏层的输出为512，输出层的输出10，导入模型得到报错，神经网络的输入必须为<code>(28,28)</code>，再用 <code>tf.latest_checkpoint</code> 和 <code>tf.load_weights</code> 导入模型文件，最后输出概率最大的预测结果组成 <code>flag</code> ，由于预测图片在压缩中有精度误差，和真实结果不同，所幸赛方给出了 <code>hint4</code>，修改第4位为 2。</p><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> models,layers</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">model</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        model = models.Sequential()</span><br><span class="line">        model.add(layers.Flatten(input_shape = (<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)))</span><br><span class="line">        model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">        model.add(layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        model.summary()</span><br><span class="line">        self.model = model</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Predict</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        latest = tf.train.latest_checkpoint(<span class="string">&#x27;./weights&#x27;</span>)</span><br><span class="line">        self.cnn = model()</span><br><span class="line">        self.cnn.model.load_weights(latest)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, image_path</span>):</span><br><span class="line">        img = Image.<span class="built_in">open</span>(image_path).convert(<span class="string">&#x27;I&#x27;</span>)</span><br><span class="line">        img = img.resize((<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">        img = np.reshape(img, (<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)) / <span class="number">255.</span></span><br><span class="line">        x = np.array([img])</span><br><span class="line">        y = np.argmax(self.cnn.model.predict(x))</span><br><span class="line">        <span class="built_in">print</span>(image_path)</span><br><span class="line">        <span class="built_in">print</span>(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    p = Predict()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">14</span>):</span><br><span class="line">        p.predict(<span class="string">&#x27;./flag/&#123;&#125;.webp&#x27;</span>.<span class="built_in">format</span>(i))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> misc </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多目标优化笔记</title>
      <link href="/2022/04/19/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/04/19/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>多目标优化 ( MOO ) - Multi-Objective Optimization</p><p>从问题定义，单目标、多目标，无约束、有约束方面了解多目标优化</p><span id="more"></span><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="无约束的单目标优化问题"><a href="#无约束的单目标优化问题" class="headerlink" title="无约束的单目标优化问题"></a>无约束的单目标优化问题</h3><script type="math/tex; mode=display">\min_{x} f(x),x \in R^{N}</script><h3 id="无约束的多目标优化问题"><a href="#无约束的多目标优化问题" class="headerlink" title="无约束的多目标优化问题"></a>无约束的多目标优化问题</h3><script type="math/tex; mode=display">\min_{x} F(x) = [f_1(x), f_2(x),\cdots, f_n(x)], x \in R^{N}</script><p>$ n $ 为子目标的数量，$ f_n(x) $为一阶可导目标函数 $F(x)$ 的子函数</p><h3 id="有约束的单目标优化问题"><a href="#有约束的单目标优化问题" class="headerlink" title="有约束的单目标优化问题"></a>有约束的单目标优化问题</h3><script type="math/tex; mode=display">\begin{array}{lcl}\min_{x} & f(x) & \\\text { s.t. } & g_{i}(x) \geq 0 & , i \in[1, M] \\& h_{j}(x)=0 & , j \in[1, L]\end{array}</script><p>$\text { s.t. }$ 为 <code>subject to</code> ，受限于的缩写，设 $ D $ 为可行域</p><script type="math/tex; mode=display">D = \{x | g_i(x) \geq 0, i \in [1, M], h_j(x) =0 , j \in [1,L]  \}</script><h3 id="有约束的多目标优化问题"><a href="#有约束的多目标优化问题" class="headerlink" title="有约束的多目标优化问题"></a>有约束的多目标优化问题</h3><script type="math/tex; mode=display">\begin{array}{lcl}\min_x & F(x) = [f_1(x), f_2(x),\cdots, f_n(x)]\\\text{s.t.} & g_i(x) \geq 0 ,\quad i\in(1, M) \\& h_j(x) = 0 ,\quad j \in (1,L)\end{array}</script><p>设 $ D $ 为可行域</p><script type="math/tex; mode=display">D = \{x | g_i(x) \geq 0, i \in [1, M], h_j(x) =0 , j \in [1,L]  \}</script><h2 id="MOO-的解集"><a href="#MOO-的解集" class="headerlink" title="MOO 的解集"></a>MOO 的解集</h2><p>对于 MOO，通常没有解  $ x^\ast \in D $, 使 $ f_i(x), \forall i \in [1,N] $  同时处于最优解，因此单目标优化问题的解在 MOO 中通常不适用</p><p>MOO 中的解集分为 <strong>绝对有效解</strong>，<strong>有效解</strong>，<strong>弱有效解</strong></p><p>设 $R^N $为 $N$ 维的实向量空间，$y=(y_1,y_2,\cdots, y_N)^{T}$，$z=(z_1,z_2,\cdots, z_N)^{T}$</p><script type="math/tex; mode=display">\begin{cases}\text { 相等 } & y=z \Leftrightarrow y_{i}=z_{i}, i=1,2, \ldots, N \\ \text { 严格小于 } & y<z \Leftrightarrow y_{i}<z_{i}, i=1,2, \ldots, N \\ \text { 小于 } & y \leqq z \Leftrightarrow y_{i} \leqslant z_{i}, i=1,2, \ldots, N \\ \text { 小于且不相等(支配) } & y \leqslant z \Leftrightarrow y_{i} \leqslant z_{i}, i=1,2, \ldots, N, y \neq z\end{cases}</script><h3 id="Pareto-支配（Pareto-Dominance）"><a href="#Pareto-支配（Pareto-Dominance）" class="headerlink" title="Pareto 支配（Pareto Dominance）"></a>Pareto 支配（Pareto Dominance）</h3><p>$ \forall x_1, x_2 \in R^{N}     $， 对于 $ k = 1,\cdots,K $，都有 $ f_k(x_1) \leqslant f_k(x_2)$，则 $ x_1 $ 支配 $ x_2 $</p><p><img src="/image/多目标优化笔记/D.webp" style="zoom:67%;" /></p><h3 id="Pareto-解集（绝对最优解）"><a href="#Pareto-解集（绝对最优解）" class="headerlink" title="Pareto 解集（绝对最优解）"></a>Pareto 解集（绝对最优解）</h3><p>$ x^{\ast}{\in}{D} $ ， $ \forall x \in D,\quad f(x^\ast) {\leqq} f(x) $ ，即 $ \forall k \in 1,\cdots,K, \quad f<em>K(x^\ast) \leqq f</em>{K}(x) $ ，则 $ x^\ast $ 为 MOO 问题的最优解</p><h3 id="Pareto-解集（有效解）"><a href="#Pareto-解集（有效解）" class="headerlink" title="Pareto 解集（有效解）"></a>Pareto 解集（有效解）</h3><p>$x^\ast\in{D}$ ，若 $f_k(x)\leq f_k(x^\ast) \wedge \exists i,f_i(x) &lt; f_i(x^\ast),i\in [1,k]$ 不成立，则 $x^\ast$ 是 MOO 问题的有效解，也叫 Pareto 最优解，其含义是如果 $x^\ast$ 是 Pareto 最优解，则找不到这样的可行解  $x\in{D}$ ，使得 $f(x)$ 的每个目标值都不比 $f(x^\ast)$ 的目标值坏,并且 $ f (x) $ 至少有一个目标比 $f(x^\ast)$ 的相应目标值好，即 $ x^\ast $ 是最好的，不能再进行改进（Pareto 改进）</p><h3 id="Pareto-解集（弱有效解）"><a href="#Pareto-解集（弱有效解）" class="headerlink" title="Pareto 解集（弱有效解）"></a>Pareto 解集（弱有效解）</h3><p>$x^\ast\in{D}$ ，如果不存在 $x\in{D}$，使得 $f(x)&lt;f(x^\ast)$ ，即</p><script type="math/tex; mode=display">f_k(x) < f_k(x^*) \quad \wedge \quad \forall k \in [1,K]</script><p>则 $x^\ast$ 是 MOO 问题的有效解，其含义是如果 $x^\ast$ 是弱有效解,则找不到这样的可行解 $x\in{D}$，使得 $f(x)$ 的每个目标值都比 $f(x^\ast)$ 的目标值严格（ $&lt;$ ）的好</p><h3 id="Pareto-最优解集（Pareto-optimal-Set）"><a href="#Pareto-最优解集（Pareto-optimal-Set）" class="headerlink" title="Pareto 最优解集（Pareto-optimal Set）"></a>Pareto 最优解集（Pareto-optimal Set）</h3><p>给定问题的有效解集（Pareto 最优解）构成的解集，集合中的解是相互非支配的，两两非支配关系，简称 $PS$</p><h3 id="Pareto-最优前沿（Pareto-optimal-Front）"><a href="#Pareto-最优前沿（Pareto-optimal-Front）" class="headerlink" title="Pareto 最优前沿（Pareto-optimal Front）"></a>Pareto 最优前沿（Pareto-optimal Front）</h3><p>Pareto 每一个解对应的目标值向量组成的集合，简称 $PF$</p><script type="math/tex; mode=display">PF = \{F(x)|x\in PS\}</script><p><img src="/image/多目标优化笔记/PF.webp" style="zoom:50%;" /></p><h3 id="MOO-的最优性条件"><a href="#MOO-的最优性条件" class="headerlink" title="MOO 的最优性条件"></a>MOO 的最优性条件</h3><p>约束规格定义：对优化问题的约束函数，附加某些限制条件，使得其最优解满足的最优性条件</p><p>下面给出一个严格条件下多目标优化的充分必要条件，给出的充要条件前，先引入了约束规格条件</p><script type="math/tex; mode=display">\begin{aligned}\min _{x \in \hat{D}} F(x) &=\sum_{k=1}^{K} f_{k}(x) \\\hat{D} &=x \in D \mid f(x) \leq f(\hat{x})\end{aligned}</script><p>定理：设 $ f (x)$ ，$ g(x) $ 为凸函数,且在 $x \in D$ 处可微，$h(x)$ 为线性函数，且 $\hat{D} = x \in D|f (x) \leq f (\hat{x})$ 满足 $ KKT $ 约束规格，则 $x^\ast$ 是 MOO 的有效解的充分必要条件是存在 $ \lambda \in  R^K , u \in R^M , v \in R^L$， 使得</p><script type="math/tex; mode=display">\left\{\begin{array}{l}\nabla_{x} L\left(x^{*}, \lambda^{*}, u^{*}, v^{*}\right)=\nabla f\left(x^{*}\right) \lambda^{*}+\nabla g\left(x^{*}\right) u^{*}+\nabla h\left(x^{*}\right) v^{*}=0 \\u^{* T} g\left(x^{*}\right)=0 \\\lambda^{*}>0, u^{*} \geq 0\end{array}\right.</script><h2 id="MOO-的经典算法"><a href="#MOO-的经典算法" class="headerlink" title="MOO 的经典算法"></a>MOO 的经典算法</h2><h3 id="线性加权法"><a href="#线性加权法" class="headerlink" title="线性加权法"></a>线性加权法</h3><p>根据 $f(x)$ 的重要程度，设定权重进行线性加权</p><script type="math/tex; mode=display">\begin{array}{r}& \min _{x} \sum_{k=1}^{K} \lambda_{k} f_{k}(x) \\\text { s.t. } & g_{i}(x) \geq 0,\quad i \in[1, M] \\& h_{j}(x)=0,\quad j \in[1, L]\end{array}</script><p><img src="/image/多目标优化笔记/weight.webp" style="zoom:67%;" /></p><p>于是就变成了单目标优化问题，上述问题存在有效解的条件，对于给定的 $\lambda \in \Lambda^{++}$ ，则上述问题的最优解是 MOO 问题的有效解，其中</p><script type="math/tex; mode=display">\Lambda^{++}=\left\{\lambda \mid \lambda_{k}>0, k=1,2 \ldots K, \sum_{k=1}^{K} \lambda_{k}=1\right\}</script><ul><li>优点：实现简单，有成熟的算法求解</li><li>缺点：$\lambda_k$ 难以确定，求出的解的优劣无法确定</li></ul><h3 id="主要目标法"><a href="#主要目标法" class="headerlink" title="主要目标法"></a>主要目标法</h3><p>也称 $\epsilon$-约束方法</p><script type="math/tex; mode=display">\begin{array}{l}& \min _{x} & f_{p}(x) \\\text { s.t. } & f_{k}(x) &\leq \epsilon_{k} &,k=1, \ldots, K, k \neq p \\& g_{i}(x) &\geq 0, i \in[1, M] \\& h_{j}(x) &= 0, j \in[1, L]\end{array}</script><p><img src="/image/多目标优化笔记/constraint.webp" style="zoom: 67%;" /></p><p>$\epsilon$-约束方法从 $K$ 个目标中选择最重要的子目标作为优化目标,其余的子目标作为约束条件。每个子目标,通过上界 $\epsilon_{K}$ 来约束</p><h4 id="主要目标法最优解和-MOO-解集的关系"><a href="#主要目标法最优解和-MOO-解集的关系" class="headerlink" title="主要目标法最优解和 MOO 解集的关系"></a>主要目标法最优解和 MOO 解集的关系</h4><ul><li><p>主要目标法最优解是 MOO 解的弱有效解</p></li><li><p>若主要目标 $f_p (x)$ 是严格凸函数，可行域为 $\hat{D}$ 的凸集，则主要目标法最优解是 MOO 解的有效解</p></li></ul><h4 id="界限值-epsilon-的选取"><a href="#界限值-epsilon-的选取" class="headerlink" title="界限值 $\epsilon$ 的选取"></a>界限值 $\epsilon$ 的选取</h4><p>可以取子目标函数的上限值</p><script type="math/tex; mode=display">\min \left\{f_{k} \mid f_{k}(x), k=1, \ldots, K, k \neq p\right\} \leq \epsilon_{k}</script><p>这种取法可以使得某些 $f_k(x)$ 留在可行域 $\hat{D}$ 内,并且 $\hat{D}$ 内有较多的点靠近 $f_k (x)$ 的最优解</p><ul><li>优点：简单，能应用到凸函数和非凸函数场景下</li><li>缺点：$\epsilon_k$ 如果取值不合适，可行域 $\hat{D}$ 可能为空值</li></ul><h3 id="逼近目标法"><a href="#逼近目标法" class="headerlink" title="逼近目标法"></a>逼近目标法</h3><p>提出一个目标值 $f^0 = (f_1^0,f_2^0,\cdots,f_k^0)$，使得每个目标函数 $f_k(x)$ 都逼近对应的目标值</p><script type="math/tex; mode=display">\begin{array}{l}L = (f(x), f^0) & = ||f(x) - f^0 ||^{\lambda}_{2} \\&= \sum_{k=1}^{K} \lambda_k(f_k(x)-f^0)^2,\lambda \in \Lambda^{++}\end{array}</script><p>和机器学习中的损失函数类似，是一个单目标优化问题，可以通过经典的方法进行求解，这里求解的最优解和有效解及弱有效解没有直接的联系，反映了决策者希望的目标值</p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>这是一种直接优化的方法，而上面提到的算法都是采取先验的知识将多目标优化转化成单目标优化</p><h4 id="最速梯度下降"><a href="#最速梯度下降" class="headerlink" title="最速梯度下降"></a>最速梯度下降</h4><p>简单起见，将讨论问题限制在无约束的单目标优化问题，并要求无约束的单目标优化问题中的 $f (x)$ 具有一阶连续偏导数，对于这类问题，能够从某一点出发，选择目标函数 $f (x) $ 下降最快的方向进行搜索，尽快达到最小值，问题是如何选择下降最快的方向</p><script type="math/tex; mode=display">DF(x;d) = \nabla f(x)^T d</script><p>求 $f(x)$ 在点 $x$ 处的下降最快的方向导数，归结为求如下最优化问题</p><script type="math/tex; mode=display">\min \nabla f(x)^T d \\\text{s.t.} \quad ||d|| \leq 1</script><p>其中 $||\cdot||$ 为欧式距离，上述问题的解为</p><script type="math/tex; mode=display">d = -\frac{\nabla f(x)}{||\nabla f(x)||}</script><p>负梯度方向为最速下降方向，最速下降法的迭代公式为</p><script type="math/tex; mode=display">x^{t+1} = x^t + \lambda_k d^{(k)}</script><p>其中，$\lambda _k$ 可以通过一维搜索来得到</p><h4 id="多目标梯度下降算法"><a href="#多目标梯度下降算法" class="headerlink" title="多目标梯度下降算法"></a>多目标梯度下降算法</h4><p>设当前为 $t+1$ 轮迭代，梯度迭代公式</p><script type="math/tex; mode=display">x^{t+1} = x^t + \lambda \cdot d^{t}</script><p>多目标优化的方向导数</p><script type="math/tex; mode=display">\nabla f_k(x)^T d,k = 1,2,\cdots,K</script><p>定义最大方向导数</p><script type="math/tex; mode=display">M_x(d^t) = max\{ \nabla f_k(x)^T d^t \mid k = 1,2,\cdots,K\}</script><p>多目标问题的最速梯度下降方向，可以归结为求解以下问题</p><script type="math/tex; mode=display">\begin{array}{l}\min & M_x(d^t) + \frac{1}{2} \| d^t \| ^2 \\\text{s.t.} & d^t \in R\end{array}</script><p>上述优化问题是闭且强凸优化问题，一定存在最优解，令 $M_x (d^t) = \alpha$，可以将一阶偏导项消去</p><script type="math/tex; mode=display">\begin{array}{l}\min & \alpha + \frac{1}{2} \|d^t \|^2 \\\text{s.t.} & \nabla f_k(x)^T d^t \leq \alpha ,k = 1,2,\cdots,K \\& d^t \in R\end{array}</script><p>上述问题为带线性不等式约束的凸二次规划问题</p><p>令 $d^\ast$，$a^\ast$ 为上述优化问题的最优解，得到</p><ul><li>若 $x^\ast$ 为 Pareto 最优，则 $d^\ast=0,a^\ast=0$ </li><li>若 $x^\ast$ 不为 Pareto 最优，则 $a^\ast&lt;0$</li></ul><p>且</p><script type="math/tex; mode=display">\alpha  \leq-\frac{1}{2} \|\left. d^{t}\right|^{2}<0 \\\nabla f_{k}(x)^{T} d^{t}  \leq \alpha, k=1, \cdots, K</script><p>因此</p><ul><li>如果 $d^\ast = 0$，则说明此时不存在下降方向,使得所有的目标都下降</li><li>如果 $d^\ast \neq 0$，则有 $\nabla f_{k}(x)^{T} d^{t} &lt; 0$，则 $d^t$ 是一个有效的多目标搜索方向，按如下公式更新，即可以使目标函数下降</li></ul><script type="math/tex; mode=display">\begin{array}{l}x^{(t+1)}&=& x^{t}+\lambda \cdot d^{t} \\f_{k}\left(x^{(t+1)}\right) &\leq& f_{k}\left(x^{t}\right), k=1, \ldots, K\end{array}</script><h2 id="多任务学习（MTL）"><a href="#多任务学习（MTL）" class="headerlink" title="多任务学习（MTL）"></a>多任务学习（MTL）</h2><p>多任务学习（MTL）- Multi-Task Learning</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在同一时间学习多个任务，求得最优解</p><p>设有 $N$ 个样本点 ${ x,y_i^1,y_i^2,\cdots, y_i^T},i \in N$，其中 $T$ 为任务数量，$y_i^t$ 是第 $t^{th}$ 个任务，第 $i^{th}$ 个样本点标签，定义为</p><script type="math/tex; mode=display">f^t(x;\theta^{sh},\theta^t):X \rightarrow Y^t</script><p>其中 $\theta^{sh}$ 为多个任务的共享参数，$\theta^t$ 为单个任务的独有参数</p><p>$Loss$ 为</p><script type="math/tex; mode=display">L^t(\_,\_) = Y^t \times Y^t \rightarrow R^+</script><script type="math/tex; mode=display">\min_{\theta^{sh} ,\theta} = \sum_{t=1}^{T} c^t \hat{L}^{t}(\theta^{sh},\theta)</script><p>$c^t$ 为每个具体任务的权重，每个具体任务 $t$ 的 $Loss$ 为</p><script type="math/tex; mode=display">\hat{L}^t(\theta^{sh},\theta) \triangleq \frac{1}{N} \sum_t L(f^t(x;\theta^{sh}, \theta^t), y^t_i)</script><h3 id="多任务学习转化为多目标优化"><a href="#多任务学习转化为多目标优化" class="headerlink" title="多任务学习转化为多目标优化"></a>多任务学习转化为多目标优化</h3><p>将多任务学习转换为 MOO 问题求解，定义</p><script type="math/tex; mode=display">\min _{\theta^{s h}, \theta} L\left(\theta^{s h}, \theta^{1}, \ldots, \theta^{T}\right)=\min _{\theta^{s h}, \theta}\left(\hat{L}^{1}\left(\theta^{s h}, \theta^{1}\right), \cdots, \hat{L}^{T}\left(\theta^{s h}, \theta^{T}\right)\right)</script><p>多目标优化的目的是求得 Pareto 最优解，多目标优化的 Pareto 最优解定义</p><p>一个解 $\theta$ 支配另一个解 $\bar{\theta}$ ，如果</p><script type="math/tex; mode=display">\hat{L}(\theta^{sh},\theta^t) \leq \hat{L}(\bar{\theta}^{sh},\bar{\theta}^t)</script><p>对于所有的任务 $t$ 都成立，且</p><script type="math/tex; mode=display">L(\theta^{sh},\theta^1,\theta^2,\cdots,\theta^{T}) \neq L(\bar{\theta}^{sh},\bar{\theta}^1,\bar{\theta}^2,\cdots, \bar{\theta}^T)</script><p>一个解 $\theta^{\ast}$ 称为 Pareto 最优解的集合称为 Pareto 最优解集，其图像称为 Pareto 前沿（Pareto Front）</p><h2 id="多任务求解：单个-Pareto-解"><a href="#多任务求解：单个-Pareto-解" class="headerlink" title="多任务求解：单个 Pareto 解"></a>多任务求解：单个 Pareto 解</h2><h3 id="问题转化"><a href="#问题转化" class="headerlink" title="问题转化"></a>问题转化</h3><p>单个 Pareto 解使用了多重梯度下降法，由多目标优化的 $KKT$ 条件，得</p><p>存在 $\alpha^1,\alpha^2,\cdots,\alpha^T \geq 0$，使得</p><script type="math/tex; mode=display">\begin{array}{r}\sum_{t=1}^{T} \alpha^T = 1 \\\sum_{t=1}^{T} \alpha^t \nabla_{\theta^{sh}} \hat{L}(\theta^{sh},\theta^{t}) = 0\end{array}</script><p>对应所有的任务 $t$</p><script type="math/tex; mode=display">\nabla_{\theta^t}\hat{L}(\theta^{sh},\theta^t) = 0</script><p>满足上式的解称为 Pareto 平衡点（Pareto Stationary Point），Pareto 最优点都是 Pareto 平稳点，反之不一定成立，考虑如下的优化问题</p><script type="math/tex; mode=display">\begin{gathered}\min _{\alpha^{1}, \cdots ,\alpha^{T}}\left\|\sum_{t=1}^{T} \alpha^{t} \nabla_{\theta s h} \hat{L}^{t}\left(\theta^{s h}, \theta^{t}\right)\right\| \\\sum_{t=1}^{T} \alpha^{t}=1, \alpha^{t} \geq 0, \forall t\end{gathered}</script><p>上述优化问题的解存在两种情况</p><ul><li>最优值 $=0$，则对应的解满足 $KKT$ 条件</li><li>最优值 $\neq 0$，则对应的解给出了下降方向，使得多任务目标函数提升（函数值下降）上述优化问题等价于在输入点集凸包中找到最小模点</li></ul><h3 id="两个任务的情形"><a href="#两个任务的情形" class="headerlink" title="两个任务的情形"></a>两个任务的情形</h3><p>由多目标优化的 $KKT$ 条件，得</p><script type="math/tex; mode=display">\begin{gathered}\min _{\alpha^{1}, \ldots \alpha^{T}}\|\gamma \theta+(1-\gamma) \bar{\theta}\| \\\gamma+(1-\gamma)=1, \gamma \geq 0\end{gathered}</script><p>其中 $\theta,\bar{\theta}$ 定义为</p><script type="math/tex; mode=display">\begin{aligned}&\theta \triangleq \nabla_{\theta s h} \hat{L}^{1}\left(\theta^{s h}, \theta^{1}\right) \\&\bar{\theta} \triangleq \nabla_{\theta s h} \hat{L}^{2}\left(\theta^{s h}, \theta^{2}\right)\end{aligned}</script><p>其解的情况枚举如下</p><ul><li>当 $\theta^T \bar{\theta} \geq \theta^T \theta,\gamma = 1$</li><li>当 $\theta^T \bar{\theta} \geq \bar{\theta}^T \bar{\theta},\gamma = 0$</li><li>$\text{otherwise}$</li></ul><script type="math/tex; mode=display">\gamma=\frac{(\bar{\theta}-\theta)^{T} \bar{\theta}}{\| \bar{\theta}-\theta) \|_{2}^{2}}p7</script><p>几何解释</p><p><img src="/image/多目标优化笔记/interpret.webp" style="zoom:66%;" /></p><p><img src="/image/多目标优化笔记/algo1.webp" style="zoom:60%;" /></p><p>基于 Frank-wolfe 算法，得求解 MTL 任务算法</p><p><img src="/image/多目标优化笔记/algo2.webp" style="zoom: 66%;" /></p><p><img src="/image/多目标优化笔记/frankwolfesolver.webp" style="zoom:62%;" /></p><h2 id="多任务求解：多个-Pareto-解"><a href="#多任务求解：多个-Pareto-解" class="headerlink" title="多任务求解：多个 Pareto 解"></a>多任务求解：多个 Pareto 解</h2><p>上一节介绍的方法只能求得一个 pareto 解，有时需要多个 pareto 解才能做出更好的决策</p><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>将多任务学习分解为多个带约束的多目标子问题，通过对子问题进行并行求解</p><p>原始多任务学习定义</p><script type="math/tex; mode=display">\min_{\theta} L(\theta) = (L_1(\theta),L_2(\theta),\cdots,L_i(\theta),\cdots,L_m(\theta))</script><p>$L_i(\theta)$ 是第 $i$ 个任务的损失函数</p><p>下图所示</p><p><img src="/image/多目标优化笔记/pareto1.webp" style="zoom:60%;" /></p><p><img src="/image/多目标优化笔记/pareto2.webp" style="zoom:60%;" /></p><p>用一组分布良好的 Preference Vectors（PV）将多任务学习的目标空间分解为 $K$ 个子区域</p><script type="math/tex; mode=display">PV = \{u_1,u_2,\cdots,u_k,\cdots,u_K\},u_k \in R_+^m</script><p>重新定义多任务学习</p><script type="math/tex; mode=display">\min_{\theta} L(\theta) = (L_1(\theta),L_2(\theta),L_m(\theta)) \\s.t. \quad L(\theta) \in \Omega_k,k = 1,\cdots,K</script><p>$\Omega_k$ 是目标空间的子区域</p><script type="math/tex; mode=display">\Omega_k = \{v \in R_+^m | u_j^T v \leq u_k^Tv,\forall j = 1,\cdots,K \}</script><p>$\Omega_k$ 中的元素 $v$</p><script type="math/tex; mode=display">v \in \Omega_k \Leftrightarrow u_k^T v = \| u_k \| \cdot \| v \| \cos{\alpha}</script><p>$u_k^T v$ 为最大的内积</p><p>重新定义多任务学习</p><script type="math/tex; mode=display">\begin{array}{l}& \min_\theta L(\theta) = (L_1(\theta),L_2(\theta),L_m(\theta)) \\\text{s.t.} & G_j(\theta_t) = (u_j - u_k)^T \\& L(\theta_t) \leq 0 \\& j=1,\cdots,K\end{array}</script><p>这样得到的解集将会分布在不同的子区域 $\Omega_k$</p><h3 id="子问题的梯度下降方法"><a href="#子问题的梯度下降方法" class="headerlink" title="子问题的梯度下降方法"></a>子问题的梯度下降方法</h3><h4 id="寻找初始解-theta-r"><a href="#寻找初始解-theta-r" class="headerlink" title="寻找初始解 $\theta_r$"></a>寻找初始解 $\theta_r$</h4><p>求解多任务学习，需要找到一个满足约束的基本可行解，对于随机产生的可行解 $\theta_r$ ，一种最直接的方法就是找到初始可行解 $\theta_0$ </p><script type="math/tex; mode=display">\begin{array}{l}& \min_{\theta_0} \| \theta_0 - \theta_r \|^2 \\ \text{s.t.} & L(\theta_0) \in \Omega_k\end{array}</script><p>上述问题投影方法求解的效率不高，特别是对于大规模的深度神经网络，改写为无约束优化问题，使用序列梯度方法找到初始解 $\theta_0$</p><p>定义活跃限制集合（activated constraints）</p><script type="math/tex; mode=display">I(\theta_r) = \{ j|G_j(\theta_r) \geq 0,j=1,\cdots,K \}</script><p>令 $I(\theta_r)$ 中所有的活跃限制函数值下降的方向</p><script type="math/tex; mode=display">(d_r,\alpha_r) = \text{argmin}_{d \in R^n,\alpha \in R} \quad \alpha + \frac{1}{2} \|d\|^2 \\\text{s.t.} \quad \nabla G_j (\theta_r)^T d \leq \alpha,j\in I(\theta_r)</script><p>得到更新公式</p><script type="math/tex; mode=display">\theta_{r_{t+1}} = \theta_{r_t} + \eta_r d_{r_t}</script><p>上述方法能将活跃集内的约束目标值减少，使得越来越多的约束目标小于 0，$I(\theta<em>{r})$ 最后变为空集，则 $\theta</em>{r}$ 是可行解</p><h4 id="求解子问题"><a href="#求解子问题" class="headerlink" title="求解子问题"></a>求解子问题</h4><p>受限 pareto 最优：$\theta^{\ast}$ 是多任务 $L(\theta)$ 在子区域 $\Omega_k$ 的最优解，如果 $\theta^{\ast} \in \Omega_k$ 且不存在 $\hat{\theta} \in  \Omega_k$，使得 $\hat{\theta} &lt; \theta^{\ast}$</p><p>考虑如下多目标优化问题</p><script type="math/tex; mode=display">\begin{array}{l}& (d_t,\alpha_t)  = \text{argmin}_{d\in R^n,\alpha \in R} \quad \alpha + \frac{1}{2}\|d\|^2 \\\text{s.t.} & \nabla L_i(\theta_t)^Td \leq \alpha,i=1,\cdots,m\\& \nabla G_j(\theta_t)^Td \leq \alpha,j \in I_{\in}(\theta_t)\end{array}</script><p> 其中 $I_{\in}(\theta_t)$ 定义</p><script type="math/tex; mode=display">I_{\in} (\theta_t) = \{j \in I | G_j (\theta) \geq - \epsilon \}</script><p>令 $(d^k,a^k)$ 是多任务学习问题的解，则</p><ul><li><p>如果 $\theta_t$ 是严格受限于 $\Omega_k$，则 $d_t = 0 \in R^n$ 且 $\alpha_t = 0$</p></li><li><p>如果 $\theta_t$ 不是严格受限于 $\Omega_k$，则</p><script type="math/tex; mode=display">\begin{array}{l}\alpha_{t} \leq-\frac{1}{2}\left\|d_{t}\right\|^{2}<0 \\\nabla L_{i}\left(\theta_{t}\right)^{T} d \leq \alpha,& i=1, \ldots, m \\\nabla G_{j}\left(\theta_{t}\right)^{T} d \leq \alpha,& j \in I_{\epsilon}\left(\theta_{t}\right)\end{array}</script><p>迭代公式</p><script type="math/tex; mode=display">\theta_{t+1} = \theta_t + \eta d_t</script></li></ul><p>通过求解上述问题，能够获得一个有效的搜索方向</p><h4 id="大规模求解方法"><a href="#大规模求解方法" class="headerlink" title="大规模求解方法"></a>大规模求解方法</h4><p>上述方法能够获得一个有效的搜索方向，对于大规模的问题，会比较困难，这里将问题重写，将其表示为对偶形式</p><ul><li><p>$KKT$ 条件</p><script type="math/tex; mode=display">\begin{array}{l}& d_{t} =-\sum_{i=1}^{m} \lambda_{i} \nabla L_{i}\left(\theta_{t}\right)+\sum_{j \in I_{\epsilon}(\theta)} \beta_{i} \nabla G_{j}\left(\theta_{t}\right) \\\text{s.t.} & \sum_{i=1}^{m} \lambda_{i}+\sum_{j \in I_{\epsilon}(\theta)} \beta_{j}=1\end{array}</script></li><li><p>对偶问题</p><script type="math/tex; mode=display">\begin{array}{l}& \max_{\lambda_{i}, \beta_{j}}-\frac{1}{2}\left\|\sum_{i=1}^{m} \lambda_{i} \nabla L_{i}\left(\theta_{t}\right)+\sum_{j \in I_{\epsilon}(\theta)} \beta_{i} \nabla G_{j}\left(\theta_{t}\right)\right\|^{2} \\\text { s.t. } & \sum_{i=1}^{m} \lambda_{i}+\sum_{j \in I_{\epsilon}(\theta)} \beta_{j}=1 \\ & \lambda_{i} \geq 0, \beta_{j} \geq 0 \\& \forall i=1, \cdots, m\\& \forall j \in I_{\epsilon}(\theta)\end{array}</script></li></ul><p>将多任务学习转化为其对偶问题后，求解空间不再是参数空间，而是变成了任务个数和受限条件数，使得求解问题极大的减少了</p><p>Pareto MTL 算法如下图</p><p><img src="/image/多目标优化笔记/pareto_MTL.webp" style="zoom: 50%;" /></p><p>为表述方便，这里引用论文中关于多任务学习的定义</p><p>设 $f(x)$ 表面光滑</p><script type="math/tex; mode=display">f(x):\mathcal{R}^n \rightarrow \mathcal{R}^m \\f_i(x):\mathcal{R}^n \rightarrow \mathcal{R},i=1,\cdots,m</script><h3 id="准备：-Krylov-子空间"><a href="#准备：-Krylov-子空间" class="headerlink" title="准备：$Krylov$ 子空间"></a>准备：$Krylov$ 子空间</h3><p>大规模稀疏线性方程组 $AX=b$ 求解的首先方法是 $krylov$ 子空间方法，基本思想是在一个较小的子空间 $\mathcal{K} \subset R_n$ 中寻找近似解</p><p>定义：设 $A \in R^{n \times n},r \in R^n$，则</p><script type="math/tex; mode=display">\mathcal{K}_{m}(A, r) \triangleq \text{span}\{r, Ar, \cdots, A^{m-1} r\} \subseteq R_{n}</script><p>是由 $A$ 和 $r$ 生成的 $Krylov$ 子空间，通常简记为 $\mathcal{K}_m$，$Krylov$ 子空间有如下的三个性质</p><ul><li>$Krylov$ 子空间嵌套性：$\mathcal{K}<em>{1} \subseteq \mathcal{K}</em>{2} \subseteq \cdots \subseteq \mathcal{K}_{m}$</li><li>$\mathcal{K}_m$ 的维数不超过 $m$</li><li>$\mathcal{K}_{m}(A, r)={x=p(A)}$，$r$ 为次数小于 $m$ 的多项式</li></ul><p>求解 $Krylov$ 子空间的解来近似原始线性方程组的解</p><p><img src="/image/多目标优化笔记/krylov.webp" style="zoom:50%;" /></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>Pareto 平稳点（Pareto Stationary）：设 $f_i(x)$ 连续可身微，点 $x$ 称为 Pareto 平稳点，如果存在 $\alpha \in \mathcal{R}^{m},a_i \geq 0$ 使得下式成立</p><script type="math/tex; mode=display">\sum_{i=1}^{m} \alpha_i \nabla f_i(x) = 0 \\\sum_{i=1}^{m} \alpha_i = 1</script></li><li><p>Pareto 点都是 Pareto 平稳点</p></li><li><p>设 $f(x)$ 是光滑且 $x^{\ast}$ 是 Pareto 点，$x(t)$ 是过点 $x^{\ast}$ 的曲线</p><script type="math/tex; mode=display">x(t):t\in (-\epsilon,\epsilon) \rightarrow \mathcal{R}^n \\x(0) = x^{\ast}</script><p>则存在 $\beta \in \mathcal{R}^m$ 使得</p><script type="math/tex; mode=display">H(x^{\ast})x'(t) = \nabla f(x^{\ast})^T \beta \\H(x^{\ast}) = \sum_{i=1}^{m} \alpha_i \nabla^2 f_i(x^{\ast})</script><p>$x’(t)$ 为切线</p></li></ul><p>上式表明,算子 $H(x^{\ast})$ 将点 $x$ 处的切向量 $v = x’(t)$ 变换为由 $∇f_i(x^{\ast})$ 扩张成的 $Krylov$ 子空间的向量</p><p><img src="/image/多目标优化笔记/krylov_expand.webp" style="zoom:70%;" /></p><h3 id="离散-Pareto-求解"><a href="#离散-Pareto-求解" class="headerlink" title="离散 Pareto 求解"></a>离散 Pareto 求解</h3><p>给定初始点 $x_0 \in \mathcal{R}^n,f_i(x)$ 光滑，可以从如下三步来获取连续 Pareto 解</p><ul><li>求解 Pareto 平稳点: 从初始点 $x_0$ 出发，通过梯度下降的方法求解 Pareto 平稳点 $x_0^{\ast}$</li><li>扩展 Pareto 平稳点，$f(x)$ 在点 $x_0^{\ast}$ 处光滑，如果 Pareto 前沿存在，则在点 $x_0^{\ast}$ 处的某个领域内存在 Pareto 平稳点，由此出发，可以求得一系列的 Pareto 平稳点 $x_i^{\ast}$</li><li>将已知的平稳点所在的局部 Pareto 前沿进行连接合并，扩充成更大的连续Pareto 前沿</li></ul><p><img src="/image/多目标优化笔记/pareto_front.webp" style="zoom:50%;" /></p><p>接下来讨论获取 Pareto 平稳点的方法</p><h3 id="梯度求解法"><a href="#梯度求解法" class="headerlink" title="梯度求解法"></a>梯度求解法</h3><p>见上述</p><h3 id="一阶梯度求解法扩张"><a href="#一阶梯度求解法扩张" class="headerlink" title="一阶梯度求解法扩张"></a>一阶梯度求解法扩张</h3><p>通过梯度求解法求解出 Pareto 平稳点 $x_0^{\ast}$ 后，基于该点扩展出局部 Pareto 集 ${x_i }$，这一过程分解为两步</p><ul><li>计算权重 $\alpha$</li><li>求解搜索方向 $v$，估计梯度迭代的搜索方向 $v_i$</li></ul><p>通过如下更新公式求解</p><script type="math/tex; mode=display">x_i = x_0^{\ast} + sv_i</script><p>计算 $\alpha$ 可以归结为求解如下的约束问题</p><script type="math/tex; mode=display">\min_\alpha \| \sum_{i=1}^m \alpha_i \nabla f_i(x_0^{\ast}) \|^2 \\\text{s.t.} \quad \alpha_i \geq 0,\sum_{i=1}^m \alpha_i = 1</script><p>上述问题规模为 $m$， 量级较小，可以很方便的求解出来</p><p>再由基本概念，得求解的线性方程组</p><script type="math/tex; mode=display">H(x_0^{\ast})v = \nabla f(x_0^{\ast})^T \beta</script><p>上述问题求解有两个难点</p><ul><li>$x_0^{\ast}$ 不一定为 Pareto 平稳点</li><li>当 $n$ 非常大时，求解起来非常困难</li></ul><p>为此引入校正向量 $c$（correction vector），约束问题改写为</p><script type="math/tex; mode=display">\begin{array}{l}&\min_{a,c} \| c \|^2 \\\text{s.t.} & \alpha_i \geq 0 \\& \sum_{i=1}^m \alpha_i = 1 \\& \sum_{i=1}^{m} \alpha_i(\nabla f_i(x_0^{\ast})-c)=0\end{array}</script><p>用 $\nabla f_i(x_0^{\ast})-c$ 近似 $\nabla f_i(x_0^{\ast})$，$x_0^{\ast}$ 将会是 Pareto 平稳点</p><p>设 $a^{\ast}$ 是未引入校正向量约束问题的解，则引入 $c$ 后的解为</p><script type="math/tex; mode=display">(a,c) = (a^{\ast},\nabla f(x_0^{\ast})^T a^{\ast})</script><p>在计算出 $a^{\ast},x_0^{\ast},c$ 后，可以计算出 $\nabla f(x_0^{\ast})$，考虑如下稀疏线性方程组</p><script type="math/tex; mode=display">H(x_0^{\ast})v = (\nabla f(x_0^{\ast})^T - c^T) \beta</script><p>$\beta$ 为随机生成的向量，$v$ 为待求解的变量。上述式可以通过 $krylov$ 子空间，$MINERS$ 方法求解</p><p>寻找离散 Pareto 解集合的求解算法</p><ul><li>Input：随机初始化网络</li><li>ParetoExpand($x^{\ast}$) 生成点 $x^{\ast}$ 的 $K$ 个搜索方向 $v_i$</li><li>由 $K$ 个搜索方向扩展出 $K$ 个子网络</li><li>更新子网络节点 $x_i = x^{\ast} + sv_i$</li><li>ParetoExpand($x_i$) 输出 Pareto 平稳点 $x_i^{\ast}$</li><li>Output：$N$ 个 Pareto 平稳网络</li></ul><p><img src="/image/多目标优化笔记/efficient_Pareto.webp" style="zoom: 60%;" /></p><h3 id="连续-Pareto-解（Pareto-front）构建"><a href="#连续-Pareto-解（Pareto-front）构建" class="headerlink" title="连续 Pareto 解（Pareto front）构建"></a>连续 Pareto 解（Pareto front）构建</h3><p>通过前面求解出来 $N$ 个 Pareto 平稳网络（父节点及 $K$ 个子网络），由离散 Pareto 点合并成更大的连续 Pareto 前沿</p><p>给定 $x<em>i^{\ast}$ 及基其对应的 $K$ 个节点 ${ {x_i^{\ast}}_1,\cdots,{x_i^{\ast}}_k }$，定义连续变量 $r</em>{i \rightarrow {i}_j} \in [0,1]$ 以及搜索方向</p><script type="math/tex; mode=display">v_{i \rightarrow {i}_j} = {x_i^{\ast}}_j - x_i^{\ast},j = 1,2,\cdots,K</script><p>$x_i^{\ast}$ 处局部 Pareto 集可以通过下式进行构建</p><script type="math/tex; mode=display">S(x_i^{\ast}) = \{x_i^{\ast} + \sum_{i=1}^K r_{i \rightarrow {i}_j} u_{i \rightarrow {i}_j} | r_{i \rightarrow {i}_j} \geq 0,\sum_{i=1}^K r_{i \rightarrow {i}_j} \leq 1 \}</script><p>$S(x_i^{\ast})$ 是点 $x_i^{\ast}$ 及对应的 $K$ 个子节点 ${ {x_i^{\ast}}_1,\cdots,{x_i^{\ast}}_K }$ 构成的凸包，切平面中切向量的线性组合仍然在切平面</p><p>对于 $N$ 个局部 Pareto 集</p><script type="math/tex; mode=display">\{ S(x_1^{\ast}),\cdots,S(x_N^{\ast}) \}</script><p>可以将两两接壤处合并成一个更大的局部 Pareto 集合，全部合并完后，可以生成多个的连续 Pareto 前沿</p><p><img src="/image/多目标优化笔记/pareto_continuous.webp" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 数学建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多目标优化 </tag>
            
            <tag> 最优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用平行线段的相机标定[计划更新]</title>
      <link href="/2022/04/16/%E4%BD%BF%E7%94%A8%E5%B9%B3%E8%A1%8C%E7%BA%BF%E6%AE%B5%E7%9A%84%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/"/>
      <url>/2022/04/16/%E4%BD%BF%E7%94%A8%E5%B9%B3%E8%A1%8C%E7%BA%BF%E6%AE%B5%E7%9A%84%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>一种使用平行线段来进行相机标定以及形状重建的方法</p><span id="more"></span><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>监控摄像机标定一直是计算机视觉工业应用的一个重要课题。内部和外部参数都必须精确校准，才能从记录的 2D 图像或视频中识别出 3D 场景中发生的事情，相机标定的一般方法是使用标定对象，如棋盘标定。然而，这些方法对于监控摄像机来说往往是不切实际的，因为它们需要一个足够大的校准对象来覆盖整个拍摄区域。</p><blockquote><p> 原论文下载：</p></blockquote><h2 id="直线提取"><a href="#直线提取" class="headerlink" title="直线提取"></a>直线提取</h2><h3 id="Canny"><a href="#Canny" class="headerlink" title="Canny"></a>Canny</h3><h3 id="Hough-Transform"><a href="#Hough-Transform" class="headerlink" title="Hough Transform"></a>Hough Transform</h3><h2 id="形状重建"><a href="#形状重建" class="headerlink" title="形状重建"></a>形状重建</h2><h3 id="Affine-Rectification"><a href="#Affine-Rectification" class="headerlink" title="Affine Rectification"></a>Affine Rectification</h3><h3 id="Euclidean-Rectification"><a href="#Euclidean-Rectification" class="headerlink" title="Euclidean Rectification"></a>Euclidean Rectification</h3><h2 id="度量指标属性"><a href="#度量指标属性" class="headerlink" title="度量指标属性"></a>度量指标属性</h2><h2 id="相机标定"><a href="#相机标定" class="headerlink" title="相机标定"></a>相机标定</h2><h2 id="世界坐标定位"><a href="#世界坐标定位" class="headerlink" title="世界坐标定位"></a>世界坐标定位</h2><h2 id="3D-形状重构"><a href="#3D-形状重构" class="headerlink" title="3D 形状重构"></a>3D 形状重构</h2><h2 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h2><h3 id="Fitting-Vanishing-Point"><a href="#Fitting-Vanishing-Point" class="headerlink" title="Fitting Vanishing Point"></a>Fitting Vanishing Point</h3><h3 id="Fitting-Line-at-Infinity"><a href="#Fitting-Line-at-Infinity" class="headerlink" title="Fitting Line at Infinity"></a>Fitting Line at Infinity</h3><h3 id="Fitting-Conics"><a href="#Fitting-Conics" class="headerlink" title="Fitting Conics"></a>Fitting Conics</h3><h2 id="总结分析"><a href="#总结分析" class="headerlink" title="总结分析"></a>总结分析</h2><h2 id="计算结果"><a href="#计算结果" class="headerlink" title="计算结果"></a>计算结果</h2>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文复现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>设计实现内存的分配和回收算法</title>
      <link href="/2022/04/13/%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%E5%86%85%E5%AD%98%E7%9A%84%E5%88%86%E9%85%8D%E5%92%8C%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/"/>
      <url>/2022/04/13/%E8%AE%BE%E8%AE%A1%E5%AE%9E%E7%8E%B0%E5%86%85%E5%AD%98%E7%9A%84%E5%88%86%E9%85%8D%E5%92%8C%E5%9B%9E%E6%94%B6%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>操作系统作业</p><span id="more"></span><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>如下数据结构下，设计实现内存的分配和回收算法：</p><ul><li>归还区有下邻空闲区；</li><li>归还区有上邻空闲区；</li><li>归还区既有上邻空闲区又有下邻空闲区；</li><li>归还区既无上邻空闲区又无下邻空闲区</li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">struct</span><br><span class="line">&#123; float address;  /*已分分区起始地址*/</span><br><span class="line"> float length;  /*已分分区长度，单位为字节*/</span><br><span class="line"> int flag;   /*已分配区表登记栏标志，用“0”表示空栏目，实验中只支持一个字符的作业名*/</span><br><span class="line">&#125;used_table[n]; /*已分配区表*/</span><br><span class="line">struct</span><br><span class="line">&#123;float address;/*空闲区起始地址*/</span><br><span class="line"> float length;    /*空闲区长度，单位为字节*/</span><br><span class="line"> int flag;  /*空闲区表登记栏标志，用“0”表示空栏目，用“1”表示未分配*/</span><br><span class="line">&#125;free_table[m]; /*空闲区表*/</span><br></pre></td></tr></table></figure><p>编写程序，并输出结果。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">//区表数组结构</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">UsedTable</span> &#123;</span></span><br><span class="line">    <span class="type">int</span> address;  <span class="comment">/*已分分区起始地址*/</span></span><br><span class="line">    <span class="type">int</span> length;  <span class="comment">/*已分分区长度，单位为字节*/</span></span><br><span class="line">    <span class="type">int</span> flag;   <span class="comment">/*已分配区表登记栏标志，用“0”表示空栏目，实验中只支持一个编号的作业名*/</span></span><br><span class="line">&#125; usedTable[<span class="number">100</span>]; <span class="comment">/*已分配区表*/</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">FreeTable</span> &#123;</span></span><br><span class="line">    <span class="type">int</span> address;<span class="comment">/*空闲区起始地址*/</span></span><br><span class="line">    <span class="type">int</span> length;    <span class="comment">/*空闲区长度，单位为字节*/</span></span><br><span class="line">    <span class="type">int</span> flag;  <span class="comment">/*空闲区表登记栏标志，用“0”表示空栏目，用“1”表示未分配*/</span></span><br><span class="line">&#125; freeTable[<span class="number">100</span>]; <span class="comment">/*空闲区表*/</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">InitTable</span><span class="params">()</span>;                                                   <span class="comment">// 初始化区表</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">allocTable</span><span class="params">(<span class="type">const</span> <span class="type">int</span> proName, <span class="type">const</span> <span class="type">int</span> proLenth)</span>;             <span class="comment">// 创建作业</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">reclaimTable</span><span class="params">(<span class="type">const</span> <span class="type">int</span> proName)</span>;                               <span class="comment">// 回收作业</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">findProName</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> UsedTable temp[], <span class="type">const</span> <span class="type">int</span> proName)</span>;  <span class="comment">// 按作业名查找下标</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sortUsedTable</span><span class="params">(<span class="keyword">struct</span> UsedTable temp[])</span>;                        <span class="comment">// 排序已分配区表</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">printUsedTable</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> UsedTable temp)</span>;                   <span class="comment">// 输出作业分区信息</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">printFreeTable</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> FreeTable temp)</span>;                   <span class="comment">// 输出空闲分区信息</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> i=<span class="number">0</span>;                    <span class="comment">//记录当前 已分配/空闲 区表数量</span></span><br><span class="line"><span class="type">int</span> Minsize=<span class="number">1</span>;              <span class="comment">//作业间最小空余地址</span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">InitTable</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="comment">//OS</span></span><br><span class="line">    usedTable[<span class="number">0</span>].address = <span class="number">0</span>;</span><br><span class="line">    usedTable[<span class="number">0</span>].length = <span class="number">3</span>;</span><br><span class="line">    usedTable[<span class="number">0</span>].flag = <span class="number">1</span>; <span class="comment">//OS 的作业名为1</span></span><br><span class="line">    </span><br><span class="line">    freeTable[<span class="number">0</span>].address = <span class="number">3</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].length = <span class="number">97</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].flag = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//图形界面</span></span><br><span class="line">    freeTable[<span class="number">0</span>].address = <span class="number">8</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].length = <span class="number">92</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].flag = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    usedTable[<span class="number">1</span>].address = <span class="number">3</span>;</span><br><span class="line">    usedTable[<span class="number">1</span>].length = <span class="number">5</span>;</span><br><span class="line">    usedTable[<span class="number">1</span>].flag = <span class="number">2</span>; <span class="comment">//图形界面 的作业名为2</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">//网络程序</span></span><br><span class="line">    freeTable[<span class="number">0</span>].address = <span class="number">8</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].length = <span class="number">12</span>;</span><br><span class="line">    freeTable[<span class="number">0</span>].flag = <span class="number">1</span>;</span><br><span class="line">    </span><br><span class="line">    usedTable[<span class="number">2</span>].address = <span class="number">20</span>;</span><br><span class="line">    usedTable[<span class="number">2</span>].length = <span class="number">10</span>;</span><br><span class="line">    usedTable[<span class="number">2</span>].flag = <span class="number">3</span>; <span class="comment">//网络程序 的作业名为3</span></span><br><span class="line"></span><br><span class="line">    freeTable[<span class="number">1</span>].address = <span class="number">30</span>;</span><br><span class="line">    freeTable[<span class="number">1</span>].length = <span class="number">70</span>;</span><br><span class="line">    freeTable[<span class="number">1</span>].flag = <span class="number">1</span>;</span><br><span class="line">    i += <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">allocTable</span><span class="params">(<span class="type">const</span> <span class="type">int</span> proName, <span class="type">const</span> <span class="type">int</span> proLenth)</span> &#123;</span><br><span class="line">    <span class="type">int</span> j=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(freeTable[j].flag) &#123;</span><br><span class="line">        <span class="keyword">if</span>(freeTable[j].length &gt;= proLenth) <span class="keyword">break</span>;</span><br><span class="line">        j++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(freeTable[j].flag == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mNo free space available\033[0m\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(freeTable[j].length - proLenth &lt;= Minsize) &#123;</span><br><span class="line">        usedTable[i].address = freeTable[j].address;</span><br><span class="line">        usedTable[i].length  = freeTable[j].length;</span><br><span class="line">        usedTable[i].flag    = proName;</span><br><span class="line">        <span class="keyword">while</span>(freeTable[j].flag) &#123;</span><br><span class="line">            freeTable[j] = freeTable[j+<span class="number">1</span>];</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        freeTable[j].address = freeTable[j].length = freeTable[j].flag = <span class="number">0</span>;</span><br><span class="line">        i++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        usedTable[i].address = freeTable[j].address;</span><br><span class="line">        usedTable[i].length  = proLenth;</span><br><span class="line">        usedTable[i].flag    = proName;</span><br><span class="line">        freeTable[j].address = freeTable[j].address + proLenth;</span><br><span class="line">        freeTable[j].length  = freeTable[j].length  - proLenth;</span><br><span class="line">        i++;</span><br><span class="line">    &#125; <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">findProName</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> UsedTable temp[], <span class="type">const</span> <span class="type">int</span> proName)</span> &#123;</span><br><span class="line">    <span class="type">int</span> m=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(; m&lt;i; m++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(temp[m].flag == proName)</span><br><span class="line">            <span class="keyword">return</span> m;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">reclaimTable</span><span class="params">(<span class="type">const</span> <span class="type">int</span> proName)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span>(proName==<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mCannot reclaim OS progress!\n\033[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> k=<span class="number">0</span>,j=<span class="number">0</span>,h=<span class="number">0</span>;</span><br><span class="line">    sortUsedTable(usedTable);</span><br><span class="line">    <span class="keyword">if</span>(findProName(usedTable, <span class="number">1</span>) == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mERROR!\033[0m&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    k = findProName(usedTable, proName);</span><br><span class="line">    <span class="keyword">if</span>(k == <span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mCannot find this progress!\033[0m\n&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">while</span>(freeTable[j].flag || usedTable[h].flag) &#123;</span><br><span class="line">            <span class="keyword">if</span>(freeTable[j].address + freeTable[j].length == usedTable[k].address) &#123;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span>(freeTable[j+<span class="number">1</span>].flag &amp;&amp; usedTable[k].address + usedTable[k].length == freeTable[j+<span class="number">1</span>].address) &#123;</span><br><span class="line">                    freeTable[j].length += (usedTable[k].length + freeTable[j+<span class="number">1</span>].length);</span><br><span class="line">                    usedTable[k].address = usedTable[k].flag = usedTable[k].length = <span class="number">0</span>;</span><br><span class="line">                    <span class="keyword">for</span>(<span class="type">int</span> l=j+<span class="number">1</span>; freeTable[l].flag; l++) &#123;</span><br><span class="line">                        freeTable[l] = freeTable[l+<span class="number">1</span>];</span><br><span class="line">                    &#125; <span class="comment">//上有，下有</span></span><br><span class="line">                    i--;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    freeTable[j].length += usedTable[k].length;</span><br><span class="line">                    usedTable[k].address = usedTable[k].length = usedTable[k].flag = <span class="number">0</span>;</span><br><span class="line">                    i--;</span><br><span class="line">                    <span class="keyword">return</span>;</span><br><span class="line">                &#125; <span class="comment">//上有，下无</span></span><br><span class="line"></span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (usedTable[k].address + usedTable[k].length == freeTable[j].address) &#123;</span><br><span class="line">                freeTable[j].address = usedTable[k].address;</span><br><span class="line">                freeTable[j].length += usedTable[k].length;</span><br><span class="line">                usedTable[k].address = usedTable[k].flag = usedTable[k].length = <span class="number">0</span>;</span><br><span class="line">                <span class="comment">//上无，下有</span></span><br><span class="line">                i--;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">            &#125;  <span class="keyword">else</span> <span class="keyword">if</span> (usedTable[k<span class="number">-1</span>].address + usedTable[k<span class="number">-1</span>].length + usedTable[k].length == usedTable[k+<span class="number">1</span>].address) &#123;</span><br><span class="line">                <span class="type">int</span> e=<span class="number">0</span>;</span><br><span class="line">                <span class="keyword">while</span>(freeTable[e].flag) &#123;</span><br><span class="line">                    e++;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> m=e; j&lt;m; m--) &#123;</span><br><span class="line">                    freeTable[m].address = freeTable[m<span class="number">-1</span>].address;</span><br><span class="line">                    freeTable[m].length  = freeTable[m<span class="number">-1</span>].length;</span><br><span class="line">                    freeTable[m].flag    = <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                freeTable[j].address = usedTable[k].address;</span><br><span class="line">                freeTable[j].length  = usedTable[k].length;</span><br><span class="line">                usedTable[k].address = usedTable[k].length = usedTable[k].flag = <span class="number">0</span>;</span><br><span class="line">                <span class="comment">//上无，下无</span></span><br><span class="line">                i--;</span><br><span class="line">                <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">            &#125; <span class="comment">//elif</span></span><br><span class="line">            j++, h++;</span><br><span class="line">        &#125; <span class="comment">//while</span></span><br><span class="line">    &#125; <span class="comment">//else</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">sortUsedTable</span><span class="params">(<span class="keyword">struct</span> UsedTable temp[])</span> &#123;</span><br><span class="line">    <span class="type">int</span> j, k, f;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">UsedTable</span> <span class="title">t</span>;</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">0</span>; l&lt;i; l++) &#123;</span><br><span class="line">        <span class="keyword">if</span>(temp[l].flag == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> m = l; m&lt;=i; m++) temp[m] = temp[m+<span class="number">1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(j=<span class="number">0</span>; j&lt;i<span class="number">-1</span>; j++) &#123;</span><br><span class="line">        f=<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(k=<span class="number">0</span>; k&lt;i<span class="number">-1</span>-j; k++)</span><br><span class="line">            <span class="keyword">if</span>(temp[k+<span class="number">1</span>].address &lt; temp[k].address) &#123;</span><br><span class="line">                f=<span class="number">0</span>;</span><br><span class="line">                t         = temp[k];</span><br><span class="line">                temp[k]   = temp[k+<span class="number">1</span>];</span><br><span class="line">                temp[k+<span class="number">1</span>] = t;</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span>(f) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">printUsedTable</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> UsedTable temp)</span> &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\t%d\t%d\t%d\tNormal\n&quot;</span>, temp.flag, temp.address, temp.length);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">printFreeTable</span><span class="params">(<span class="type">const</span> <span class="keyword">struct</span> FreeTable temp)</span> &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;\033[0;36m\tNULL\t%d\t%d\tNULL\n\033[0m&quot;</span>, temp.address, temp.length);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> a=<span class="number">0</span>, proName=<span class="number">0</span>, proLenth=<span class="number">0</span>;</span><br><span class="line">    InitTable();</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">        <span class="type">int</span> m=<span class="number">0</span>, j=<span class="number">0</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32m*Select operation:\n\033[0m&quot;</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\t(0) - exit\n\t(1) - allocate memory\n\t(2) - reclaim memory\n\t(3) - print table\n&quot;</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32minput operation: \033[0m&quot;</span>);</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;a);</span><br><span class="line">        <span class="keyword">switch</span>(a) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;Bye~\n&quot;</span>);</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">while</span>(<span class="number">1</span>) &#123;</span><br><span class="line">                    <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32mthe name of the progress: \033[0m&quot;</span>);</span><br><span class="line">                    <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;proName);</span><br><span class="line">                    <span class="keyword">if</span>(proName == <span class="number">0</span>) &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31munvalidated proName!\033[0m\n&quot;</span>);</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">if</span>(findProName(usedTable, proName) != <span class="number">-1</span>) &#123;</span><br><span class="line">                        <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mthis proName has been used!\033[0m\n&quot;</span>);</span><br><span class="line">                        <span class="keyword">continue</span>;</span><br><span class="line">                    &#125;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32mrequired memory size: \033[0m&quot;</span>);</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;proLenth);</span><br><span class="line">                allocTable(proName, proLenth);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">2</span>:</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32mthe name of the progress to reclaim: \033[0m&quot;</span>);</span><br><span class="line">                <span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;proName);</span><br><span class="line">                reclaimTable(proName);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> <span class="number">3</span>:</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;----------------------------------------------\n&quot;</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;\033[0;32mmemory table:\033[0m\n&quot;</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;\033[0;34m\tproName\taddress\tlength\tstatus\033[0m\n&quot;</span>);</span><br><span class="line">                sortUsedTable(usedTable);</span><br><span class="line">                <span class="keyword">while</span>(usedTable[m].flag || freeTable[j].flag) &#123;</span><br><span class="line">                    <span class="keyword">if</span>(usedTable[m].address &lt; freeTable[j].address &amp;&amp; usedTable[m].flag) &#123;</span><br><span class="line">                        printUsedTable(usedTable[m]);</span><br><span class="line">                        m++;</span><br><span class="line">                    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (freeTable[j].flag) &#123;</span><br><span class="line">                        printFreeTable(freeTable[j]);</span><br><span class="line">                        j++;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;----------------------------------------------\n&quot;</span>);</span><br><span class="line">                <span class="comment">//getchar();</span></span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">default</span>: <span class="built_in">printf</span>(<span class="string">&quot;\033[0;31mNo such this operation\n\033[0m&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 操作系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>制作目标检测数据集常用python脚本整理</title>
      <link href="/2022/04/09/%E5%88%B6%E4%BD%9C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%B8%E7%94%A8python%E8%84%9A%E6%9C%AC%E6%95%B4%E7%90%86/"/>
      <url>/2022/04/09/%E5%88%B6%E4%BD%9C%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E5%B8%B8%E7%94%A8python%E8%84%9A%E6%9C%AC%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>制作个人目标检测数据集时，有查找爬取的错误图片，更改标注路径等等需要，写了以下python脚本</p><span id="more"></span><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><h3 id="查找错误图片"><a href="#查找错误图片" class="headerlink" title="查找错误图片"></a>查找错误图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> imghdr</span><br><span class="line"><span class="keyword">from</span> progressbar <span class="keyword">import</span> ProgressBar</span><br><span class="line"></span><br><span class="line">path =<span class="string">&#x27;./JPEGImages&#x27;</span></span><br><span class="line">original_images =[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> root, dirs, filenames <span class="keyword">in</span> os.walk(path):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">        original_images.append(os.path.join(root, filename))</span><br><span class="line"></span><br><span class="line">original_images = <span class="built_in">sorted</span>(original_images)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;num:&#x27;</span>,<span class="built_in">len</span>(original_images))</span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;check_error.txt&#x27;</span>,<span class="string">&#x27;w+&#x27;</span>)</span><br><span class="line">error_images =[]</span><br><span class="line">progress = ProgressBar()</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> progress(original_images):</span><br><span class="line">    check = imghdr.what(filename)</span><br><span class="line">    <span class="keyword">if</span> check == <span class="literal">None</span>:</span><br><span class="line">        f.write(filename)</span><br><span class="line">        f.write(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        error_images.append(filename)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(error_images))</span><br><span class="line">f.seek(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> f:</span><br><span class="line">    <span class="built_in">print</span>(s)</span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h3 id="按顺序重命名标注文件和图片（分成两部分防止图片和标注文件数量不一致）"><a href="#按顺序重命名标注文件和图片（分成两部分防止图片和标注文件数量不一致）" class="headerlink" title="按顺序重命名标注文件和图片（分成两部分防止图片和标注文件数量不一致）"></a>按顺序重命名标注文件和图片（分成两部分防止图片和标注文件数量不一致）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BatchRename</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.path1 = <span class="string">&#x27;./JPEGImages&#x27;</span></span><br><span class="line">        self.path2 = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rename</span>(<span class="params">self</span>):</span><br><span class="line">        filelist1 = os.listdir(self.path1)</span><br><span class="line">        filelist1.sort()</span><br><span class="line">        total_num1 = <span class="built_in">len</span>(filelist1)</span><br><span class="line">        i1 = <span class="number">1</span></span><br><span class="line">        srcpath1 = os.path.abspath(self.path1)</span><br><span class="line">        <span class="keyword">for</span> item1 <span class="keyword">in</span> filelist1:</span><br><span class="line">            os.rename(os.path.join(srcpath1, item1), os.path.join(srcpath1, <span class="built_in">str</span>(i1).zfill(<span class="number">5</span>) + <span class="string">&#x27;.jpg&#x27;</span>))</span><br><span class="line">            i1 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        filelist2 = os.listdir(self.path2)</span><br><span class="line">        filelist2.sort()</span><br><span class="line">        total_num2 = <span class="built_in">len</span>(filelist2)</span><br><span class="line">        i2 = <span class="number">1</span></span><br><span class="line">        srcpath2 = os.path.abspath(self.path2)</span><br><span class="line">        <span class="keyword">for</span> item2 <span class="keyword">in</span> filelist2:</span><br><span class="line">            os.rename(os.path.join(srcpath2, item2), os.path.join(srcpath2, <span class="built_in">str</span>(i2).zfill(<span class="number">5</span>) + <span class="string">&#x27;.xml&#x27;</span>))</span><br><span class="line">            i2 += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo = BatchRename()</span><br><span class="line">    demo.rename()</span><br></pre></td></tr></table></figure><h3 id="修改标注文件中的图片路径为本目录下"><a href="#修改标注文件中的图片路径为本目录下" class="headerlink" title="修改标注文件中的图片路径为本目录下"></a>修改标注文件中的图片路径为本目录下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os,sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FilesChange</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.path = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Change</span>(<span class="params">self</span>):</span><br><span class="line">        filelist = os.listdir(self.path)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> filelist:</span><br><span class="line">            srcfile = os.path.join(os.path.abspath(self.path), item)</span><br><span class="line">            item = item.rstrip(<span class="string">&#x27;.xml&#x27;</span>)</span><br><span class="line">            item = item.zfill(<span class="number">5</span>)</span><br><span class="line">            line_replace = <span class="number">2</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(srcfile,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">                lines = fd.readlines()</span><br><span class="line">            lines[line_replace] = (<span class="string">&#x27;        &lt;filename&gt;&#x27;</span> + item + <span class="string">&#x27;.webp&lt;/filename&gt;\n&#x27;</span>)</span><br><span class="line">            lines[line_replace + <span class="number">1</span>] = (<span class="string">&#x27;        &lt;path&gt;&#x27;</span> + <span class="built_in">format</span>(srcfile[:-<span class="number">21</span>]) + <span class="string">&#x27;JPEGImages/&#x27;</span> + item + <span class="string">&#x27;.webp&lt;/path&gt;\n&#x27;</span>)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(srcfile,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">                fd.writelines(lines)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo = FilesChange()</span><br><span class="line">    demo.Change()</span><br></pre></td></tr></table></figure><h3 id="输出没有标注的图片和标注文件名到-txt-文件"><a href="#输出没有标注的图片和标注文件名到-txt-文件" class="headerlink" title="输出没有标注的图片和标注文件名到 txt 文件"></a>输出没有标注的图片和标注文件名到 txt 文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os,sys,re</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AppleSearch</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.path = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Search</span>(<span class="params">self</span>):</span><br><span class="line">        filetxt = <span class="string">&#x27;NoAppleXml.txt&#x27;</span></span><br><span class="line">        txtlist = []</span><br><span class="line">        filelist = os.listdir(self.path)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> filelist:</span><br><span class="line">            srcfile = os.path.join(os.path.abspath(self.path), item)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(srcfile,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">                content = fd.read()</span><br><span class="line">                target = re.findall(<span class="string">&quot;apple&quot;</span>, content)</span><br><span class="line">            <span class="keyword">if</span> target == []:</span><br><span class="line">                txtlist.append(item.rstrip(<span class="string">&#x27;.xml&#x27;</span>))</span><br><span class="line">        txtpath = os.path.join(os.path.abspath(<span class="string">&#x27;./&#x27;</span>), filetxt)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(txtpath,<span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> txtlist:</span><br><span class="line">                <span class="built_in">print</span>(i)</span><br><span class="line">                fd.write(<span class="string">&quot;&#123;&#125;\n&quot;</span>.<span class="built_in">format</span>(i)) </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo = AppleSearch()</span><br><span class="line">    demo.Search()</span><br></pre></td></tr></table></figure><h3 id="从-txt-文件读取要删除的图片和标注文件"><a href="#从-txt-文件读取要删除的图片和标注文件" class="headerlink" title="从 txt 文件读取要删除的图片和标注文件"></a>从 txt 文件读取要删除的图片和标注文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os,sys,fileinput,re</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DeleteNoApple</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.path1 = <span class="string">&#x27;./JPEGImages&#x27;</span></span><br><span class="line">        self.path2 = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Delete</span>(<span class="params">self</span>):</span><br><span class="line">        filetxt = <span class="string">&#x27;./NoAppleXml.txt&#x27;</span></span><br><span class="line">        content = []</span><br><span class="line">        txtpath = os.path.join(os.path.abspath(<span class="string">&#x27;./&#x27;</span>), filetxt)</span><br><span class="line">        <span class="comment"># with open(txtpath) as fd:</span></span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> fileinput.<span class="built_in">input</span>(txtpath):</span><br><span class="line">            content.append(line.rstrip(<span class="string">&#x27;\n&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> content:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> content:</span><br><span class="line">                srcpath1 = os.path.join(os.path.abspath(self.path1), i+<span class="string">&#x27;.webp&#x27;</span>)</span><br><span class="line">                os.remove(srcpath1)</span><br><span class="line">                srcpath2 = os.path.join(os.path.abspath(self.path2), i+<span class="string">&#x27;.xml&#x27;</span>)</span><br><span class="line">                os.remove(srcpath2)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo = DeleteNoApple()</span><br><span class="line">    demo.Delete()</span><br></pre></td></tr></table></figure><h3 id="批量图片格式转换-（未过滤-jpg-图片防止图片编码错误）"><a href="#批量图片格式转换-（未过滤-jpg-图片防止图片编码错误）" class="headerlink" title="批量图片格式转换 （未过滤 jpg 图片防止图片编码错误）"></a>批量图片格式转换 （未过滤 jpg 图片防止图片编码错误）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">imgList = os.listdir(<span class="string">&#x27;./JPEGImages&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> imgList:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        img = Image.<span class="built_in">open</span>(item)</span><br><span class="line">        file_name, file_type = os.path.splitext(item)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># if file_type == &#x27;jpg&#x27;:</span></span><br><span class="line">        <span class="comment">#     continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img.save(<span class="string">&quot;%s.webp&quot;</span>%(file_name), <span class="string">&#x27;jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;picture convert error&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="批量更改标签名"><a href="#批量更改标签名" class="headerlink" title="批量更改标签名"></a>批量更改标签名</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os,sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FilesChange</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.path = <span class="string">&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Change</span>(<span class="params">self</span>):</span><br><span class="line">        old = <span class="string">&quot;Apple&quot;</span></span><br><span class="line">        new = <span class="string">&quot;apple&quot;</span></span><br><span class="line">        filelist = os.listdir(self.path)</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> filelist:</span><br><span class="line">            srcfile = os.path.join(os.path.abspath(self.path), item)</span><br><span class="line">            line_replace = <span class="number">2</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(srcfile,<span class="string">&#x27;r+&#x27;</span>) <span class="keyword">as</span> fd:</span><br><span class="line">                lines = fd.readlines()</span><br><span class="line">                fd.seek(<span class="number">0</span>)</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                    <span class="keyword">if</span> old <span class="keyword">in</span> line:</span><br><span class="line">                        lines=<span class="string">&quot;&quot;</span>.join(lines).replace(old,new)</span><br><span class="line">                fd.writelines(<span class="string">&quot;&quot;</span>.join(lines))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    demo = FilesChange()</span><br><span class="line">    demo.Change()</span><br></pre></td></tr></table></figure><h3 id="查找无对应图片的标签-和-无对应标签的图片"><a href="#查找无对应图片的标签-和-无对应标签的图片" class="headerlink" title="查找无对应图片的标签 和 无对应标签的图片"></a>查找无对应图片的标签 和 无对应标签的图片</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">path1 = <span class="string">r&#x27;./JPEGImages&#x27;</span></span><br><span class="line">path2 = <span class="string">r&#x27;./Annotations&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">file_name</span>(<span class="params">image_dir,xml_dir</span>):</span><br><span class="line">    jpg_list = []</span><br><span class="line">    xml_list = []</span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(image_dir):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            jpg_list.append(os.path.splitext(file)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> root, dirs, files <span class="keyword">in</span> os.walk(xml_dir):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            xml_list.append(os.path.splitext(file)[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(jpg_list))</span><br><span class="line">    diff = <span class="built_in">set</span>(xml_list).difference(<span class="built_in">set</span>(jpg_list))  <span class="comment"># 差集，在a中但不在b中的元素</span></span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> diff:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;no jpg&quot;</span>, name + <span class="string">&quot;.xml&quot;</span>)</span><br><span class="line">    diff2 = <span class="built_in">set</span>(jpg_list).difference(<span class="built_in">set</span>(xml_list))  <span class="comment"># 差集，在b中但不在a中的元素</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(diff2))</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> diff2:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;no xml&quot;</span>, name + <span class="string">&quot;.webp&quot;</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    file_name(path1,path2)</span><br></pre></td></tr></table></figure><h3 id="使用-imgaug-库图像增广"><a href="#使用-imgaug-库图像增广" class="headerlink" title="使用 imgaug 库图像增广"></a>使用 <code>imgaug</code> 库图像增广</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> os <span class="keyword">import</span> getcwd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> imgaug <span class="keyword">as</span> ia</span><br><span class="line"><span class="keyword">from</span> imgaug <span class="keyword">import</span> augmenters <span class="keyword">as</span> iaa</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> ImageFile</span><br><span class="line"></span><br><span class="line">ImageFile.LOAD_TRUNCATED_IMAGES = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ia.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_xml_annotation</span>(<span class="params">root, image_id</span>):</span><br><span class="line">    in_file = <span class="built_in">open</span>(os.path.join(root, image_id))</span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    bndboxlist = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">object</span> <span class="keyword">in</span> root.findall(<span class="string">&#x27;object&#x27;</span>):  <span class="comment"># 找到root节点下的所有country节点</span></span><br><span class="line">        bndbox = <span class="built_in">object</span>.find(<span class="string">&#x27;bndbox&#x27;</span>)  <span class="comment"># 子节点下节点rank的值</span></span><br><span class="line"></span><br><span class="line">        xmin = <span class="built_in">int</span>(<span class="built_in">float</span>(bndbox.find(<span class="string">&#x27;xmin&#x27;</span>).text))</span><br><span class="line">        xmax = <span class="built_in">int</span>(<span class="built_in">float</span>(bndbox.find(<span class="string">&#x27;xmax&#x27;</span>).text))</span><br><span class="line">        ymin = <span class="built_in">int</span>(<span class="built_in">float</span>(bndbox.find(<span class="string">&#x27;ymin&#x27;</span>).text))</span><br><span class="line">        ymax = <span class="built_in">int</span>(<span class="built_in">float</span>(bndbox.find(<span class="string">&#x27;ymax&#x27;</span>).text))</span><br><span class="line">        <span class="comment"># print(xmin,ymin,xmax,ymax)</span></span><br><span class="line">        bndboxlist.append([xmin,ymin,xmax,ymax])</span><br><span class="line">        <span class="comment"># print(bndboxlist)</span></span><br><span class="line"></span><br><span class="line">    bndbox = root.find(<span class="string">&#x27;object&#x27;</span>).find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> bndboxlist</span><br><span class="line"><span class="comment"># (506.0000, 330.0000, 528.0000, 348.0000) -&gt; (520.4747, 381.5080, 540.5596, 398.6603)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">change_xml_annotation</span>(<span class="params">root, image_id, new_target</span>):</span><br><span class="line">    new_xmin = new_target[<span class="number">0</span>]</span><br><span class="line">    new_ymin = new_target[<span class="number">1</span>]</span><br><span class="line">    new_xmax = new_target[<span class="number">2</span>]</span><br><span class="line">    new_ymax = new_target[<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    in_file = <span class="built_in">open</span>(os.path.join(root, <span class="built_in">str</span>(image_id) + <span class="string">&#x27;.xml&#x27;</span>))  <span class="comment"># 这里root分别由两个意思</span></span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    xmlroot = tree.getroot()</span><br><span class="line">    <span class="built_in">object</span> = xmlroot.find(<span class="string">&#x27;object&#x27;</span>)</span><br><span class="line">    bndbox = <span class="built_in">object</span>.find(<span class="string">&#x27;bndbox&#x27;</span>)</span><br><span class="line">    xmin = bndbox.find(<span class="string">&#x27;xmin&#x27;</span>)</span><br><span class="line">    xmin.text = <span class="built_in">str</span>(new_xmin)</span><br><span class="line">    ymin = bndbox.find(<span class="string">&#x27;ymin&#x27;</span>)</span><br><span class="line">    ymin.text = <span class="built_in">str</span>(new_ymin)</span><br><span class="line">    xmax = bndbox.find(<span class="string">&#x27;xmax&#x27;</span>)</span><br><span class="line">    xmax.text = <span class="built_in">str</span>(new_xmax)</span><br><span class="line">    ymax = bndbox.find(<span class="string">&#x27;ymax&#x27;</span>)</span><br><span class="line">    ymax.text = <span class="built_in">str</span>(new_ymax)</span><br><span class="line">    tree.write(os.path.join(root, <span class="built_in">str</span>(image_id) + <span class="string">&quot;_aug&quot;</span> + <span class="string">&#x27;.xml&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">change_xml_list_annotation</span>(<span class="params">root, image_id, new_target,saveroot,<span class="built_in">id</span></span>):</span><br><span class="line"></span><br><span class="line">    in_file = <span class="built_in">open</span>(os.path.join(root, <span class="built_in">str</span>(image_id) + <span class="string">&#x27;.xml&#x27;</span>))  <span class="comment"># 这里root分别由两个意思</span></span><br><span class="line">    tree = ET.parse(in_file)</span><br><span class="line">    xmlroot = tree.getroot()</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">object</span> <span class="keyword">in</span> xmlroot.findall(<span class="string">&#x27;object&#x27;</span>):  <span class="comment"># 找到root节点下的所有country节点</span></span><br><span class="line">        bndbox = <span class="built_in">object</span>.find(<span class="string">&#x27;bndbox&#x27;</span>)  <span class="comment"># 子节点下节点rank的值</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># xmin = int(bndbox.find(&#x27;xmin&#x27;).text)</span></span><br><span class="line">        <span class="comment"># xmax = int(bndbox.find(&#x27;xmax&#x27;).text)</span></span><br><span class="line">        <span class="comment"># ymin = int(bndbox.find(&#x27;ymin&#x27;).text)</span></span><br><span class="line">        <span class="comment"># ymax = int(bndbox.find(&#x27;ymax&#x27;).text)</span></span><br><span class="line"></span><br><span class="line">        new_xmin = new_target[index][<span class="number">0</span>]</span><br><span class="line">        new_ymin = new_target[index][<span class="number">1</span>]</span><br><span class="line">        new_xmax = new_target[index][<span class="number">2</span>]</span><br><span class="line">        new_ymax = new_target[index][<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        xmin = bndbox.find(<span class="string">&#x27;xmin&#x27;</span>)</span><br><span class="line">        xmin.text = <span class="built_in">str</span>(new_xmin)</span><br><span class="line">        ymin = bndbox.find(<span class="string">&#x27;ymin&#x27;</span>)</span><br><span class="line">        ymin.text = <span class="built_in">str</span>(new_ymin)</span><br><span class="line">        xmax = bndbox.find(<span class="string">&#x27;xmax&#x27;</span>)</span><br><span class="line">        xmax.text = <span class="built_in">str</span>(new_xmax)</span><br><span class="line">        ymax = bndbox.find(<span class="string">&#x27;ymax&#x27;</span>)</span><br><span class="line">        ymax.text = <span class="built_in">str</span>(new_ymax)</span><br><span class="line"></span><br><span class="line">        index = index + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    tree.write(os.path.join(saveroot, <span class="built_in">str</span>(image_id) + <span class="string">&quot;_aug_&quot;</span> + <span class="built_in">str</span>(<span class="built_in">id</span>) + <span class="string">&#x27;.xml&#x27;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mkdir</span>(<span class="params">path</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 去除首位空格</span></span><br><span class="line">    path = path.strip()</span><br><span class="line">    <span class="comment"># 去除尾部 \ 符号</span></span><br><span class="line">    path = path.rstrip(<span class="string">&quot;\\&quot;</span>)</span><br><span class="line">    <span class="comment"># 判断路径是否存在</span></span><br><span class="line">    <span class="comment"># 存在     True</span></span><br><span class="line">    <span class="comment"># 不存在   False</span></span><br><span class="line">    isExists = os.path.exists(path)</span><br><span class="line">    <span class="comment"># 判断结果</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isExists:</span><br><span class="line">        <span class="comment"># 如果不存在则创建目录</span></span><br><span class="line">         <span class="comment"># 创建目录操作函数</span></span><br><span class="line">        os.makedirs(path)</span><br><span class="line">        <span class="built_in">print</span>(path + <span class="string">&#x27; 创建成功&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 如果目录存在则不创建，并提示目录已存在</span></span><br><span class="line">        <span class="built_in">print</span>(path + <span class="string">&#x27; 目录已存在&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    IMG_DIR = <span class="string">&quot;./JPEGImages&quot;</span></span><br><span class="line">    XML_DIR = <span class="string">&quot;./Annotations&quot;</span></span><br><span class="line"></span><br><span class="line">    AUG_XML_DIR = <span class="string">&quot;./aug_Annotations&quot;</span>  <span class="comment"># 存储增强后的XML文件夹路径</span></span><br><span class="line">    mkdir(AUG_XML_DIR)</span><br><span class="line"></span><br><span class="line">    AUG_IMG_DIR = <span class="string">&quot;./aug_JPEGImages&quot;</span>  <span class="comment"># 存储增强后的影像文件夹路径</span></span><br><span class="line">    mkdir(AUG_IMG_DIR)</span><br><span class="line"></span><br><span class="line">    AUGLOOP = <span class="number">2</span> <span class="comment"># 每张影像增强的数量</span></span><br><span class="line"></span><br><span class="line">    boxes_img_aug_list = []</span><br><span class="line">    new_bndbox = []</span><br><span class="line">    new_bndbox_list = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 影像增强</span></span><br><span class="line">    seq = iaa.Sequential([</span><br><span class="line">        <span class="comment"># iaa.Flipud(0.5),  # vertically flip 20% of all images</span></span><br><span class="line">        iaa.Fliplr(<span class="number">0.5</span>),  <span class="comment"># 镜像</span></span><br><span class="line">        iaa.Multiply((<span class="number">1.2</span>, <span class="number">1.5</span>)),  <span class="comment"># change brightness, doesn&#x27;t affect BBs</span></span><br><span class="line">        iaa.GaussianBlur(sigma=(<span class="number">0</span>, <span class="number">3.0</span>)), <span class="comment"># iaa.GaussianBlur(0.5),</span></span><br><span class="line">        iaa.Affine(</span><br><span class="line">            translate_px=&#123;<span class="string">&quot;x&quot;</span>: <span class="number">15</span>, <span class="string">&quot;y&quot;</span>: <span class="number">15</span>&#125;,</span><br><span class="line">            scale=(<span class="number">0.8</span>, <span class="number">0.95</span>),</span><br><span class="line">            rotate=(-<span class="number">30</span>, <span class="number">30</span>)</span><br><span class="line">        )  <span class="comment"># translate by 40/60px on x/y axis, and scale to 50-70%, affects BBs</span></span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> root, sub_folders, files <span class="keyword">in</span> os.walk(XML_DIR):</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> files:</span><br><span class="line"></span><br><span class="line">            bndbox = read_xml_annotation(XML_DIR, name)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(AUGLOOP):</span><br><span class="line">                seq_det = seq.to_deterministic()  <span class="comment"># 保持坐标和图像同步改变，而不是随机</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 读取图片</span></span><br><span class="line">                img = Image.<span class="built_in">open</span>(os.path.join(IMG_DIR, name[:-<span class="number">4</span>] + <span class="string">&#x27;.webp&#x27;</span>))</span><br><span class="line">                img = np.array(img)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># bndbox 坐标增强</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(bndbox)):</span><br><span class="line">                    bbs = ia.BoundingBoxesOnImage([</span><br><span class="line">                        ia.BoundingBox(x1=bndbox[i][<span class="number">0</span>], y1=bndbox[i][<span class="number">1</span>], x2=bndbox[i][<span class="number">2</span>], y2=bndbox[i][<span class="number">3</span>]),</span><br><span class="line">                    ], shape=img.shape)</span><br><span class="line"></span><br><span class="line">                    bbs_aug = seq_det.augment_bounding_boxes([bbs])[<span class="number">0</span>]</span><br><span class="line">                    boxes_img_aug_list.append(bbs_aug)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># new_bndbox_list:[[x1,y1,x2,y2],...[],[]]</span></span><br><span class="line">                    new_bndbox_list.append([<span class="built_in">int</span>(bbs_aug.bounding_boxes[<span class="number">0</span>].x1),</span><br><span class="line">                                            <span class="built_in">int</span>(bbs_aug.bounding_boxes[<span class="number">0</span>].y1),</span><br><span class="line">                                            <span class="built_in">int</span>(bbs_aug.bounding_boxes[<span class="number">0</span>].x2),</span><br><span class="line">                                            <span class="built_in">int</span>(bbs_aug.bounding_boxes[<span class="number">0</span>].y2)])</span><br><span class="line">                <span class="comment"># 存储变化后的图片</span></span><br><span class="line">                image_aug = seq_det.augment_images([img])[<span class="number">0</span>]</span><br><span class="line">                path = os.path.join(AUG_IMG_DIR, <span class="built_in">str</span>(name[:-<span class="number">4</span>]) + <span class="string">&quot;_aug_&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&#x27;.webp&#x27;</span>)</span><br><span class="line">                <span class="comment"># image_auged = bbs.draw_on_image(image_aug, thickness=0)</span></span><br><span class="line">                Image.fromarray(image_aug).save(path)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 存储变化后的XML</span></span><br><span class="line">                change_xml_list_annotation(XML_DIR, name[:-<span class="number">4</span>], new_bndbox_list,AUG_XML_DIR,epoch)</span><br><span class="line">                <span class="built_in">print</span>(<span class="built_in">str</span>(name[:-<span class="number">4</span>]) + <span class="string">&quot;_aug_&quot;</span> + <span class="built_in">str</span>(epoch) + <span class="string">&#x27;.webp&#x27;</span>)</span><br><span class="line">                new_bndbox_list = []</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据集 </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faster-RCNN原理笔记</title>
      <link href="/2022/03/30/Faster-RCNN%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/03/30/Faster-RCNN%E5%8E%9F%E7%90%86%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>详细记录了 Faster RCNN 网络原理和个人的理解</p><span id="more"></span><ul><li>[论文下载](</li><li>[论文源码](</li></ul><h2 id="前期"><a href="#前期" class="headerlink" title="前期"></a>前期</h2><p>[Tensorflow 2.0 基础](</p><p>[RCNN 原理](</p><p>[Fast RCNN 原理](</p><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="论文中的网络结构图解"><a href="#论文中的网络结构图解" class="headerlink" title="论文中的网络结构图解"></a>论文中的网络结构图解</h3><p><img src="/image/Faster-RCNN原理笔记/faster-rcnn-network.webp" style="zoom:30%;" /></p><p>主要步骤是</p><ol><li>输入图片</li><li>对图片进行卷积，提取特征</li><li>使用 RPN 网络生成 Anchor box，对其裁剪过滤后，通过 softmax 对前景和后景分类，同时，bounding box regression 修正 anchor box，形成校正后的 proposals</li><li>将 proposals 映射到 feature maps 上</li><li>通过 RoI pooling 层使每个 RoI 生成固定尺寸的 feature map</li><li>利用 Softmax Loss 和 Smooth L1 Loss 对分类概率和边框回归联合训练</li></ol><h3 id="Faster-RCNN-具体的网络结构图"><a href="#Faster-RCNN-具体的网络结构图" class="headerlink" title="Faster RCNN 具体的网络结构图"></a>Faster RCNN 具体的网络结构图</h3><p><img src="/image/Faster-RCNN原理笔记/construction.webp" alt="construction"></p><h3 id="主干特征提取网络"><a href="#主干特征提取网络" class="headerlink" title="主干特征提取网络"></a>主干特征提取网络</h3><p>可选 ResNet，MobileNet，VGG16 等网络，本模型使用的是 VGG16 网络，由卷积层模块后接全连接层模块构成，每个卷积层的参数分别为 <code>kernel_size=(3,3), padding=&#39;same&#39;, activation=&#39;relu&#39;, kernel_regularizer=&#39;l2&#39;</code>，最大池化层的参数为 <code>pool_size=(2,2), padding=&#39;same&#39;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">____________________________________________________________________________________________________</span><br><span class="line">Layer (<span class="built_in">type</span>)                                 Output Shape                            Param <span class="comment">#        </span></span><br><span class="line">====================================================================================================</span><br><span class="line">input_1 (InputLayer)                         [(<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">3</span>)]                   <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d (Conv2D)                              (<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">64</span>)                    <span class="number">1792</span>           </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_1 (Conv2D)                            (<span class="literal">None</span>, <span class="number">500</span>, <span class="number">500</span>, <span class="number">64</span>)                    <span class="number">36928</span>          </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d (MaxPooling2D)                 (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">64</span>)                    <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_2 (Conv2D)                            (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">128</span>)                   <span class="number">73856</span>          </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_3 (Conv2D)                            (<span class="literal">None</span>, <span class="number">250</span>, <span class="number">250</span>, <span class="number">128</span>)                   <span class="number">147584</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_1 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">128</span>)                   <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_4 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">295168</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_5 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">590080</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_6 (Conv2D)                            (<span class="literal">None</span>, <span class="number">125</span>, <span class="number">125</span>, <span class="number">256</span>)                   <span class="number">590080</span>         </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_2 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">256</span>)                     <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_7 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">1180160</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_8 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_9 (Conv2D)                            (<span class="literal">None</span>, <span class="number">63</span>, <span class="number">63</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">max_pooling2d_3 (MaxPooling2D)               (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">0</span>              </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_10 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_11 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">conv2d_12 (Conv2D)                           (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">512</span>)                     <span class="number">2359808</span>        </span><br><span class="line">____________________________________________________________________________________________________</span><br><span class="line">dense (Dense)                                (<span class="literal">None</span>, <span class="number">32</span>, <span class="number">32</span>, <span class="number">10</span>)                      <span class="number">5130</span>           </span><br><span class="line">====================================================================================================</span><br><span class="line">Total params: <span class="number">14</span>,<span class="number">719</span>,<span class="number">818</span></span><br><span class="line">Trainable params: <span class="number">14</span>,<span class="number">719</span>,<span class="number">818</span></span><br><span class="line">Non-trainable params: <span class="number">0</span></span><br><span class="line">____________________________________________________________________________________________________</span><br></pre></td></tr></table></figure><p>使用 VGG16 网络不像resnet那么复杂，更深的网络理论上效果也更好</p><h3 id="RPN-Region-Proposal-Networks"><a href="#RPN-Region-Proposal-Networks" class="headerlink" title="RPN (Region Proposal Networks)"></a>RPN (Region Proposal Networks)</h3><p>在图像中产生所有可能为目标的候选区域，用来解决生成检测框耗时较多的问题。RPN 根据 CNN 生成的特征图，在 img 的尺度上生成多个锚框，对生成的锚框进行分类和回归。</p><p><img src="/image/Faster-RCNN原理笔记/rpn.webp" alt="rpn" style="zoom:67%;" /></p><p>网络分为2条线，上面一条通过softmax分类 anchors 获得positive 和 negative 分类，下面一条用于计算对于 anchors 的 bounding box regression 偏移量，获得精确的 proposal。最后的 Proposal layer 负责综合 positive anchors 和对应 bounding box regression 偏移量获取 proposals，同时剔除太小和超出边界的 proposals</p><h4 id="ahchors"><a href="#ahchors" class="headerlink" title="ahchors"></a>ahchors</h4><p>是一种多尺度方法，以一个像素点为中心，生成一组描述 9 个矩形的矩阵，每行4个值 $(x_min, y_min, x_max, y_max)$ 表示矩形左上和右下角点坐标，长宽比为 $ width:height \in { 1:1, 1:2, 2:1 } $ </p><p><img src="/image/Faster-RCNN原理笔记/anchors.webp" alt="anchors"></p><p>其中，anchors size 是根据检测图像设置的，Faster RCNN网络会把所有输入的图像 reshape 成固定大小，在论文中，会为 feature map 中的每个像素点生成 anchors，后面的2次 bounding box regression 会修正 anchors 检测框位置</p><p><img src="/image/Faster-RCNN原理笔记/anchors.webp" alt="anchors" style="zoom:67%;" /></p><p>上图截取自论文，其中</p><ul><li>256-d: 论文中主干特征提取网络的最后一层 num_output=256，对应生成的 feature map 是256维的</li><li>sliding window: feature map 在进入 RPN 网络后，又进行了一次 3x3 的卷积，256-d 没有变</li><li>$cls \quad layer$: 已知每个像素点上有 k 个 anchor(图中 k = 9)，每个 anchor 要分前景(positive)和背景(negative)，所以每个点由 256-d 的 feature map 转化为 2k scores</li><li>$reg \quad layer$: 已知每个像素点上有 k 个 anchor(图中 k = 9)，每个 anchor 有 $(x, y, w, h)$ 对应的4个偏移量，所以每个点由 256-d 的 feature map 转化为 4k coordinates</li></ul><p><img src="/image/Faster-RCNN原理笔记/gernerate_anchors.webp" alt="gernerate_anchors" style="zoom:60%;" /></p><p>上图以 图片大小 500x500 为例，计算生成的 gernerate anchors 的数量</p><script type="math/tex; mode=display">\operatorname{ceil}(500 / 16) \times \operatorname{ceil}(500 / 16) \times 9=32 \times 32 \times 9= 9216</script><p>ceil()为向上取整，因为图中VGG16网络输出的 feature map size 为整数</p><h4 id="判定-positive-negative"><a href="#判定-positive-negative" class="headerlink" title="判定 positive/negative"></a>判定 positive/negative</h4><p>主要步骤：</p><ol><li>RPN 网络图中上面一条输入为共享层卷积的输出</li><li>进行通道数为2k(k=num_anchors)的 1x1 卷积</li><li>reshape 成两个通道</li><li>对通道层做归一化，使类别预测的概率和为 1</li><li>取最终的预测类别和概率</li><li>reshape 回复原状 <code>[1, h, w, 9*2]</code></li></ol><p>论文作者在源码中的 softmax_loss_layer.cpp 对最后 reshape层 的解释:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;Number of labels must match number of predictions; &quot;</span></span><br><span class="line"><span class="string">&quot;e.g., if softmax axis == 1 and prediction shape is (N, C, H, W), &quot;</span></span><br><span class="line"><span class="string">&quot;label count (number of labels) must be N*H*W, &quot;</span></span><br><span class="line"><span class="string">&quot;with integer values in &#123;0, 1, ..., C-1&#125;.&quot;</span>;</span><br></pre></td></tr></table></figure><h4 id="bounding-box-regression"><a href="#bounding-box-regression" class="headerlink" title="bounding box regression"></a>bounding box regression</h4><p>图中所示，绿色框为苹果的 ground truth，红色为提取的 positive anchors，即便红色的框被分类器识别为苹果，但是由于红色的框定位不准，这张图相当于没有正确的检测出苹果。所以需要采用一种方法对红色的框进行微调，使得 positive anchors 和 ground truth 更加接近</p><p><img src="/image/Faster-RCNN原理笔记/bounding.webp" alt="bounding" style="zoom:75%;" /></p><p>对于窗口一般使用四维向量 $(x, y, w, h)$ 表示，分别表示窗口的中心点坐标和宽高，红框代表原始的positive anchors，绿框代表目标的 ground truth，使得输入原始的 anchor 经过映射得到一个跟 ground truth 更接近的回归窗口，即</p><p>positive anchors:  $A = (A_x, A_y, A_w, A_h)$</p><p>ground truth:  $GT = (G_x, G_y, G_w, G_h)$</p><p>寻找 $F$，使 $F(A) = (G<em>{x}^{\prime}, G</em>{y}^{\prime}, G<em>{w}^{\prime}, G</em>{h}^{\prime})$</p><p>其中 $(G<em>{x}^{\prime}, G</em>{y}^{\prime}, G<em>{w}^{\prime}, G</em>{h}^{\prime}) \approx (G<em>{x}, G</em>{y}, G<em>{w}, G</em>{h})$</p><p><img src="/image/Faster-RCNN原理笔记/fag.webp" alt="fag" style="zoom:33%;" /></p><p>通过变换 $F$从 $A$ 变换到 $G’$，我们要做的是</p><p>平移</p><script type="math/tex; mode=display">G_x^\prime = A_w \cdot d_x(A) + A_x \\G_y^\prime = A_h \cdot d_y(A) + A_y</script><p>缩放</p><script type="math/tex; mode=display">G_w^\prime = A_w \cdot exp(d_w(A))\\G_h^\prime = A_h \cdot exp(d_h(A))</script><p>需要学习的是 $d_x(A), d_y(A), d_w(A) ,d_h(A)$ 这四个变换。当输入的 $A$ 与 $GT$ 相差较小时，认为这种变换是一种线性变换， 用线性回归来建模对窗口进行微调，当 $A$ 和 $GT$ 比较接近时，认为是复杂的非线性问题</p><p>已知线性回归公式 $Y = WX$，$X$ 为 feature map，定义为 $\phi$，训练传入$A$与$GT$之间的变换量 $(t_x, t_y, t_w, t_h, )$，$Y$为 $(d_x(A), d_y(A), d_w(A) ,d_h(A))$，则目标函数为</p><script type="math/tex; mode=display">d_*(A) = W_*^T\phi(A)</script><p>其中 $\phi(A)$ 是对应 anchor 的 feature map 组成的特征向量，$W<em>{*}$ 是需要学习的参数，$d</em>{*}(A)$ 是得到的预测值</p><p>在 Faster RCNN 论文中，positive anchor 与 ground truth 之间的平移量 $(t_x, t_y)$ 与尺度因子 $(t_w, t_h)$  如下</p><script type="math/tex; mode=display">t_x = (x-x_a)/w_a \quad t_y = (y-y_a)/h_a \\t_w = \log(w/w_a) \quad t_h = \log(h/h_a)</script><p>为了让预测值 $ {d<em>{*}(A) }$ 与真实值差距最小， ${\operatorname{smooth}</em>{L_{1}}}$ 损失函数为</p><script type="math/tex; mode=display">Loss = \begin{cases} 0.5 \cdot (\sum_i^N (t_*^i -W_*^T \cdot \phi(A^i))^2 & \text{if}|x| <1 \\\sum_i^N |t_*^i -W_*^T \cdot \phi(A^i)| - 0.5 & \text{otherwise} \\\end{cases}</script><p>优化目标函数为</p><script type="math/tex; mode=display">\hat{W}_* = {argmin}_{W_*} \sum_i^n (t_*^i -W_*^T \cdot \phi(A^i))^2 + \lambda \| W_* \|</script><p>之后可通过梯度下降等方法修正 anchor 位置，注意当 $A$ 和 $GT$ 比较接近时，才可近似认为上述线性变换及优化目标函数成立</p><h4 id="对-proposals-进行-bounding-box-regression"><a href="#对-proposals-进行-bounding-box-regression" class="headerlink" title="对 proposals 进行 bounding box regression"></a>对 proposals 进行 bounding box regression</h4><p>在第二条线路中，num_output=36，即经过该卷积输出图像为 WxHx36，存储为 <code>[1, 4x9, H, W]</code>，这里相当于 feature maps 每个点都有9个 anchors，每个 anchors 又都有4个用于回归的 $(d_x(A), d_y(A), d_w(A) ,d_h(A))$ 变换量</p><p>VGG16 网络输出 $32 <em> 32 </em> 512$ 的特征，对应设置 $32<em>32</em>k$ 个 anchors，因此RPN输出</p><ul><li><p>大小为 $32 <em> 32 </em> 2k$ 的 positive/negative softmax 分类特征矩阵</p></li><li><p>大小为 $32 <em> 32 </em> 4k$ 的 regression 坐标回归特征矩阵</p></li></ul><p>对应 RPN 的 positive/negative 分类和 bounding box regression 坐标回归</p><h4 id="Proposal-Layer"><a href="#Proposal-Layer" class="headerlink" title="Proposal Layer"></a>Proposal Layer</h4><p>Proposal Layer负责综合所有 $(d_x(A), d_y(A), d_w(A), d_h(A))$ 变换量和 positive anchors，计算出精准的proposal，送入后续 RoI Pooling Layer</p><p><img src="/image/Faster-RCNN原理笔记/proposal_layer.webp" style="zoom:67%;" /></p><p>Proposal Layer有3个输入：positive/negative anchors 分类器结果 rpn_cls_score，$(d_x(A), d_y(A), d_w(A), d_h(A))$ 的变换量 rpn_bbox_pred，img_info(包含 feat_stride = 16)</p><p>img_info: 对于一副任意大小 PxQ 图像，传入 Faster RCNN 前_prob概首先reshape到固定 MxN，im_info=[M, N, scale_factor] 保存了此次缩放的所有信息。然后经过 VGG16，经过4次 max_pooling2d 变为 WxH=(M/16)x(N/16) 大小，其中 feature_stride=16 则保存了该信息，用于计算 anchor 偏移量</p><p>Proposal Layer forward（前传函数）按照以下顺序依次处理: </p><ol><li>生成anchors，利用 $ (d_x(A), d_y(A), d_w(A), d_h(A)) $ 对所有的 anchors 做 bbox regression 回归（这里的 anchors 生成和训练时相同）</li><li>按照输入的 positive softmax scores 由大到小排序 anchors，提取前 pre_nms_top N(e.g. 5000) 个anchors，即提取修正位置后的 positive anchors</li><li>限定超出图像边界的 positive anchors 为图像边界，防止后续 ROIpooling 时 proposal 超出图像边界</li><li>剔除尺寸非常小的 positive anchors</li><li>对剩余的 positive anchors 进行NMS(极大值抑制)</li><li>Proposal Layer 有3个输入: positive 和 negative anchors 分类器结果 rpn_cls_score，对应的 bbox reg 的 (e.g. 300) 结果作为 proposal 输出</li></ol><p>输出 proposal 为 <code>[x_min, y_min, x_max, y_max]</code>，由于需要将 anchors 映射回原图判断是否超出边界，所以 proposal 对应的图像尺度为 MxN</p><h3 id="ROIHead"><a href="#ROIHead" class="headerlink" title="ROIHead"></a>ROIHead</h3><p>在传统的CNN网络中，当训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的 vector or matrix，如果输入图像大小不定，过去有2种解决办法:</p><ul><li>从图像中 crop 一部分传入网络</li><li>将图像warp成需要的大小后传入网络</li></ul><p><img src="/image/Faster-RCNN原理笔记/crop_warp.webp" alt="crop_warp" style="zoom:67%;" /></p><p>crop后破坏了图像的完整结构，warp后破坏了图像原始形状信息，两种方法都不好</p><p>为了使网络可以接收不同大小的图像，Faster RCNN 中提出了 ROIPooling，ROIPooling 从 [Spatial Pyramid Pooling]( 发展而来，这里不展开讨论</p><h4 id="ROI-pooling"><a href="#ROI-pooling" class="headerlink" title="ROI pooling"></a>ROI pooling</h4><p>ROIpooling 对 proposal 对 feature map 裁剪后的 ROIs 进行 maxpooling 使输入的 shape 相同，生成 proposal feature maps，它有3个参数: </p><ul><li>pooled_w: proposal feature maps 的 width</li><li>pooled_h: proposal feature maps 的 width</li><li>spatial_scale: 是 VGG16 提取 feature map 后对图像尺度的改变，也就是 feature_stride=16</li></ul><p>由于 proposal 是对应 MxN 尺度的，所以首先使用 spatial_scale 将其映射回  (M/16)x(N/16) 大小的 feature map 尺度，再将每个 proposal 对应的 feature map 区域水平分为 pooled_w x pooled_h 的网格，对网格的每一份都进行max pooling处理</p><p>例:</p><p>假定输入 feature map 为</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample1.webp" alt="pool_sample1" style="zoom:35%;" /></p><p>假定区域建议为</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample2.webp" alt="pool_sample2" style="zoom:35%;" /></p><p>假定 pooled_w=2, pooled_h=2</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample3.webp" alt="pool_sample3" style="zoom:35%;" /></p><p>对网格的每一份都进行 max pooling 处理</p><p><img src="/image/Faster-RCNN原理笔记/pool_sample4.webp" alt="pool_sample4" style="zoom:40%;" /></p><p>这种方法显著加快了训练和测试时间</p><h4 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h4><p><img src="/image/Faster-RCNN原理笔记/classfication.webp" alt="classfication" style="zoom:67%;" /></p><p>利用 ROIpooling 输出的 proposal feature maps，通过 1x1的conv2d 层与 softmax 计算每个 proposal 具体属于那个类别，输出 cls_prob 概率向量，同时再次利用 bounding box regression 获得每个 proposal 的位置偏移量 bbox_pred，用于回归更加精确的目标检测框</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>论文源码中训练 Faster RCNN 有两种方式，一种是四步交替训练法，一种是 end-to-end 训练法，本文只讨论四步交替训练法</p><p>由前面我们可知，Faster RCNN 大概可以分为 RPN 网络和 Fast RCNN 网络部分</p><ol><li><p>训练 RPN，用 feature map 初始化 RPN 网络，并端到端微调，生成 region proposal</p></li><li><p>用 feature map 初始化 Fast RCNN 网络部分，利用第一步的 RPN 生成的 region proposals 作为输入数据，接着训练 Fast RCNN部分，这时两个网络没有共享卷积层</p></li><li><p>用第二步的 Fast RCNN model 初始化 RPN 第二次进行训练，但固定共享的卷积层，并且只微调 RPN 独有的层，现在两个网络共享卷积层</p></li><li><p>由第三步的 RPN model 初始化 Fast RCNN 网络部分，输入数据为第三步生成的 proposals，保持共享的卷积层固定，微调 Fast RCNN 网络部分 Classification 中的卷积层，两个网络共享相同的卷积层，构成一个统一的网络，也就是论文中的 unified network</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卷积运算的输入输出shape</title>
      <link href="/2022/03/16/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BAshape/"/>
      <url>/2022/03/16/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BAshape/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>记录一下二维卷积神经网络中卷积运算的输入输出shape大小</p><span id="more"></span><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><ul><li><p>输入：$(H,W)$</p></li><li><p>卷积核：$(FH,FW)$</p></li><li><p>填充：$P$</p></li><li><p>步幅：$S$</p></li><li><p>输出：$(OH,OW)$</p></li></ul><script type="math/tex; mode=display">OH = \frac{H + 2P - FH}{S} + 1 \\\\OW = \frac{W + 2P - FW}{S} + 1</script>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>最小化交叉熵损失与最大似然估计的推导</title>
      <link href="/2022/03/05/%E6%9C%80%E5%B0%8F%E5%8C%96%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E4%B8%8E%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%8E%A8%E5%AF%BC/"/>
      <url>/2022/03/05/%E6%9C%80%E5%B0%8F%E5%8C%96%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E4%B8%8E%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>机器学习所使用的交叉熵损失函数与信息论里的交叉熵的推导与思考</p><span id="more"></span><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="信息论中的交叉熵定义"><a href="#信息论中的交叉熵定义" class="headerlink" title="信息论中的交叉熵定义"></a>信息论中的交叉熵定义</h3><script type="math/tex; mode=display">H(p,q) = H(p) + D_{KL}(p||q)</script><p>在 $ p,q $ 是离散分布时， 上式等价为</p><script type="math/tex; mode=display">H(p,q) = -\sum^K_{i=1} p(x_i) \log q(x_i)</script><p>其中， $x_i$ 是 $p,q$ 分布共同样本空间的同一个样本点，样本空间的大小为 $K$</p><h3 id="机器学习中的交叉熵定义"><a href="#机器学习中的交叉熵定义" class="headerlink" title="机器学习中的交叉熵定义"></a>机器学习中的交叉熵定义</h3><p>机器学习进行优化时， 会把所有样本的交叉熵值求平均，假设有 $N$ 个样本</p><script type="math/tex; mode=display">J(w) = \frac{1}{N} \sum^N_{n=1} H(p_n,q_n)</script><p>而信息论中的交叉熵仅仅是针对一个样本</p><p>因为交叉熵常用于解决分类问题，而分类问题的概率本质是计算类别变量的广义的伯努利分布，所以机器学习采用的是交叉熵的离散形式</p><script type="math/tex; mode=display">CE = -\sum^K_{i=1} t_i \log s_i</script><p>其中，$t_i$ 是期望的类别标签，$s_i$ 是模型对第 $i$ 个类别计算得到的 $score$ ，通常在计算损失之前，会用激活函数对 $score$ 加以转换，用 $f(s_i)$ 替代上式的 $s_i$</p><p>得到机器学习的交叉熵损失函数</p><script type="math/tex; mode=display">J(w) = - \frac{1}{N} \sum^N_{n=1} \sum^K_{i=1} t_i \log s_i</script><p>因为对于分类问题，假设模型的输出层上只有2个输出结点，而且是一个二分类单标签问题，如果输出层用符号 $Y$ 表示，那么 $Y$ 服从 $0-1$ 分布(是二项分布的特例，或称伯努利分布，二元分布)，即随机变量 $Y$ 的样本空间有两个样本点(分别对应输出层的两个输出结点)，每个样本点就是一个类别。我们希望机器学习模型训练出的分布是某个类别的概率为 $1$ ，另一个类别的概率为 $0$ 。推广到多分类单标签问题，那么 $Y$ 服从广义的伯努利分布(是多项式分布的特例，或称 $Category$ 分布，范畴分布，类别分布，$Multinoulli$ 分布(2012年在《Machine Learning - A Probabilistic Perspective》中正式提出))。</p><h3 id="最小化交叉熵损失与最大似然推导"><a href="#最小化交叉熵损失与最大似然推导" class="headerlink" title="最小化交叉熵损失与最大似然推导"></a>最小化交叉熵损失与最大似然推导</h3><p>先从一个直观的例子感受最小化交叉熵损失与最大似然的关系</p><script type="math/tex; mode=display">J(w)=-\frac{1}{N} \sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>去掉 $\frac{1}{N}$ 并不影响函数的单调性，机器学习任务的也可以是最小化下面的交叉熵损失</p><script type="math/tex; mode=display">J(w)=-\sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>等价于最大化</p><script type="math/tex; mode=display">J(w)=\sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>这其实就是对伯努利分布求最大似然中的对数似然函数</p><h3 id="伯努利分布的最大似然推导"><a href="#伯努利分布的最大似然推导" class="headerlink" title="伯努利分布的最大似然推导"></a>伯努利分布的最大似然推导</h3><p>有二元随机变量 $Y \in{0,1}$ ，设 $p(Y=1) = \beta$ ，那么它的概率质量函数(PMF)为</p><script type="math/tex; mode=display">P(Y \mid \beta)=\beta^{Y}(1-\beta)^{1-Y}</script><p>现有 $D=\left{y<em>{1}, y</em>{2}, \ldots, y_{N}\right} $ 来自 $Y$ ，样本容量为 $N$ 的一个样本，似然函数为</p><script type="math/tex; mode=display">P(D \mid \beta)=\prod_{i=1}^{N} P\left(Y=y_{i} \mid \beta\right)=\prod_{i=1}^{N} \beta^{y_{i}}(1-\beta)^{1-y_{i}}</script><p>在机器学习中，对 $\beta$ 的定义为</p><script type="math/tex; mode=display">\beta = p_{\theta}(Y = 1|x_i)</script><p>其中，$X=\left{x<em>{1}, \ldots, x</em>{N}\right}$ ，$ x_{i} \in X$，$X$ 是 $D$ 中每个样本点对应类别的特征的集合。即给定模型参数 $\theta$ 和随机变量的样本点 $Y=1$ 的属性特征 $x_i$ ( $x_i$ 可以是一个向量)，让模型估计出事件 $Y=1$ 的概率(同时也是当前伯努利分布的参数)</p><p>故上述的似然函数的参数不再是伯努利分布的 $\beta$，而是模型的参数 $\theta$，有</p><script type="math/tex; mode=display">P(D \mid \theta, X)=\prod_{i=1}^{N} p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}}</script><p>易得对数似然函数</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(\theta ; X, D) &= \log \prod_{i=1}^{N} p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\&= \sum_{i=1}^{N} \log p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\&= \sum_{i=1}^{N} \log p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}+\log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\&= \sum_{i=1}^{N} y_{i} \log p_{\theta}\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)\end{aligned}</script><p>以下给出最大似然估计与最小化交叉熵损失的转化过程，意在说明在伯努利分布下，最大似然估计与最小化交叉熵损失是同概念的</p><script type="math/tex; mode=display">\begin{aligned}\theta_{p} &= \arg \max _{\theta} \sum_{i=1}^{N} y_{i} \log p_{\theta}\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right) \\&= \arg \max_{\theta} \sum^N_{i=1}y_i \log \hat{y_i} + (1-y_i) \log (1- \hat{y_i}) \\&= \arg \min_{\theta} - \sum^N_{i=1}y_i \log \hat{y_i} + (1-y_i) \log (1- \hat{y_i}) \\&= \arg \min_{\theta} \sum^N_{i=1} H(y_i,\hat{y_i})\end{aligned}</script><h3 id="广义伯努利分布的最大似然推导"><a href="#广义伯努利分布的最大似然推导" class="headerlink" title="广义伯努利分布的最大似然推导"></a>广义伯努利分布的最大似然推导</h3><p>单标签多分类任务的类别随机变量只服从多项式分布中试验次数为1的情况，广义伯努利分布（ $Category$ 分布）对应的是更常见的单标签多分类任务，以下讨论伯努利分布到广义伯努利分布的过渡以及与最大似然估计的关系</p><p>有 $K$ 元类别随机变量 $Y \in{1, \ldots, K} $ ，且 $p(Y=J)=\beta_{j}$，概率质量函数库为</p><script type="math/tex; mode=display">P(Y|B)=\prod^K_{i=1} \beta ^{I(Y=i)}_i</script><p>如果 $Y=i$，$I(Y=i) = 1$ ，否则， $I(Y=i)=0$ </p><blockquote><p>这个概率质量函数之所以看上去有点奇怪，是因为它出现了 $Identity$ 函数。而符合我们直觉的是维基百科对广义伯努利分布的另一个PMF定义，即 $p(Y=i)=p_i$ ，即直接根据列联表，获得该事件发生的概率。如果想解释上面晦涩难懂的包含 $Identity$ 函数的PMF，可以用严格按照伯努利试验的描述来解释：在进行1次试验中抽到第 $j$ 个类别，其他概率被抽中0次的概率，即</p><script type="math/tex; mode=display">P(Y=j)=(1,1) \ldots p_{j-1}^{0} p_{j}^{1} p_{j+1}^{0} \ldots</script></blockquote><p>$D=\left{y<em>{1}, y</em>{2}, \ldots, y_{N}\right}$ 是来自 $Y$ 的，样本容量为 $N$，的一个样本，那么似然函数为</p><script type="math/tex; mode=display">P(D \mid \beta)=\prod_{i=1}^{N} \prod_{j=1}^{K} \beta_{j}^{I\left(y_{i}=j\right)}</script><p>同样地，在机器学习模型中，对上述关于对 $\beta$ 的定义做出转变</p><script type="math/tex; mode=display">\beta_{j}=p_{\theta}\left(Y=j \mid x_{i}\right)</script><p>$X=\left{x<em>{1}, \cdots, x</em>{N}\right}$，$x_i \in X$，$X$ 是 $D$ 中的每个样本点对应特征的集合</p><p>同理上述的似然函数的参数不再是广义伯努利分布的 $\beta_1,beta_2,\cdots,\beta_k$，而是模型的参数 $\theta$，所以似然函数有</p><script type="math/tex; mode=display">P(D \mid \theta, X)=\prod_{i=1}^{N} \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)}</script><p>易得对数似然函数</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}(\theta ; X, D) &= \log \prod_{i=1}^{N} \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)}\\&= \sum_{i=1}^{N} \log \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)} \\&= \sum_{i=1}^{N} \sum_{j=1}^{K} \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)} \\&= \sum_{i=1}^{N} \sum_{j=1}^{K} I\left(y_{i}=j\right) \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)\end{aligned}</script><p>因为最大化上述式子具有约束条件 $\sum^K_{i=1} \beta_i = 1$，所以最大化上面的对数似然函数是一个条件极值问题，使用拉格朗日乘数法进行求解，得到下面的关于求解广义伯努利分布下的交叉熵的拉格朗日函数</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}(\theta ; X, D, \lambda)=\sum_{i=1}^{N} \sum_{j=1}^{K} I\left(y_{i}=j\right) \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)+\lambda\left(1-\sum_{k} \beta_{i}\right)</script><p>在没有使用机器学习模型的前提下，我们只需对分布的参数和 $\lambda$ 求偏导就能得到参数的估计值</p><p>用模型的参数记号 $\hat{\beta<em>j}$ 表示 $p</em>{\theta} (y_i = j | x_i)$，用 one-hot 向量表示 $Identity$ 函数值</p><p>得</p><script type="math/tex; mode=display">\mathcal{L}(\theta ; X, D)=\sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j}</script><p>下面同样给出极大似然估计与最小化广义伯努利分布的交叉熵损失函数的转化过程，意在说明在广义伯努利分布下，最大似然估计与最小化交叉熵损失也是同概念的</p><script type="math/tex; mode=display">\begin{aligned}\theta_{p} &= \arg \max _{\theta} \sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j} \\&= \arg \min _{\theta}-\sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j} \\&= \arg \min _{\theta} \sum_{i=1}^{N} H\left(y_{j}, \hat{\beta}_{j}\right)\end{aligned}</script><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最小化交叉熵损失函数与最大似然估计之间的等价并非巧合，同是处理信息的公式，只是应用的方向不同</p>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率与信息论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用Opencv+Python的AR小demo</title>
      <link href="/2022/02/23/%E4%BD%BF%E7%94%A8Opencv+Python%E7%9A%84AR%E5%B0%8Fdemo/"/>
      <url>/2022/02/23/%E4%BD%BF%E7%94%A8Opencv+Python%E7%9A%84AR%E5%B0%8Fdemo/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>浅浅了解一下 Python OpenCV，试着给自己的 iphone 8 做一下相机标定</p><span id="more"></span><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><blockquote><p>增强现实( AR ) 是一种真实世界环境的交互式体验，其中存在于现实世界中的对象通过计算机生成的感知信息得到增强，有时跨越多种感官模式，包括视觉、听觉、触觉、体感和嗅觉。AR 可以定义为一个包含三个基本特征的系统：真实和虚拟世界的结合、实时交互以及虚拟和真实对象的准确 3D 配准。重叠的感觉信息可以是建设性的（即对自然环境的补充）或破坏性的（即对自然环境的掩蔽）。这种体验与物理世界无缝交织，因此被视为真实环境的沉浸式体验。[4]通过这种方式，增强现实改变了人们对现实世界环境的持续感知，而虚拟现实完全用模拟环境取代了用户的现实世界环境。增强现实与两个主要同义词相关：混合现实和计算机介导的现实。</p><p align="right">——以上内容来自Wiki百科</p></blockquote><h2 id="类别"><a href="#类别" class="headerlink" title="类别"></a>类别</h2><h3 id="Vision-based-AR（基于计算机视觉的AR）"><a href="#Vision-based-AR（基于计算机视觉的AR）" class="headerlink" title="Vision based AR（基于计算机视觉的AR）"></a>Vision based AR（基于计算机视觉的AR）</h3><h4 id="Marker-Based-AR-（基于标定的AR）"><a href="#Marker-Based-AR-（基于标定的AR）" class="headerlink" title="Marker-Based AR （基于标定的AR）"></a>Marker-Based AR （基于标定的AR）</h4><p>如：</p><p><img src="/image/使用Opencv+Python的AR小demo/6.png" alt=""></p><h4 id="Marker-Less-AR（基于特征点的AR"><a href="#Marker-Less-AR（基于特征点的AR" class="headerlink" title="Marker-Less AR（基于特征点的AR)"></a>Marker-Less AR（基于特征点的AR)</h4><p>如：</p><p><img src="/image/使用Opencv+Python的AR小demo/7.png" alt=""></p><h3 id="LBS-based-AR（基于地理位置信息的AR）"><a href="#LBS-based-AR（基于地理位置信息的AR）" class="headerlink" title="LBS based AR（基于地理位置信息的AR）"></a>LBS based AR（基于地理位置信息的AR）</h3><p>如：</p><p><img src="/image/使用Opencv+Python的AR小demo/8.png" alt=""></p><blockquote><p>本文将具体讲解和实验基于特征点的AR技术 </p></blockquote><h2 id="Demo-演示"><a href="#Demo-演示" class="headerlink" title="Demo 演示"></a>Demo 演示</h2><h3 id="演示环境"><a href="#演示环境" class="headerlink" title="演示环境"></a>演示环境</h3><ul><li><p>iphone 8 手机：App Store 下载 Focus [+] # 手动对焦拍摄</p></li><li><p>计算机：vim，python和 conda</p></li><li><p>OpenCV 棋盘标定纸</p></li></ul><p><img src="/image/使用Opencv+Python的AR小demo/calibration_img/9.png" alt=""></p><h3 id="准备图片"><a href="#准备图片" class="headerlink" title="准备图片"></a>准备图片</h3><ul><li>参考图片</li></ul><p><img src="/image/使用Opencv+Python的AR小demo/referenceImage.png" alt=""></p><ul><li>用例图片</li></ul><p><img src="/image/使用Opencv+Python的AR小demo/sourceImage.png" alt=""></p><h3 id="相机标定原理"><a href="#相机标定原理" class="headerlink" title="相机标定原理"></a>相机标定原理</h3><p>从世界坐标系转换到图像坐标系，求投影矩阵 $P$ 的过程</p><p>分为两步</p><ul><li>从世界坐标系转换为相机坐标系，这一步是三维点到三维点的转换，包括 $R,t$ （相机外参）等参数</li></ul><p><img src="/image/使用Opencv+Python的AR小demo/5.png" alt="5"></p><script type="math/tex; mode=display">\widetilde{X}_{c a m}=R(\widetilde{X}-\widetilde{C})</script><pre><code>* $ \widetilde&#123;X&#125; $ 为 $X$ 在世界坐标中的位置* $ R $ 为旋转矩阵* $ \widetilde&#123;C&#125; $ 为相机原点 $C$ 所在世界坐标中的位置* $ \widetilde&#123;X&#125;_&#123;c a m&#125; $ 为 $ X $ 在相机坐标系中的位置</code></pre><ul><li>从相机坐标系转换为图像坐标系，这一步是三维点到二维点的转换，包括 $K$（相机内参）等参数</li></ul><p><img src="/image/使用Opencv+Python的AR小demo/9.png" alt=""></p><ul><li><p>$C$为相机的中心点，也是相机坐标系的中心点</p></li><li><p>$Z$为相机的主轴</p></li><li><p>$p$为相机的像平面，也就是图片坐标系所在的二维平面</p></li><li><p>$C$ 点到 $p$点的距离$f$，为相机的焦距</p></li></ul><p>可得到</p><script type="math/tex; mode=display">\begin{aligned}x &=f X / Z \\y &=f Y / Z \\(X, \quad Y, \quad Z) & \mapsto(f X / Z, \quad f Y / Z)\end{aligned}</script><p>由图可知偏移量</p><p><img src="/image/使用Opencv+Python的AR小demo/10.png" alt=""></p><script type="math/tex; mode=display">(X, \quad Y, \quad Z) \mapsto\left(f X / Z+p_{x}, \quad f Y / Z+p_{y}\right)</script><p>矩阵形式为</p><script type="math/tex; mode=display">\left(\begin{array}{c}X \\Y \\Z \\    1\end{array}\right) \mapsto\left(\begin{array}{c}f X+Z p_{x} \\f Y+Z p_{y} \\Z\end{array}\right)=\left[\begin{array}{ccc}f & p_{x} & 0 \\& f & p_{y} & 0 \\& & 1 & 0\end{array}\right]\left(\begin{array}{c}X \\Y \\Z \\1\end{array}\right)</script><p>化简得</p><script type="math/tex; mode=display">\left(\begin{array}{c}f X+Z p_{x} \\f Y+Z p_{y} \\Z\end{array}\right)=\left[\begin{array}{cc}f & p_{x} \\& f & p_{y} \\& & 1\end{array}\right]\left[\begin{array}{llll}1 & & & 0 \\& 1 & & 0 \\& & 1 & 0\end{array}\right]\left(\begin{array}{l}X \\Y \\Z \\1\end{array}\right)</script><p>则</p><script type="math/tex; mode=display">K=\left[\begin{array}{ccc}f & & p_{x} \\& f & p_{y} \\& & 1\end{array}\right]</script><p>设旋转矩阵 $R$ 为单位矩阵 $I$，平移矩阵 $t$ 为0</p><script type="math/tex; mode=display">\begin{aligned}P &=K[R \mid t] \\&=K[I \mid 0]\end{aligned}</script><blockquote><p>畸变参数本例未考虑到，不作讨论</p></blockquote><h3 id="获得相机标定矩阵"><a href="#获得相机标定矩阵" class="headerlink" title="获得相机标定矩阵"></a>获得相机标定矩阵</h3><h4 id="手动对焦，固定焦距，拍摄各个方面的标定板"><a href="#手动对焦，固定焦距，拍摄各个方面的标定板" class="headerlink" title="手动对焦，固定焦距，拍摄各个方面的标定板"></a>手动对焦，固定焦距，拍摄各个方面的标定板</h4><p><img src="/image/使用Opencv+Python的AR小demo/11.png" alt=""></p><h4 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h4><ul><li>提取角点 本例使用的标定板来自 [calib](  有13 * 9 个角点</li><li>提取亚像素角点 提高精度</li><li>角点绘制</li><li>标定</li></ul><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p><img src="/image/使用Opencv+Python的AR小demo/13.png" alt=""></p><p>得到 iphone 8 的相机标定矩阵为 (代码见camera_calibration.py)</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1.09358481e+03</span> <span class="number">0.00000000e+00</span> <span class="number">5.12119524e+02</span>]</span><br><span class="line">[<span class="number">0.00000000e+00</span> <span class="number">1.08983166e+03</span> <span class="number">6.61345525e+02</span>]</span><br><span class="line">[<span class="number">0.00000000e+00</span> <span class="number">0.00000000e+00</span> <span class="number">1.00000000e+00</span>]]</span><br></pre></td></tr></table></figure><h3 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h3><h4 id="特征检测"><a href="#特征检测" class="headerlink" title="特征检测"></a>特征检测</h4><p>使用ORB法进行特征检测，ORB基于FAST算法，FAST算法的原理如下</p><p><img src="/image/使用Opencv+Python的AR小demo/12.png" alt=""></p><p>任选图像中的一点 $P$，以该点为圆形，$r$为半径确定一个圆，在圆上均匀取$m$个像素点，设定一个阈值$t$，如果$m$个像素点中，有连续$N$个像素点的大小均大于或小于$t$，则这个点就是角点。但是在进行FAST进行角点检测时，边缘位置的部分易混淆，针对这种情况，ORB算法通过增加图像金字塔和计算角度的方法，用Harris角点检测器把$N$个关键点进行等级排序，使用者可提取前n个自己需要的点。不同的是，ORB在进行特征点匹配时，检测出的角点需要满足尺度不变形和旋转不变性。</p><ul><li>尺度不变形</li></ul><p>通过对初始图像的按1/2的比例不断下采样(即按1/2的比例不断缩放)，得到一系列图像，形成图像金字塔。对每层图像，进行FAST角点检测</p><ul><li>旋转不变形</li></ul><p>采用灰度质心法进行计算每个特征点的主方向</p><script type="math/tex; mode=display">\mathrm{m}_{p q}=\sum_{x, y} x^{p} y^{q} I(x, y)</script><p>其中$x,y$分别表示像素点周围圆上所选取点的横坐标和纵坐标，$I(x,y)$表示灰度值大小，$p,q$表示指数，角度计算的方法如下</p><script type="math/tex; mode=display">\theta=\operatorname{atan} 2(\mathrm{m_{01}}, \mathrm{m_{10}})</script><h4 id="特征描述"><a href="#特征描述" class="headerlink" title="特征描述"></a>特征描述</h4><p>ORB法采用BRIEF描述子计算算法实现，BRIEF算法可分为两步</p><ul><li>特征点大小的对比</li></ul><p>以特征点为中心，取邻域窗口，在窗口上选择两个点p(x)和p(y)，比较两个点像素值的大小</p><script type="math/tex; mode=display">\tau(p ; x, y):=\left\{\begin{array}{cc}1 & if\quad p(x)<p(y) \\0 & \text { otherwise }\end{array}\right.</script><ul><li>重复第一步进行像素值大小的比较，形成二进制编码</li></ul><p>OBR算法对BRIEF有两种改变，分别为 steer BRIEF 和 rBRIEF</p><ul><li>steer BRIEF具备旋转不变形的特征，已知 $ /theta $，将该点周围的点旋转 $ /theta $ 度，得到新的点对<script type="math/tex; mode=display">D_{\theta}=R_{\theta} D</script></li></ul><p>$R$ 为旋转矩阵<br>旋转后，在新的位置上比较像素值的大小，得到描述子</p><ul><li>rBRIEF算法通过改变描述子的计算方法，进一步减弱同一图像中特征点的描述子的相关性，对每个角点，考虑其 $31X31$ 的邻域，使用领域中每个点周围的 $5X5$ 的邻域的像素值平均值作为该点的像素值，进而比较点对的大小。上面计算可得到 $(31-5+1)*(31-5+1)=729$ 个子窗口，提取点对的方法有 $729X728=265356$ 种，通过在这 $265356$ 中方法中选取 $256$ 种取法，形成描述子</li></ul><p>结果</p><p><img src="/image/使用Opencv+Python的AR小demo/1.png" alt=""></p><h4 id="特征匹配"><a href="#特征匹配" class="headerlink" title="特征匹配"></a>特征匹配</h4><p>本例使用 Brute-Force Matcher 进行特征匹配，也就是暴力匹配</p><p>结果</p><p><img src="/image/使用Opencv+Python的AR小demo/2.png" alt=""></p><h4 id="映射"><a href="#映射" class="headerlink" title="映射"></a>映射</h4><p>将参考图像表面的平面的点映射到用例图像的平面上，也就是单应性变换，单应性变换是将一个平面（齐次坐标）中的点映射到另一个平面的二维投影变换</p><script type="math/tex; mode=display">\left[\begin{array}{l}x^{\prime} \\y^{\prime} \\z^{\prime}\end{array}\right]=\left[\begin{array}{lll}h_{1} & h_{2} & h_{3} \\h_{4} & h_{5} & h_{6} \\h_{7} & h_{8} & h_{9}\end{array}\right]\left[\begin{array}{l}x \\y \\z\end{array}\right]</script><p>从两个图像中传递点集，它将找到该对象的透视变换，至少需要四个正确的点才能找到转换，但两幅图像之间的单应性变换包含不适合的点。会导致匹配时出现错误，影响结果，使用 RANSAC 迭代法验证拟合</p><p>结果</p><p><img src="/image/使用Opencv+Python的AR小demo/3.webp" alt=""></p><h4 id="3D-绘制"><a href="#3D-绘制" class="headerlink" title="3D 绘制"></a>3D 绘制</h4><p>使用 <a href="https://github.com/yarolig/OBJFileLoader">yarolig的OBJFileLoader</a> 加载 3D obj 模型 (代码见 objloader_simple.py)</p><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><p><img src="/image/使用Opencv+Python的AR小demo/4.webp" alt="4"></p><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">ar_python_opencv.py</summary><pre><code class="python">import cv2import numpy as npimport mathimport matplotlib.pyplot as pltfrom objloader_simple import *referenceImage = cv2.imread('/home/pacaep/Tests/OpenCvArDemo/img/referenceImage.webp',0)plt.imshow(referenceImage, cmap = 'gray')sourceImage = cv2.imread('/home/pacaep/Tests/OpenCvArDemo/img/sourceImage.webp',0)plt.imshow(sourceImage, cmap='gray')orb = cv2.ORB_create()referenceImagePts = orb.detect(referenceImage, None)sourceImagePts = orb.detect(sourceImage, None)referenceImagePts, referenceImageDsc = orb.compute(referenceImage, referenceImagePts)sourceImagePts, sourceImageDsc = orb.compute(sourceImage, sourceImagePts)referenceImageFeatures = cv2.drawKeypoints(referenceImage, referenceImagePts,                                                                                        referenceImage, color = (0,255,0), flags = 0)sourceImageFeatures = cv2.drawKeypoints(sourceImage, sourceImagePts,                                                                                        sourceImage, color = (0,255,0), flags = 0)plt.figure(figsize=(10,5))plt.subplot(1,2,1)plt.axis("off")plt.imshow(referenceImageFeatures, cmap = 'gray')plt.title('Reference Image Features')plt.subplot(1,2,2)plt.axis("off")plt.imshow(sourceImageFeatures,cmap='gray')plt.title('Source Image Features')plt.tight_layout()plt.show()MIN_MATCHES = 30bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)referenceImagePts, referenceImageDsc = orb.detectAndCompute(referenceImage, None)sourceImagePts, sourceImageDsc = orb.detectAndCompute(sourceImage, None)matches = bf.match(referenceImageDsc, sourceImageDsc)matches = sorted(matches, key = lambda x: x.distance)if len(matches) > MIN_MATCHES:    idxPairs = cv2.drawMatches(referenceImage, referenceImagePts,                                sourceImage, sourceImagePts, matches[:MIN_MATCHES],0,flags =2)    plt.figure(figsize=(12,6))    plt.axis('off')    plt.imshow(idxPairs, cmap='gray')    plt.title('Matching between features')    plt.show()else:    print("Not enough matches have been found - %d/%d" %(len(matches), MIN_MATCHES))    matchesMask = Noneif len(matches) > MIN_MATCHES:    sourcePoints = np.float32([referenceImagePts[m.queryIdx].pt for m in matches]).reshape(-1,1,2)    destinationPoints = np.float32([sourceImagePts[m.trainIdx].pt for m in matches]).reshape(-1,1,2)    homography, mask = cv2.findHomography(sourcePoints, destinationPoints, cv2.RANSAC, 5.0)    matchesMask = mask.ravel().tolist()    h, w = referenceImage.shape    corners = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)    transformedCorners = cv2.perspectiveTransform(corners, homography)    sourceImageMarker = cv2.polylines(sourceImage, [np.int32(transformedCorners)], True,                                      255, 5, cv2.LINE_AA)else:    print("Not enough matches are found - %d/%d" % (len(matches), MIN_MATCHES))    matchesMask = NonedrawParameters = dict(matchColor=(0, 255, 0), singlePointColor=None,                      matchesMask=matchesMask, flags=2)result = cv2.drawMatches(referenceImage, referenceImagePts, sourceImageMarker,                         sourceImagePts, matches, None, **drawParameters)plt.figure(figsize=(12, 6))plt.imshow(result, cmap='gray')plt.show()camera_parameters = np.array([[1108.38916, 0,          513.796472],                              [0,          1111.41724, 661.637500],                              [0,          0,          1]])obj = OBJ('/home/pacaep/Tests/OpenCvArDemo/models/fox.obj', swapyz = True)def projection_matrix(camera_parameters, homography):    homography = homography * (-1)    rot_and_transl = np.dot(np.linalg.inv(camera_parameters), homography )    col_1 = rot_and_transl[:,0]    col_2 = rot_and_transl[:,1]    col_3 = rot_and_transl[:,2]    l = math.sqrt(np.linalg.norm(col_1, 2) * np.linalg.norm(col_2, 2))    rot_1 = col_1 / l    rot_2 = col_2 / l    translation = col_3 / l    c = rot_1 + rot_2    p = np.cross(rot_1, rot_2)    d = np.cross(c,p)    rot_1 = np.dot(c/np.linalg.norm(c,2) + d / np.linalg.norm(d,2), 1/math.sqrt(2))    rot_2 = np.dot(c/np.linalg.norm(c,2) - d / np.linalg.norm(d,2), 1/math.sqrt(2))    rot_3 = np.cross(rot_1, rot_2)    projection = np.stack((rot_1, rot_2, rot_3, translation)).T    return np.dot(camera_parameters, projection)def render(img, obj, projection, model, color=False):    vertices = obj.vertices    scale_matrix = np.eye(3)*6    h,w = model.shape    for face in obj.faces:        face_vertices = face[0]        points = np.array([vertices[vertex -1] for vertex in face_vertices])        points = np.dot(points, scale_matrix)        points = np.array([[p[0] + w / 2, p[1] + h/2, p[2]] for p in points])        dst = cv2.perspectiveTransform(points.reshape(-1,1,3), projection)        imgpts = np.int32(dst)        cv2.fillConvexPoly(img, imgpts, (80, 217, 81))    return imgsourcePoints = np.float32([referenceImagePts[m.queryIdx].pt for m in matches]).reshape(-1,1,2)destinationPoints = np.float32([sourceImagePts[m.trainIdx].pt for m in matches]).reshape(-1,1,2)homography, _ = cv2.findHomography(sourcePoints,destinationPoints, cv2.RANSAC, 5.0)matchesMask = mask.ravel().tolist()h, w = referenceImage.shapecorners = np.float32([[0,0],[0,h-1],[w-1,h-1],[w-1,0]]).reshape(-1,1,2)transformedCorners = cv2.perspectiveTransform(corners, homography)frame = cv2.polylines(sourceImage, [np.int32(transformedCorners)], True, 255,3,cv2.LINE_AA)projection = projection_matrix(camera_parameters, homography)frame = render(frame, obj, projection, referenceImage, True)plt.figure(figsize=(6,12))plt.imshow(frame, cmap='gray')plt.show()</code></pre></details><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">camera_calibration.py</summary><pre><code class="python">import cv2import numpy as npimport globcriteria = (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, 30, 0.001)objp = np.zeros((9 * 13, 3), np.float32)objp[:, :2] = np.mgrid[0:13, 0:9].T.reshape(-1, 2)obj_points = []img_points = []images = glob.glob("/home/pacaep/Tests/OpenCvArDemo/calibration_img/*.webp")i=0;for fname in images:    img = cv2.imread(fname)    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)    size = gray.shape[::-1]    ret, corners = cv2.findChessboardCorners(gray, (13, 9), None)    if ret:        obj_points.append(objp)        corners2 = cv2.cornerSubPix(gray, corners, (5, 5), (-1, -1), criteria)        if [corners2]:            img_points.append(corners2)        else:            img_points.append(corners)        cv2.drawChessboardCorners(img, (13, 9), corners, ret)        i+=1;        cv2.imwrite('conimg'+str(i)+'.webp', img)        cv2.waitKey(1500)print(len(img_points))cv2.destroyAllWindows()ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(obj_points, img_points, size, None, None)print("ret:", ret)print("mtx:\n", mtx)print("dist:\n", dist)print("rvecs:\n", rvecs)print("tvecs:\n", tvecs )print("-----------------------------------------------------")img = cv2.imread(images[2])h, w = img.shape[:2]newcameramtx, roi = cv2.getOptimalNewCameraMatrix(mtx,dist,(w,h),1,(w,h))print (newcameramtx)print("------------------use undistort-------------------")dst = cv2.undistort(img,mtx,dist,None,newcameramtx)x,y,w,h = roidst1 = dst[y:y+h,x:x+w]cv2.imwrite('calibresult.webp', dst1)print ("dst:", dst1.shape)</code></pre></details><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">objloader_simple.py</summary><pre><code class="python">class OBJ:    def __init__(self, filename, swapyz=False):        self.vertices = []        self.normals = []        self.texcoords = []        self.faces = []        material = None        for line in open(filename, "r"):            if line.startswith('#'): continue            values = line.split()            if not values: continue            if values[0] == 'v':                v = list(map(float, values[1:4]))                if swapyz:                    v = v[0], v[2], v[1]                self.vertices.append(v)            elif values[0] == 'vn':                v = list(map(float, values[1:4]))                if swapyz:                    v = v[0], v[2], v[1]                self.normals.append(v)            elif values[0] == 'vt':                self.texcoords.append(map(float, values[1:3]))            elif values[0] == 'f':                face = []                texcoords = []                norms = []                for v in values[1:]:                    w = v.split('/')                    face.append(int(w[0]))                    if len(w) >= 2 and len(w[1]) > 0:                        texcoords.append(int(w[1]))                    else:                        texcoords.append(0)                    if len(w) >= 3 and len(w[2]) > 0:                        norms.append(int(w[2]))                    else:                        norms.append(0)                self.faces.append((face, norms, texcoords))</code></pre></details>]]></content>
      
      
      <categories>
          
          <category> 图像处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>labelImg闪退错误修复</title>
      <link href="/2021/12/05/labelImg%E9%97%AA%E9%80%80%E9%94%99%E8%AF%AF%E4%BF%AE%E5%A4%8D/"/>
      <url>/2021/12/05/labelImg%E9%97%AA%E9%80%80%E9%94%99%E8%AF%AF%E4%BF%AE%E5%A4%8D/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使用labelImg时，在未选中数据标签时按<code>Ctrl+D</code>时会闪退，在终端中有以下报错</p><span id="more"></span><p><img src="/image/labelImg闪退错误修复/1.webp" alt="1"></p><h2 id="修复"><a href="#修复" class="headerlink" title="修复"></a>修复</h2><p>在<code>/usr/lib/python3.9/site-packages/labelImg/labelImg.py</code> 第784行，没有数据标签选中时<code>shape</code>对象没有<code>paint_label</code>属性，添加一个条件过滤就好</p><p>第783行<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_label</span>(<span class="params">self, shape</span>):</span><br><span class="line">    shape.paint_label = self.display_label_option.isChecked()</span><br><span class="line">    item = HashableQListWidgetItem(shape.label)</span><br><span class="line">    item.setFlags(item.flags() | Qt.ItemIsUserCheckable)</span><br><span class="line">    item.setCheckState(Qt.Checked)</span><br><span class="line">    item.setBackground(generate_color_by_text(shape.label))</span><br><span class="line">    self.items_to_shapes[item] = shape</span><br><span class="line">    self.shapes_to_items[shape] = item</span><br><span class="line">    self.label_list.addItem(item)</span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> self.actions.onShapesPresent:</span><br><span class="line">        action.setEnabled(<span class="literal">True</span>)</span><br><span class="line">    self.update_combo_box()</span><br></pre></td></tr></table></figure></p><p>改为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">add_label</span>(<span class="params">self, shape</span>):</span><br><span class="line">    <span class="keyword">if</span> shape <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;add empty label&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    shape.paint_label = self.display_label_option.isChecked()</span><br><span class="line">    item = HashableQListWidgetItem(shape.label)</span><br><span class="line">    item.setFlags(item.flags() | Qt.ItemIsUserCheckable)</span><br><span class="line">    item.setCheckState(Qt.Checked)</span><br><span class="line">    item.setBackground(generate_color_by_text(shape.label))</span><br><span class="line">    self.items_to_shapes[item] = shape</span><br><span class="line">    self.shapes_to_items[shape] = item</span><br><span class="line">    self.label_list.addItem(item)</span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> self.actions.onShapesPresent:</span><br><span class="line">        action.setEnabled(<span class="literal">True</span>)</span><br><span class="line">    self.update_combo_box()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 懵逼的深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> labelImg </tag>
            
            <tag> 数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2021亚太数学建模竞赛A题简要思路</title>
      <link href="/2021/11/29/2021%E4%BA%9A%E5%A4%AA%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9BA%E9%A2%98/"/>
      <url>/2021/11/29/2021%E4%BA%9A%E5%A4%AA%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9BA%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>小记一下第一次参加数模赛</p><span id="more"></span><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p><img src="/image/2021亚太数学建模竞赛A题/mind.png" alt="1"></p><h3 id="问题一"><a href="#问题一" class="headerlink" title="问题一"></a>问题一</h3><p>用高斯滤波对图像 Pic1<em>1 和 Pic1_2 进行降噪处理，用灰度值开运算降低图像 Pic1</em> 3的噪声；对三个图像进行二值化处理，将灰度图像转化为二值化图像，其次用圆形元素对区域进行膨胀，再用改进的 canny 方法提取亚像素精确边缘，对 XLD 轮廓用最小二乘近似线平滑，再根据斜率以及拐点将 XLD 轮廓分割为直线段和圆弧段或椭圆弧，最后根据轮廓的相对位置对轮廓进行排序，并获取XLD轮廓的长度及区域位置</p><h3 id="问题二"><a href="#问题二" class="headerlink" title="问题二"></a>问题二</h3><p>为达到自标定消除畸变的目的，通过最小二乘近似线平滑、均值滤波降噪、局部阈值分割等算法，将输出的XLD轮廓分割，连接两个标志性的对象元组重复问题一的步骤得到最佳的校准径向畸变参数，用来校准图像的径向畸变由轮廓的相对位置对轮廓进行排序并获取XLD轮廓的长度，最后根据轮廓长度以及实际的物理坐标计算出6个轮廓的真实长度</p><h3 id="问题三"><a href="#问题三" class="headerlink" title="问题三"></a>问题三</h3><p>依据问题一的模型将 XLD 轮廓分割为直线段和圆弧段或椭圆弧。当分割轮廓为直线段时，用最小二乘法进行直线拟合并用多边形生成XLD轮廓；圆弧段或圆，用 algebraic 方法使轮廓点与生成的圆之间的代数距离最小化；椭圆弧或椭圆，基于 tukey 的算法对轮廓点进行加权并忽略异常值</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><h3 id="问题一-1"><a href="#问题一-1" class="headerlink" title="问题一"></a>问题一</h3><p><img src="/image/2021亚太数学建模竞赛A题/ContoursSplit1.png" alt="1_1"></p><p><img src="/image/2021亚太数学建模竞赛A题/ContoursSplit2.png" alt="1_2"></p><p><img src="/image/2021亚太数学建模竞赛A题/ContoursSplit3.png" alt="1_3"></p><h3 id="问题二-1"><a href="#问题二-1" class="headerlink" title="问题二"></a>问题二</h3><p><img src="/image/2021亚太数学建模竞赛A题/Borders.webp" alt="2"></p><h3 id="问题三-1"><a href="#问题三-1" class="headerlink" title="问题三"></a>问题三</h3><p><img src="/image/2021亚太数学建模竞赛A题/SortedContours.webp" alt="3"></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">问题一</summary><pre><code class="halcon">read_image (Img, 'Pic1_1.bmp')dev_close_window ()get_image_size (Img, Width, Height)dev_open_window (0, 0, Width, Height, 'black', WindowHandle)gauss_filter(Img, ImgGauss, 11)binary_threshold (ImgGauss, Region,  'smooth_histo', 'dark', UsedThreshold)boundary (Region, RegionBorder, 'inner')dilation_circle (RegionBorder, RegionDread_image (Img, 'Pic1_1.bmp')dev_close_window ()get_image_size (Img, Width, Height)dev_open_window (0, 0, Width, Height, 'black', WindowHandle)gauss_filter(Img, ImgGauss, 11)binary_threshold (ImgGauss, Region,  'smooth_histo', 'dark', UsedThreshold)boundary (Region, RegionBorder, 'inner')dilation_circle (RegionBorder, RegionDilation, 2.5)reduce_domain (ImgGauss, RegionDilation, ImgReduced)edges_sub_pix (ImgReduced, Edges, 'canny', 2, 20, 60)smooth_contours_xld(Edges, ImgSmoothed, 9)segment_contours_xld (ImgSmoothed, ContoursSplit, 'lines_circles', 5, 4, 3)count_obj (ContoursSplit, Number)dev_display (Img)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)dev_display (ContoursSplit)sort_contours_xld (ContoursSplit, SortedContours, 'upper_left', 'true', 'column')count_obj (ContoursSplit, Number)length_xld(SortedContours,Length)gen_empty_obj (Line)for i := 1 to 24 by 1    select_obj (SortedContours, ObjectSelected, i)    get_contour_xld (ObjectSelected, Row, Col)    gen_region_polygon (Region, Row,Col)    concat_obj (Line, Region, Line)endforX := ColY := Rowilation, 2.5)reduce_domain (ImgGauss, RegionDilation, ImgReduced)edges_sub_pix (ImgReduced, Edges, 'canny', 2, 20, 60)smooth_contours_xld(Edges, ImgSmoothed, 9)segment_contours_xld (ImgSmoothed, ContoursSplit, 'lines_circles', 5, 4, 3)count_obj (ContoursSplit, Number)dev_display (Img)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)dev_display (ContoursSplit)sort_contours_xld (ContoursSplit, SortedContours, 'upper_left', 'true', 'column')count_obj (ContoursSplit, Number)length_xld(SortedContours,Length)gen_empty_obj (Line)for i := 1 to 24 by 1    select_obj (SortedContours, ObjectSelected, i)    get_contour_xld (ObjectSelected, Row, Col)    gen_region_polygon (Region, Row,Col)    concat_obj (Line, Region, Line)endforX := ColY := Rowread_image (Img, 'Pic1_2.bmp')dev_close_window ()get_image_size (Img, Width, Height)dev_open_window (0, 0, Width, Height, 'black', WindowHandle)gauss_filter(Img, ImgGauss, 11)binary_threshold (ImgGauss, Region,  'smooth_histo', 'dark', UsedThreshold)boundary (Region, RegionBorder, 'inner')dilation_circle (RegionBorder, RegionDilation, 2.5)reduce_domain (ImgGauss, RegionDilation, ImgReduced)edges_sub_pix (ImgReduced, Edges, 'canny', 2, 20, 60)smooth_contours_xld(Edges, ImgSmoothed, 9)segment_contours_xld (ImgSmoothed, ContoursSplit, 'lines_circles', 5, 4, 3)count_obj (ContoursSplit, Number)dev_display (Img)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)dev_display (ContoursSplit)sort_contours_xld (ContoursSplit, SortedContours, 'upper_left', 'true', 'column')count_obj (ContoursSplit, Number)length_xld(SortedContours,Length)gen_empty_obj (Line)for i := 1 to 84 by 1    select_obj (SortedContours, ObjectSelected, i)    get_contour_xld (ObjectSelected, Row, Col)    gen_region_polygon (Region, Row,Col)    concat_obj (Line, Region, Line)endforx := round(Col)y := round(Row)read_image (Img, 'Pic1_3.bmp')dev_close_window ()get_image_size (Img, Width, Height)dev_open_window (0, 0, Width, Height, 'black', WindowHandle)gray_opening_shape (Img, ImgOpen, 16, 16, 'octagon')mean_image(ImgOpen, ImgMean, 9.3, 9.3)binary_threshold (ImgMean, Region,  'smooth_histo', 'dark',  UsedThreshold)boundary (Region, RegionBorder, 'inner')dilation_circle (RegionBorder, RegionDilation, 4.6)reduce_domain (ImgMean, RegionDilation, ImgReduced)edges_sub_pix (ImgReduced, Edges, 'canny', 2, 20, 60)union_cotangential_contours_xld (Edges, UnionContours, 2, 5, 3.14, 25.0, 10, 2.0, 'attr_forget')smooth_contours_xld(UnionContours, ImgSmoothed, 9)segment_contours_xld (ImgSmoothed, ContoursSplit, 'lines_circles', 5, 4, 3)count_obj (ContoursSplit, Number)dev_display (Img)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)dev_display (ContoursSplit)sort_contours_xld (ContoursSplit, SortedContours, 'upper_left', 'true', 'column')count_obj (ContoursSplit, Number)length_xld(SortedContours,Length)gen_empty_obj (Line)for i := 1 to 120 by 1    select_obj (SortedContours, ObjectSelected, i)    get_contour_xld (ObjectSelected, Row, Col)    gen_region_polygon (Region, Row,Col)    concat_obj (Line, Region, Line)endforx := round(Col)y := round(Row)</code></pre></details><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">问题二</summary><pre><code class="halcon">dev_update_off ()read_image (Img, 'Pic2_4.bmp')dev_close_window ()dev_open_window_fit_image (Img, 0, 0, -1, -1, WindowHandle)set_display_font (WindowHandle, 16, 'mono', 'true', 'false')dev_display (Img)disp_message (WindowHandle, 'Img with radial distortions', 'window', 0, 0, 'black', 'true')disp_continue_message (WindowHandle, 'black', 'true')gen_empty_obj (Borders)for N := 1 to 4 by 1    read_image (Img, 'Pic2_' + N$'d' + '.bmp')    edges_sub_pix (Img, ImgBorders, 'canny', 1, 10, 40)    smooth_contours_xld(ImgBorders, ImgSmoothed, 9)    segment_contours_xld (ImgSmoothed, SplitBorders, 'lines_circles', 5, 4, 2)    select_shape_xld (SplitBorders, SelectedBorders, 'contlength', 'and', 30, 100000)    concat_obj (Borders, SelectedBorders, Borders)    dev_display (Img)    dev_set_colored (12)    dev_display (SelectedBorders)    disp_message (WindowHandle, 'Edges extracted from image ' + N$'d', 'window', 0, 0, 'black', 'true')endfordev_clear_window ()dev_set_colored (12)dev_display (Borders)disp_message (WindowHandle, 'Collected edges from multiple images', 'window', 0, 0, 'black', 'true')disp_continue_message (WindowHandle, 'black', 'true')dev_clear_window ()disp_message (WindowHandle, 'Performing self-calibration...', 'window', 0, 0, 'black', 'true')radial_distortion_self_calibration (Borders, CalibrationBorders, 1296, 972, 0.08, 42, 'division', 'fixed', 0, CamParMultiImage)dev_clear_window ()dev_set_colored (12)dev_display (CalibrationBorders)disp_message (WindowHandle, 'Edges used for calibration', 'window', 0, 0, 'black', 'true')disp_continue_message (WindowHandle, 'black', 'true')change_radial_distortion_cam_par ('fixed', CamParMultiImage, 0, CamParMultiImageRect)read_image (ImgCir, 'Pic2_1.bmp')get_domain (ImgCir, Domain)change_radial_distortion_image (ImgCir, Domain, ImgCirRectified, CamParMultiImage, CamParMultiImageRect)get_image_size (ImgCir, WidthCir, HeightCir)dev_open_window_fit_image (ImgCir, 0, 0, -1, -1, WindowHandleCir)set_display_font (WindowHandleCir, 16, 'mono', 'true', 'false')dev_display (ImgCir)mean_image(ImgCir, MeanCir, 61, 61)dyn_threshold(ImgCir, MeanCir, SegCir, 5, 'dark')fill_up(SegCir, SegFillUpCir)connection(SegFillUpCir, segConnectCir)select_shape(segConnectCir, SelectedRegionsCir, ['area','circularity'], 'and', [500, 0.7], [20000, 1])smallest_circle(SelectedRegionsCir, RowCir, ColumnCir, RadiusCir)area_center(SelectedRegionsCir, AreaCir, Row1Cir, Column1Cir)CRadius:=sqrt(AreaCir/3.1415926)RowSize := mean(0.5 / CRadius)read_image (Img, 'Pic2_4.bmp')get_domain (Img, Domain)change_radial_distortion_image (Img, Domain, ImageRectified, CamParMultiImage, CamParMultiImageRect)dev_close_window ()get_image_size (ImageRectified, Width, Height)dev_open_window (0, 0, Width, Height, 'black', WindowHandle)gauss_filter(Img, ImgGauss, 11)binary_threshold (ImgGauss, Region,  'smooth_histo', 'dark', UsedThreshold)boundary (Region, RegionBorder, 'inner')dilation_circle (RegionBorder, RegionDilation, 2.5)reduce_domain (ImgGauss, RegionDilation, ImgReduced)edges_sub_pix (ImgReduced, Borders, 'canny', 2, 20, 60)smooth_contours_xld(Borders, ImgSmoothed, 9)dev_display (ImgSmoothed)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)sort_contours_xld (ImgSmoothed, SortedContours, 'upper_left', 'true', 'column')count_obj (ImgSmoothed, Number)length_xld(SortedContours,Length)RealSize := Length * RowSizeAllSzie := sum(RealSize)</code></pre></details><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">问题三</summary><pre><code class="halcon">read_contour_xld_dxf (DxfContours, 'EdgeContour.dxf', [], [], DxfStatus)segment_contours_xld (DxfContours, ContoursSplit, 'lines_circles', 5, 4, 3)sort_contours_xld (ContoursSplit, SortedContours, 'upper_left', 'true', 'column')count_obj (SortedContours, Number)dev_display (DxfContours)dev_set_draw ('margin')dev_set_color ('white')dev_update_window ('off')dev_set_colored (12)dev_set_line_width (3)dev_display (SortedContours)dev_set_colored (3)dev_set_line_width (3)ColBL := []RowsBL := []ColEL := []RowsEL := []Lines := []ColsC := []RowsC := []RadiiC := []PhisC := []ColsSC := []RowsSC := []ColsEC := []RowsEC := []ColsE := []RowsE := []RadiiE := []PhisE := []ColsSE := []RowsSE := []ColsEE := []RowsEE := []k:=0J := []for I := 1 to Number by 1   select_obj (SortedContours, ObjectSelected, I)   get_contour_global_attrib_xld (ObjectSelected, 'cont_approx', Attrib)    if(Attrib == -1)        k := k+1        J := [J,k]        fit_line_contour_xld (ObjectSelected, 'tukey', -1, 2, 6, 2, RowBegin, ColBegin, RowEnd, ColEnd, Nr, Nc, Dist)        gen_contour_polygon_xld (ContLine, [RowBegin,RowEnd], [ColBegin,ColEnd])        RowsBL := [RowsBL,RowBegin]        RowsEL := [RowsEL,RowEnd]        ColBL := [ColBL,ColBegin]        ColEL := [ColEL,ColEnd]        length_xld(ContLine,LengthL)        Lines := [Lines, LengthL]        dev_display (ContLine)    elseif(Attrib == 1)        J := [J,0]        fit_circle_contour_xld (ObjectSelected, 'algebraic', -1, 6, 0, 3, 2, RowC, ColumnC, RadiusC, StartPhiC, EndPhiC, PointOrderC)        gen_circle_contour_xld (ContCircle, RowC, ColumnC, RadiusC, 0, rad(360), PointOrderC, 1.0)        RowsC := [RowsC,RowC]        ColsC := [ColsC,ColumnC]        RadiiC := [RadiiC,RadiusC]        PhisC := [PhisC,(EndPhiC-StartPhiC)*180/3.14159]        ColsSC := [ColsSC,ColumnC+RadiusC*cos(StartPhiC)]        RowsSC := [RowsSC,RowC+RadiusC*sin(StartPhiC)]        ColsEC := [ColsEC,ColumnC+RadiusC*cos(EndPhiC)]        RowsEC := [RowsEC,RowC+RadiusC*sin(EndPhiC)]        dev_display (ContCircle)    else        fit_ellipse_contour_xld(ObjectSelected, 'ftukey',-1, 6, 0, 2, 3, 2, RowE, ColumnE, RadiusE, RaE, RbE, StartPhiE, EndPhiE, PointOrderE)        gen_ellipse_contour_xld(ContEllipse, RowE, ColumnE, RadiusE, RaE, RbE, 0, rad(360), PointOrderE, 1.0)        RowsE := [RowsE,RowE]        ColsE := [ColsE,ColumnE]        RadiiE := [RadiiE,RadiusE]        PhisE := [PhisE,(EndPhiE-StartPhiE)*180/3.14159]        ColsSE := [ColsSE,ColumnE+RadiusE*cos(StartPhiE)]        RowsSE := [RowsSE,RowE+RadiusE*sin(StartPhiE)]        ColsEE := [ColsEE,ColumnE+RadiusE*cos(EndPhiE)]        RowsEE := [RowsEE,RowE+RadiusE*sin(EndPhiE)]        dev_display (ContEllipse)        J:=[J,-1]    endifendfor</code></pre></details><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">matlab代码</summary><pre><code class="matlab">clear;clc;a = xlsread('Edge1.xlsx');b = xlsread('Edge2.xlsx');FID = dxf_open('EdgeContour.dxf');dxf_polyline(FID, a(:,1), a(:,2), zeros(length(a),1));dxf_polyline(FID, b(:,1), b(:,2), zeros(length(b),1));dxf_close(FID);</code></pre></details><blockquote><p>DXFLib 库地址: <a href="https://www.mathworks.com/matlabcentral/fileexchange/33884-dxflib">https://www.mathworks.com/matlabcentral/fileexchange/33884-dxflib</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 数学建模 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图像处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>和周小姐高中的对话（一）</title>
      <link href="/2021/11/08/%E5%92%8C%E5%91%A8%E5%B0%8F%E5%A7%90%E9%AB%98%E4%B8%AD%E7%9A%84%E5%AF%B9%E8%AF%9D%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2021/11/08/%E5%92%8C%E5%91%A8%E5%B0%8F%E5%A7%90%E9%AB%98%E4%B8%AD%E7%9A%84%E5%AF%B9%E8%AF%9D%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>和周小姐的高中日常之一</p><span id="more"></span><p>一日，周小姐忽然递过一张纸条，上面写道：</p><blockquote><p>凡食人间烟火者，皆有所好，吾之所好者，乐也；爱华君之所好者，色也。</p><p>何为色也？女色也，凡有面容姣好者，难以逃脱爱华之眼也。</p><p>嗟乎！色字当头一把刀，吾甚恐爱华君不知也，特写此书以告之。</p><p align="right">——by Ting</p></blockquote><p>我这样回复：</p><blockquote><p>汝庸也，华夏上下五千年，其人也以亿计，天地寿也数十亿年，其灵也以万亿计。此皆雌雄，男女分工之法。</p><p>万物有灵，灵皆爱美，更有学者撰《物种起源》论进化之事矣，其意亦如此。</p><p>今吾所思之女子，面容姣好，举止淑女，世人视之皆慕之，吾何理不念之。古人云：“窃窕淑女，君子好逑。”且吾止于远观而不骚扰，汝何理之言吾心色也？</p><p align="right">——by Aep</p></blockquote><p>今料我所言应如是。</p>]]></content>
      
      
      <categories>
          
          <category> 生活 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>记录第n次修复引导区</title>
      <link href="/2021/11/07/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E4%BF%AE%E5%A4%8D%E5%BC%95%E5%AF%BC%E5%8C%BA/"/>
      <url>/2021/11/07/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E4%BF%AE%E5%A4%8D%E5%BC%95%E5%AF%BC%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>给U盘装了archlinux后，没注意把固态上archlinux的/etc/fstab分区挂载配置文件修改了，以为是grub又坏了，干脆把挂载在/boot下的EFI 系统格式化了，孙子云：“置之死地而后生。” 从头开始</p><span id="more"></span><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>插网线，喝热水</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><ol><li><p>U盘启动进去后挂载根分区和启动分区，arch-chroot到根分区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">lsblk</span><br><span class="line">mkfs.vfat -F32 /dev/$&#123;/boot&#125;# $&#123;/boot&#125; 启动分区</span><br><span class="line">mount /dev/$&#123;/&#125; /mnt# $&#123;/&#125; 根分区</span><br><span class="line">mount /dev/$&#123;/boot&#125; /mnt/boot</span><br><span class="line">arch-chroot /mnt</span><br></pre></td></tr></table></figure><blockquote><p>以上操作只能在启动盘进行，如果用完整的archlinux系统，则没有arch-chroot以及pacstrap命令<br>如果使用chroot代替arch-chroot命令后，执行pacman会报非本用户错误</p></blockquote></li><li><p>重新生成引导区</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grub-install --target=x86_64-efi --efi-directory=/boot --bootloader-id=GRUB</span><br><span class="line">grub-mkconfig -o /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure><p>查看有没有Linux的入口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure><p>如果没有入口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /boot</span><br></pre></td></tr></table></figure><p>查看是否有下列文件</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">initramfs-linux.img</span><br><span class="line">intel-ucode.img</span><br><span class="line">vmlinuz-linux</span><br></pre></td></tr></table></figure><p>如果没有，重装linux</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pacman -S linux</span><br></pre></td></tr></table></figure><p>如果报错<br><code>GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.cinnamon.SettingsDaemon was not provided by any .service files</code><br>因为pacman检测到非原来用户执行，则可以执行下面命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkinitcpio -p linux</span><br></pre></td></tr></table></figure><p>重新加载内核模块，生成文件<br>再执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grub-mkconfig -o /boot/grub/grub.cfg</span><br><span class="line">ls /boot</span><br></pre></td></tr></table></figure><p>可以看到有了三个文件<br>退出/mnt根分区，回到启动盘</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">initramfs-linux.img</span><br><span class="line">intel-ucode.img</span><br><span class="line">vmlinuz-linux</span><br><span class="line">exit</span><br></pre></td></tr></table></figure></li><li>重新生成分区挂载配置文件<br>先删除原来的分区挂载配置文件，不然无法挂载分区，系统无法启动，再生成fstab文件<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rm -rf /mnt/etc/fstab</span><br><span class="line">genfstab -U /mnt &gt;&gt; /mnt/etc/fstab</span><br></pre></td></tr></table></figure>查看生成的硬盘UUID和分区命名，一致即可<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ls -l /dev/disk/by-uuid</span><br><span class="line">cat /mnt/etc/fstab</span><br><span class="line">exit</span><br><span class="line">poweroff</span><br></pre></td></tr></table></figure></li><li>设置UEFI<br>本人电脑为DELL</li></ol><ul><li>进入BIOS</li><li>选择Boot Sequence</li><li>Add Boot Option</li><li>File Name 为 /boot/EFI/grub下的grubx64.efi</li><li>Boot Option Name 非空</li><li>Apply，Exit<h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><blockquote><p>修grub的周期大概是3个月一次，各种坏掉的原因都有，记录下方便下次解决。</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> archlinux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> grub </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>求解微分方程</title>
      <link href="/2021/11/02/%E6%B1%82%E8%A7%A3%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
      <url>/2021/11/02/%E6%B1%82%E8%A7%A3%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>解微分方程是我的弱项，就靠这个了</p><span id="more"></span><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>含导数或微分的方程称之为微分方程，一般形式为$f(x,y’ \cdots y^{(n)})$</p><h3 id="阶数"><a href="#阶数" class="headerlink" title="阶数"></a>阶数</h3><p>微分方程所含的导数或微分的最高阶数称为微分方程的阶数</p><h3 id="解"><a href="#解" class="headerlink" title="解"></a>解</h3><p>解：使得微分方程成立的函数<br>特解：不含任意常数<br>通解：所含的相互独立的任意常数的个数与微分方程的阶数相等</p><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><h3 id="可分离变量微分方程"><a href="#可分离变量微分方程" class="headerlink" title="可分离变量微分方程"></a>可分离变量微分方程</h3><script type="math/tex; mode=display">\frac{\mathrm{d}y}{\mathrm{d}x} = f(x)g(y)</script><p>解</p><script type="math/tex; mode=display">\int \frac{\mathrm{d}y}{g(y)} = \int f(x)\mathrm{d}x</script><blockquote><p>e.g. </p><script type="math/tex; mode=display">\frac{\mathrm{d}y}{\mathrm{d}x} = 2xy</script><p>解</p><script type="math/tex; mode=display">\int \frac{\mathrm{d}y}{y} = \int 2x\mathrm{d}x</script><script type="math/tex; mode=display">\ln y = x^2+C</script><script type="math/tex; mode=display">\begin{aligned} y {}& = e^{x^2+C} {} \\ & = e^C e^{x^2} {} \\ & = C_1 e^{x^2} {} \\ & \quad(C_1 = e^C) \end{aligned}</script></blockquote><h3 id="一阶齐次线性微分方程"><a href="#一阶齐次线性微分方程" class="headerlink" title="一阶齐次线性微分方程"></a>一阶齐次线性微分方程</h3><script type="math/tex; mode=display">y' +P(x)y = 0</script><p>解</p><script type="math/tex; mode=display">y = Ce^{-\int{P(x)\mathrm{d}x}}</script><blockquote><p>e.g.</p><script type="math/tex; mode=display">y'-xy=0</script><p>解</p><script type="math/tex; mode=display">y = Ce^{\int x\mathrm{d}x}=Ce^{\frac{x^2}{2}}</script></blockquote><h3 id="一阶非齐次线性微分方程"><a href="#一阶非齐次线性微分方程" class="headerlink" title="一阶非齐次线性微分方程"></a>一阶非齐次线性微分方程</h3><script type="math/tex; mode=display">y'+P(x)y = Q(x) \neq 0</script><p>解</p><script type="math/tex; mode=display">y = (\int Q(x)e^{\int P(x) \mathrm{d}x}\mathrm{d}x+C)e^{-\int P(x) \mathrm{d}x}</script><blockquote><p>e.g.</p><script type="math/tex; mode=display">y'+y \tan x = \cos x</script><p>解</p><script type="math/tex; mode=display">y = (\int \cos x e^{\int \tan x \mathrm{d}x}\mathrm{d}x+C)e^{-\int\tan x \mathrm{d}x} = (x+C)\cos x</script></blockquote><h3 id="贝努利方程"><a href="#贝努利方程" class="headerlink" title="贝努利方程"></a>贝努利方程</h3><script type="math/tex; mode=display">\frac{\mathrm{d}y}{\mathrm{d}x} + P(x)y = Q(x)y^n \quad (n \neq 0)</script><p>解<br>令$z = y^{1-n}$，代入原方程得$\frac{\mathrm{d}z}{\mathrm{d}x}+(1-n)P(x)z = (1-n)Q(x)$，再求解该一阶非齐次线性方程即可</p><blockquote><p>e.g.</p><script type="math/tex; mode=display">x^2y'+xy = y^2</script><p>解</p><script type="math/tex; mode=display">z = y^{1-2} = y^{-1},\quad y' = -\frac{1}{z^2}z'</script><p>代入原式</p><script type="math/tex; mode=display">z' - \frac{1}{x}z = -\frac{1}{x^2}</script><script type="math/tex; mode=display">z = (\int (-\frac{1}{x^2})e^{\int -\frac{1}{x}\mathrm{d}x}\mathrm{d}x +C)e^{\int \frac{1}{x}\mathrm{d}x} = (\frac{1}{2x^2}+C)x</script><script type="math/tex; mode=display">\frac{1}{y} = (\frac{1}{2x^2}+C)x</script></blockquote><h3 id="可降阶微分方程"><a href="#可降阶微分方程" class="headerlink" title="可降阶微分方程"></a>可降阶微分方程</h3><h4 id="形式一"><a href="#形式一" class="headerlink" title="形式一"></a>形式一</h4><script type="math/tex; mode=display">y^{(n) = f(x)}</script><p>解<br>对方程进行$n$次不定积分</p><blockquote><p>e.g.</p><script type="math/tex; mode=display">y'' = x^2</script><p>解</p><script type="math/tex; mode=display">y' = \frac{1}{3} x^3 +C_1</script><script type="math/tex; mode=display">y = \frac{1}{12}x^4+C_1 x + C_2</script></blockquote><h4 id="形式二"><a href="#形式二" class="headerlink" title="形式二"></a>形式二</h4><script type="math/tex; mode=display">f(x,y',y'') = 0</script><p>解</p><ol><li>令$y’ = p$，则$y’’=p’$，原方程变为$f(x,p,p’) = 0$</li><li>解出$p = \varphi(x)$</li><li>$y = \int \varphi(x)\mathrm{d}x+C$<blockquote><p>e.g.</p><script type="math/tex; mode=display">y'' + \frac{y'}{x} = 0</script><p>解<br>令$y’ = p$，则$y’’ = p’$，原方程变为$p’ + \frac{p}{x} = 0$</p><script type="math/tex; mode=display">p = C_1e^{\int -\frac{1}{x}\mathrm{d}x} = \frac{C_1}{x}</script><script type="math/tex; mode=display">\frac{\mathrm{d}y}{\mathrm{d}x} = \frac{C_1}{x}</script><script type="math/tex; mode=display">y = C_1 \ln \lvert x \rvert +C_2</script></blockquote></li></ol><h4 id="形式三"><a href="#形式三" class="headerlink" title="形式三"></a>形式三</h4><script type="math/tex; mode=display">f(y,y',y'') = 0</script><p>解</p><ol><li>令$y’ = p$，则$y’’ = \frac{\mathrm{d}p}{\mathrm{d}x} = \frac{\mathrm{d}p}{\mathrm{d}y} \frac{\mathrm{d}y}{\mathrm{d}x} = p \frac{\mathrm{d}p}{\mathrm{d}y}$，原方程变为$f(y,p,p’) = 0$</li><li>解出$p = \varphi(y)$</li><li>$\int \frac{\mathrm{d}y}{\varphi(y)} = x+C$<blockquote><p>e.g.</p><script type="math/tex; mode=display">y y'' = y'^2</script><p>令$y’ = p$，$y’’ = p \frac{\mathrm{d}p}{\mathrm{d}y}$，代入原式</p><script type="math/tex; mode=display">yp\frac{\mathrm{d}p}{\mathrm{d}y}-p^2 = 0 \Rightarrow p = C_1 e^{\int \frac{1}{y} \mathrm{d}y = C_1 y }</script><script type="math/tex; mode=display">\frac{\mathrm{d}y}{\mathrm{d}x} = C_1 y \Rightarrow \int \frac{\mathrm{d}y}{y} = \int C_1 \mathrm{d}x</script><script type="math/tex; mode=display">\ln y = C_1x+C_2 \Rightarrow y = e^{C_1 x+C_2} = C_3 e^{C_1 x} \quad (C_3 = e^{C_2})</script></blockquote></li></ol><h3 id="二阶常系数线性微分方程"><a href="#二阶常系数线性微分方程" class="headerlink" title="二阶常系数线性微分方程"></a>二阶常系数线性微分方程</h3><script type="math/tex; mode=display">y'' +py' +qy = 0</script><p>解</p><ol><li>求解$y’’ +py’ +qy = 0$的特征方程是$r^2+pr+q = 0$</li><li>根据方程根的不同分为三种情况<ol><li>当特征方程有两个实根$r_1$，$r_2$，且$r_1\neq r_2$，则$y = C_1e^{r1 x}+C_2e^{r_2 x}$</li><li>当特征方程有重根$r_1 = r_2$，则$y = (C_1+C_2x)e^{r_1 x}$</li><li>当特征方程有两个共轭重根$r_{1,2} = \alpha \pm \beta i$，则$y = e^{\alpha x} (C_1 \cos \beta x + C_2 \sin \beta x)$<blockquote><p>e.g.1</p><script type="math/tex; mode=display">y'' - y' - 6y = 0</script><p>解</p><script type="math/tex; mode=display">r^2-r-6 = (r-3)(r+2) = 0 \Rightarrow r_1 = 3,\quad r_2 = -2</script><script type="math/tex; mode=display">y = C_1 e^{3x} + C_2 e^{-2x}</script><p>e.g.2</p><script type="math/tex; mode=display">y''-2y'+y = 0</script><p>解</p><script type="math/tex; mode=display">r^2-2r+1 = (r-1)^2 = 0 \Rightarrow r_{1,2} = 1</script><script type="math/tex; mode=display">y = (C_1 + C_2 x)e^x</script><p>e.g.3</p><script type="math/tex; mode=display">y'' - 2y' + 2y = 0</script><p>解</p><script type="math/tex; mode=display">r^2- 2r +2 = 0 \Rightarrow r_{1,2} = 1 \pm i</script><script type="math/tex; mode=display">y = e^x (C_1 \cos x + C_2 \sin x)</script></blockquote></li></ol></li></ol><h3 id="二阶常系数非齐次线性微分方程"><a href="#二阶常系数非齐次线性微分方程" class="headerlink" title="二阶常系数非齐次线性微分方程"></a>二阶常系数非齐次线性微分方程</h3><h4 id="形式一-1"><a href="#形式一-1" class="headerlink" title="形式一"></a>形式一</h4><script type="math/tex; mode=display">y'' + py' +qy = P_n(x)e^{kx}</script><p>其中$P_n(x)$代表$x$的$n$阶多项式<br>解<br>根据$k$的值有以下三种情况：</p><ol><li>若$k$非特征值，即$k \neq r<em>1 \neq r_2$，则特解 $y^* = e^{kx} \sum</em>{i=0}^n a_i x^i$</li><li>若$k$与一个特征值相同，则特解  $y^* = xe^{kx} \sum_{i=0}^n a_i x^i$</li><li>若$k$与两个特征值相同，则特解  $y^* = x^2e^{kx} \sum_{i=0}^n a_i x^i$</li></ol><h4 id="形式二-1"><a href="#形式二-1" class="headerlink" title="形式二"></a>形式二</h4><script type="math/tex; mode=display">y'' + py' + qy = e^{ax} [P_l \cos bx + P_s \sin bx]</script><p>其中$P_l(x)$，$P_s(x)$代表$x$的$l$，$s$阶多项式<br>解<br>根据$a$，$b$的值有以下三种情况</p><ol><li>若$a\pm b i$不是特征值，则特解 $y^* = (Q_n^{(1)}(x)\cos bx+Q_n^{(2)}(x)\sin bx)e^{ax}$</li><li>若$a\pm b i$是特征值，则特解 $y^* = x(Q_n^{(1)}(x)\cos bx+Q_n^{(2)}(x)\sin bx)e^{ax}$</li></ol>]]></content>
      
      
      <categories>
          
          <category> 高等数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拟合和统计中的检验参数</title>
      <link href="/2021/10/20/%E6%8B%9F%E5%90%88%E5%92%8C%E7%BB%9F%E8%AE%A1%E4%B8%AD%E7%9A%84%E6%A3%80%E9%AA%8C%E5%8F%82%E6%95%B0/"/>
      <url>/2021/10/20/%E6%8B%9F%E5%90%88%E5%92%8C%E7%BB%9F%E8%AE%A1%E4%B8%AD%E7%9A%84%E6%A3%80%E9%AA%8C%E5%8F%82%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>数据分析中无处不在的知识点</p><span id="more"></span><blockquote><p>SSE(和方差、误差平方和)：The sum of squares due to error<br>MSE(均方差、方差)：Mean squared error<br>RMSE(均方根、剩余标准差)：Root mean squared error<br>$R^2$(判断系数，拟合优度)：Coefficient of determination</p></blockquote><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在统计学中，均方误差是参数估计值与参数真值之差平方的期望值，是衡量“平均误差”的一种较方便的方法，MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><h4 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h4><p>方差是在概率论和统计方差衡量随机变量或一组数据的离散程度的度量方式，方差越大，离散度越大。求解方式为，各随机变量与平均值差值的平方和的平均数<br>平均数：</p><script type="math/tex; mode=display">M = \frac{x_1+x_2+x_3+\cdots+x_n}{n}</script><p>方差公式：</p><script type="math/tex; mode=display">S^2 = \frac{(x_1-M)^2+(x_1-M)^2+\cdots+(x_n-M)^2}{n}</script><h4 id="标准差"><a href="#标准差" class="headerlink" title="标准差"></a>标准差</h4><p>标准差就是方差的算术平方根，它反映组内个体间的离散程度。因此它的过程是与平均值之间进行差值计算<br>标准差公式：</p><script type="math/tex; mode=display">\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2}</script><h4 id="样本方差"><a href="#样本方差" class="headerlink" title="样本方差"></a>样本方差</h4><script type="math/tex; mode=display">\hat{\sigma}^2=\frac{1}{n-1}\sum_{i=1}^n(x_i-\mu)^2</script><h2 id="SSE（误差平方和）"><a href="#SSE（误差平方和）" class="headerlink" title="SSE（误差平方和）"></a>SSE（误差平方和）</h2><p>在统计学里，该参数计算的是拟合数据很原始数据对应点的误差的平方和，计算公式为：</p><script type="math/tex; mode=display">SSE = \sum_{i=1}^m(y_i - \hat{y}_i)^2</script><p>$y_i$是真实数据，$\hat{y}_i$是拟合数据</p><h2 id="MSE（方差）"><a href="#MSE（方差）" class="headerlink" title="MSE（方差）"></a>MSE（方差）</h2><p>是预测数据和原始数据对应点误差的平方和的均值，也就是$\frac{SSE}{n-m}$，$n$是观测数据的个数，$m$j是拟合数据的个数,和$SSE$没有太大的区别，计算公式为：</p><script type="math/tex; mode=display">MSE=\frac{SSE}{n-m}=\frac{1}{n-m}\sum_{i=1}^{n}(y_i-\hat{y_i })^2</script><h2 id="RMSE（剩余标准差）"><a href="#RMSE（剩余标准差）" class="headerlink" title="RMSE（剩余标准差）"></a>RMSE（剩余标准差）</h2><p>也是叫回归系统的拟合标准差，是$MSE$的平方根，计算公式为：</p><script type="math/tex; mode=display">RMSE=\sqrt{MSE}=\sqrt{\frac{1}{n-m}\sum_{i=1}^{n}(y_i-\hat{y_i })^2}</script><h2 id="R-2-判断系数"><a href="#R-2-判断系数" class="headerlink" title="$R^2$(判断系数)"></a>$R^2$(判断系数)</h2><p>在讲判断系数之前，先介绍另外两个参数$SSR$和$SST$，因为判断系数就是由这两个参数决定的<br>对总平方和$SST = \sum_{i=1}^{n}(y_i-\overline{y})^2$进行分解，有</p><script type="math/tex; mode=display">SST = SSE+SSR,SSR = \sum_{i=1}^{n}(\hat{y_i}-\overline{y})^2</script><p>其中$\overline{y} = \frac{1}{n}\sum_{i=1}^{n}y_i$，$SSE$是误差平方和，反映随机误差对$y$的影响，$SSR$是回归平方和，反映自变量对$y$的影响<br>判断系数定义为</p><script type="math/tex; mode=display">R^2 = \frac{SSR}{SST} = \frac{SST-SSE}{SST} = 1-\frac{SSE}{SST}</script><h2 id="调整的-R-2"><a href="#调整的-R-2" class="headerlink" title="调整的$R^2$"></a>调整的$R^2$</h2><p>统计学家主张在回归建模时，就采用尽可能少的自变量，不要盲目地追求判定系数的提高。当变量增加时，残量的自由度就会减少。而自由度越小，数据的统计趋势就越不容易显现。为此，又定义了一个调整判断系数</p><script type="math/tex; mode=display">\overline{R^2} = 1-\frac{SSE/(n-m)}{SST/(n-1)}</script><p>$\overline{R^2}$与$R^2$的关系是</p><script type="math/tex; mode=display">\overline{R^2} = 1-(1-R^2)\frac{n-1}{n-m}</script><p>当$n$很小，$m$很大时，$\overline{R^2}$会远小于$R^2$</p>]]></content>
      
      
      <categories>
          
          <category> 统计学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正交多项式</title>
      <link href="/2021/10/17/%E6%AD%A3%E4%BA%A4%E5%A4%9A%E9%A1%B9%E5%BC%8F/"/>
      <url>/2021/10/17/%E6%AD%A3%E4%BA%A4%E5%A4%9A%E9%A1%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>总结一下可能会遇到的正交多项式</p><span id="more"></span><h2 id="正交多项式定义"><a href="#正交多项式定义" class="headerlink" title="正交多项式定义"></a>正交多项式定义</h2><p>一个多项式序列${ {p<em>n}(x)} </em>{n = 0}^\infty$，阶数为$[p_n(x)] = n$,对于每一个$n$，这个多项式序列在开区间$(a,b)$上关于权函数$w(x)$正交，如果：</p><script type="math/tex; mode=display">\int_a^bw(x)p_m(x)p_n(x)dx = h_n\delta_{mn}</script><blockquote><p>$\delta$为狄克拉函数，且$h_n$为常数<br>权函数$w(x)$在区间$(a,b)$是连续且正的,下式存在:</p><script type="math/tex; mode=display">\mu_n = \int_a^bw(x)x^ndx,n \in N^+</script></blockquote><p>则多项式$f$和多项式$g$的内积定义为：</p><script type="math/tex; mode=display">\langle{f,g}\rangle = \int_a^bw(x)f(x)g(x)dx</script><p>区间$(a,b)称为正交区间，正交区间不一定是有限区间$</p><h3 id="e-g-三角函数的正交性"><a href="#e-g-三角函数的正交性" class="headerlink" title="e.g 三角函数的正交性"></a>e.g 三角函数的正交性</h3><blockquote><p>对于三角函数序列$1,sin(\theta),cos(\theta),sin(2\theta),cos(2\theta),…,cos(n\theta)$，$n \in N^+$，他们在区间$(0,2\pi )$的正交性为：</p><script type="math/tex; mode=display">\int_0^{2 \pi }sin(n \theta )cos(m \theta) d\theta = \delta_{mn}</script><p>图中阴影部分的面积加上符号求和为0.<br><img src="/image/正交多项式/1.png" alt="1"></p></blockquote><h3 id="e-g-施密特正交化"><a href="#e-g-施密特正交化" class="headerlink" title="e.g 施密特正交化"></a>e.g 施密特正交化</h3><blockquote><p>施密特正交化方法是将一组线性无关的向量组正交化的方法，对正交化后的向量组进行标准化处理，可进一步得到一组标准正交基。处理步骤如下</p><script type="math/tex; mode=display">\begin{array}{l} {\beta _1} = {\alpha _1} \\  {\beta _2} = {\alpha _2} - \frac{ {\left\langle { {\alpha _2},{\beta _1} } \right\rangle } }{ {\left\langle { {\beta _1},{\beta _1} } \right\rangle } }{\beta _1} \\   \cdots  \\  {\beta _n} = {\alpha _n} - \frac{ {\left\langle { {\alpha _n},{\beta _1} } \right\rangle } }{ {\left\langle { {\beta _1},{\beta _1} } \right\rangle } }{\beta _1} - \frac{ {\left\langle { {\alpha _n},{\beta _2} } \right\rangle } }{ {\left\langle { {\beta _2},{\beta _2} } \right\rangle } }{\beta _2} -  \cdots  - \frac{ {\left\langle { {\alpha _n},{\beta _{n - 1} } } \right\rangle } }{ {\left\langle { {\beta _{n - 1} },{\beta _{n - 1} } } \right\rangle } }{\beta _{n - 1} } \\  \end{array}</script><p>将三维空间中的一组线性无关向量$a,b,c$用施密特正交化方法处理得到正交向量组 $x,y,z$，步骤如下：</p><script type="math/tex; mode=display">\begin{array}{l} x = a \\  y = b - \frac{ {\left\langle {b,x} \right\rangle } }{ {\left\langle {x,x} \right\rangle } }x = \frac{ {\left| b \right|\cos (\theta )} }{ {\left| a \right|} }a \\  z = c - \frac{ {\left\langle {c,x} \right\rangle } }{ {\left\langle {x,x} \right\rangle } }x - \frac{ {\left\langle {c,y} \right\rangle } }{ {\left\langle {y,y} \right\rangle } }y \\ \end{array}</script><p>几何描述如图：<br><img src="/image/正交多项式/2.png" alt="2"></p></blockquote><h2 id="经典正交多项式"><a href="#经典正交多项式" class="headerlink" title="经典正交多项式"></a>经典正交多项式</h2><p>如雅克比多项式，切比雪夫多项式，勒让德多项式，拉盖尔多项式，伯恩斯坦多项式，球谐多项式等</p><h3 id="雅克比多项式"><a href="#雅克比多项式" class="headerlink" title="雅克比多项式"></a>雅克比多项式</h3><p>雅克比多项式是定义在$(-1,1)$上，关于权函数$(1-x)^{\alpha} (1+x)^{\beta} $正交的多项式，其中$\alpha ,\beta &gt; -1$<br>表达式为：</p><script type="math/tex; mode=display">P_n^{ (\alpha,\beta) } (x) = \frac{ \Gamma(\alpha+n+1) }{n!(\alpha+\beta+n+1)} \sum_{m=0}^n \begin{pmatrix} n \\ m \end{pmatrix} \frac{\Gamma(\alpha+\beta+n+m+1)}{\Gamma(\alpha+m+1)}{(\frac{x-1}{2})}^m</script><script type="math/tex; mode=display">\Gamma(\gamma) = \int_0^{+ \infty } t^{\gamma -1} e^{-t} dt</script><p>雅克比多项式的正交性：</p><script type="math/tex; mode=display">\begin{array}{l}\int_{ - 1}^1 { { {(1 - x)}^\alpha }{ {(1 + x)}^\beta }P_m^{(\alpha ,\beta )}(x)P_n^{(\alpha ,\beta )}(x)dx}  \\  = \frac{ { {2^{\alpha  + \beta  + 1} } } }{ {2n + \alpha  + \beta  + 1} }\frac{ {\Gamma (\alpha  + n + 1)\Gamma (\beta  + n + 1)} }{ {n!\Gamma (\alpha  + \beta  + n + 1)} }{\delta _{mn} } \\ \end{array}</script><h3 id="勒让德多项式"><a href="#勒让德多项式" class="headerlink" title="勒让德多项式"></a>勒让德多项式</h3><p>勒让德多项式是定义在区间 $(−1,1) $上关于权函数1正交的多项式。勒让德多项式实际上是雅克比多项式在 $\alpha=\beta=0$ 时的特殊情况<br>表达式为：</p><script type="math/tex; mode=display">P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n} [ (x^2-1)^n ]</script><p>递推公式为：</p><script type="math/tex; mode=display">(n+1)P_{n+1}(x) = (2n+1)xP_n(x)-nP_{n-1}(x)</script><p>正交性：</p><script type="math/tex; mode=display">\int_{-1}^1 P_m(x)P_n(x)dx = \frac{2}{2n+1}\delta_{mn}</script><p>前6阶勒让德多项式：</p><script type="math/tex; mode=display">P_0(x) = 1</script><script type="math/tex; mode=display">P_1(x) = x</script><script type="math/tex; mode=display">{P_2}(x) = \frac{3}{2}{x^2} - \frac{1}{2}</script><script type="math/tex; mode=display">{P_3}(x) = \frac{5}{2}{x^3} - \frac{3}{2}x</script><script type="math/tex; mode=display">{P_4}(x) = \frac{ {35} }{8}{x^4} - \frac{ {15} }{4}{x^2} + \frac{3}{8}</script><script type="math/tex; mode=display">{P_5}(x) = \frac{ {63} }{8}{x^5} - \frac{ {35} }{4}{x^3} + \frac{ {15} }{8}x</script><script type="math/tex; mode=display">{P_6}(x) = \frac{ {231} }{ {16} }{x^6} - \frac{ {315} }{ {16} }{x^4} + \frac{ {105} }{ {16} }{x^2} - \frac{5}{ {16} }</script><p>图像为：<br><img src="/image/正交多项式/3.png" alt="3"></p><h3 id="切比雪夫多项式"><a href="#切比雪夫多项式" class="headerlink" title="切比雪夫多项式"></a>切比雪夫多项式</h3><p>切比雪夫多项式是定义在区间 $(−1,1)$ 上关于权函数 $\frac{1}{\sqrt{1-x^2} }$ 正交的多项式</p><h4 id="第一类切比雪夫多项式"><a href="#第一类切比雪夫多项式" class="headerlink" title="第一类切比雪夫多项式"></a>第一类切比雪夫多项式</h4><script type="math/tex; mode=display">T_n(x) = cos(n\theta)</script><p>令$x = cos(\theta)$，则$T_n(x) = cos(n \arccos(x))$，<br>第一类切比雪夫多项式正交性：</p><script type="math/tex; mode=display">\int_{-1}^1 \frac{1}{\sqrt{1-x^2} } T_m(x) T_n(x)dx = \begin{cases}0 \quad m \neq n \\ \pi \quad n=m=0 \\ \frac{\pi}{2} \quad n=m \neq 0 \end{cases}</script><p>第一类切比雪夫多项式递推公式：</p><script type="math/tex; mode=display">{T_{n + 1} }(x) = 2x{T_n}(x) - {T_{n - 1} }(x)</script><p>第一类切比雪夫多项式的前6项：</p><script type="math/tex; mode=display">T_0(x) = 1</script><script type="math/tex; mode=display">T_1(x) = x</script><script type="math/tex; mode=display">T_2(x) = 2x^2-1</script><script type="math/tex; mode=display">T_3(x) = 4x^3-3x</script><script type="math/tex; mode=display">T_4(x) = 8x^4-8x^2+1</script><script type="math/tex; mode=display">T_5(x) = 16x^5-20x^3+5x</script><script type="math/tex; mode=display">T_6(x) = 32x^6-48x^4+18x^2-1</script><p>第一类切比雪夫多项图像：<br><img src="/image/正交多项式/4.svg" alt="4"></p><h4 id="第二类切比雪夫多项式"><a href="#第二类切比雪夫多项式" class="headerlink" title="第二类切比雪夫多项式"></a>第二类切比雪夫多项式</h4><script type="math/tex; mode=display">{T_n}(x) = \frac{ {\sin [(n + 1)\theta ]} } { {\sin \theta } }</script><p>第二类切比雪夫多项式的正交为：</p><script type="math/tex; mode=display">\int_{-1}^1 \sqrt{1-x^2}{T_m}(x){T_n}(x)dx=\begin{cases}0 \quad m\neq n \\ \frac{\pi}{2} \quad m = n \end{cases}</script><p>第二类切比雪夫多项式的递推公式：</p><script type="math/tex; mode=display">{T_{n + 1} }(x) = 2x{T_n}(x) - {T_{n - 1} }(x)</script><p>第二类切比雪夫多项式的前6项：</p><script type="math/tex; mode=display">T_0(x) = 1</script><script type="math/tex; mode=display">T_1(x) = 2x</script><script type="math/tex; mode=display">T_2(x) = 4x^2-1</script><script type="math/tex; mode=display">T_3(x) = 8x^3-4x</script><script type="math/tex; mode=display">T_4(x) = 16x^4-12x^2+1</script><script type="math/tex; mode=display">T_5(x) = 32x^5-32x^3+6x</script><script type="math/tex; mode=display">T_6(x) = 64x^6-80x^4+24x^2-1</script><p>第二类切比雪夫多项图像：<br><img src="/image/正交多项式/5.png" alt="5"></p><h3 id="拉盖尔多项式"><a href="#拉盖尔多项式" class="headerlink" title="拉盖尔多项式"></a>拉盖尔多项式</h3><p>拉盖尔多项式是定义在区间 $(0,+\infty)$ 上关于权函数 $e^{-x}x^a$ 正交的多项式<br>拉盖尔多项式的正交关系：</p><script type="math/tex; mode=display">\int_0^{ + \infty } { {x^\alpha }{e^{ - x} }L_m^{(\alpha )}(x)L_n^{(\alpha )}(x)dx}  = \frac{ {\left( {n + \alpha } \right)!} }{ {n!} }{\delta _{mn} }</script><p>拉盖尔多项式的递推关系：</p><script type="math/tex; mode=display">{L_{n + 1} }(x) = \frac{ {(2n + 1 - x){L_n}(x) - n{L_{n - 1} }(x)} }{ {n + 1} }</script><p>前6项拉盖尔多项式$(\alpha = 0)$：</p><script type="math/tex; mode=display">L_0(x) = 1</script><script type="math/tex; mode=display">L_1(x) = -x+1</script><script type="math/tex; mode=display">L_2(x) = \frac{1}{2}x^2-2x+1</script><script type="math/tex; mode=display">L_3(x) = -\frac{1}{6}x^3+\frac{3}{2}x^2-3x+1</script><script type="math/tex; mode=display">L_4(x) = \frac{1}{24}x^4-\frac{2}{3}x^3+3x^2-4x+1</script><script type="math/tex; mode=display">L_5(x) = -\frac{1}{120}x^5+\frac{5}{24}x^4-\frac{5}{3}x^3+5x^2-5x+1</script><script type="math/tex; mode=display">L_6(x) = \frac{1}{720}x^6-\frac{1}{20}x^5+\frac{5}{8}x^4-\frac{10}{3}x^3+\frac{15}{2}x^2-6x+1</script><p>前6项拉盖尔多项式的图像：<br><img src="/image/正交多项式/6.webp" alt="6"></p><h3 id="埃尔米特多项式"><a href="#埃尔米特多项式" class="headerlink" title="埃尔米特多项式"></a>埃尔米特多项式</h3><p>埃尔米特多项式是定义在区间$(-\infty,+\infty)$上关于权函数$e^{x^2}$正交的多项式<br>埃尔米特多项式分为概率论中的埃尔米特多项式和物理中的埃尔米特多项式，概率论的埃尔米特多项式是首一多项式（最高次项系数等于1），而物理学的埃尔米特多项式的最高次项系数等于$2n$，这里只介绍物理学中使用的埃尔米特多项式<br>埃尔米特多项式的表达式为：</p><script type="math/tex; mode=display">H_n(x) = (-1)^n e^{x^2} \frac{d^n}{dx^n}e^{-x^2}</script><p>埃尔米特多项式的正交性：</p><script type="math/tex; mode=display">\int_{-\infty}^{+\infty} H_m(x)H_n(x)e^{-x^2}dx = \sqrt{\pi}2^n n! \delta_{mn}</script><p>埃尔米特多项式的递推公式：</p><script type="math/tex; mode=display">H_{n+1}(x) = 2xH_n(x)-2nH_{n-1}(x)</script><p>前6项埃尔米特多项式为：</p><script type="math/tex; mode=display">H_0(x) = 1</script><script type="math/tex; mode=display">H_1(x) = 2x</script><script type="math/tex; mode=display">H_2(x) = 4x^2-2</script><script type="math/tex; mode=display">H_3(x) = $8x^3-12x</script><script type="math/tex; mode=display">H_4(x) = 16x^4-48x^2+12</script><script type="math/tex; mode=display">H_5(x) = 32x^5-160x^3+120x</script><script type="math/tex; mode=display">H_6(x) = 64x^6-480x^4+720x^2-120</script><p>前6项埃尔米特多项式图像：<br><img src="/image/正交多项式/7.webp" alt="7"></p><h3 id="正交多项式的应用"><a href="#正交多项式的应用" class="headerlink" title="正交多项式的应用"></a>正交多项式的应用</h3><p>仅以一个小的例子来说明正交多项式在函数拟合中的应用<br>实验中使用的测试函数为 $y=4x+3x^2+cos(x)+sin(2x)+e^x$，定义区间为 $(−2,2)$ ，实验比较了多项式展开3项时不同多项式的拟合均方误差（MSE），归一化<a href="![8](/image/正交多项式/8.svg">均方误差</a></p><p><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">输出</summary><pre><code class="matlab">MSE_power =    0.0131NMSE_power =    0.0419MSE_legendre =    0.0131NMSE_legendre =    0.0419MSE_chebyshev =    0.0131NMSE_chebyshev =    0.0419MSE_laguerre =    0.0131NMSE_laguerre =    0.0419MSE_hermite =    0.0067NMSE_hermite =    0.0215</code></pre></details><br>可以看出埃尔米特的拟合误差较小，其他相当<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">% 正交多项式测试</span></span><br><span class="line">clear</span><br><span class="line">clc</span><br><span class="line"><span class="comment">% 采样点数</span></span><br><span class="line">N = <span class="number">1000</span> ; </span><br><span class="line"><span class="comment">% 正交多项式阶数</span></span><br><span class="line">M = <span class="number">3</span> ;   </span><br><span class="line"><span class="comment">% 拟合函数区间为（-2,2）</span></span><br><span class="line">x = <span class="built_in">linspace</span>(<span class="number">-2</span>,<span class="number">2</span>,N)&#x27; ; </span><br><span class="line"><span class="comment">% 生成被拟合的函数，包括指数函数，余弦函数，幂函数成分</span></span><br><span class="line">y =  <span class="number">4</span>*x + <span class="number">3</span>*x.^<span class="number">2</span> + <span class="built_in">cos</span>(x) + <span class="built_in">exp</span>(x) + <span class="built_in">sin</span>(<span class="number">2</span>*x);</span><br><span class="line"> </span><br><span class="line"><span class="comment">% 生成幂级数组成的基矩阵</span></span><br><span class="line">P1 = power_p(x,M) ;</span><br><span class="line"><span class="comment">% 生成勒让德多项式组成的基矩阵</span></span><br><span class="line">P2 = legendre_p(N,M) ;</span><br><span class="line"><span class="comment">% 生成切比雪夫多项式组成的基矩阵</span></span><br><span class="line">P3 = chebyshev_p(N,M) ;</span><br><span class="line"><span class="comment">% 生成拉盖尔多项式组成的基矩阵</span></span><br><span class="line">P4 = laguerre_p(N,M) ;</span><br><span class="line"><span class="comment">% 生成诶尔米特多项式组成的基矩阵</span></span><br><span class="line">P5 = hermite_p(N,M) ;</span><br><span class="line"> </span><br><span class="line"><span class="comment">%% 用最小二乘拟合y</span></span><br><span class="line"><span class="comment">% c1对应幂级数系数</span></span><br><span class="line">c1 = P1\y ;</span><br><span class="line"><span class="comment">% c2对应勒让德系数</span></span><br><span class="line">c2 = P2\y ;</span><br><span class="line"><span class="comment">% c3对应切比雪夫系数</span></span><br><span class="line">c3 = P3\y ;</span><br><span class="line"><span class="comment">% c4对应拉盖尔系数</span></span><br><span class="line">c4 = P4\y ;</span><br><span class="line"><span class="comment">% c5对应埃尔米特系数</span></span><br><span class="line">c5 = P5\y ;</span><br><span class="line"> </span><br><span class="line"><span class="comment">%% 求MSE和NMSE</span></span><br><span class="line">MSE_power = norm(y-P1*c1)/N </span><br><span class="line">NMSE_power = norm(y-P1*c1)/norm(y) </span><br><span class="line"> </span><br><span class="line">MSE_legendre = norm(y-P2*c2)/N </span><br><span class="line">NMSE_legendre = norm(y-P2*c2)/norm(y) </span><br><span class="line"> </span><br><span class="line">MSE_chebyshev = norm(y-P3*c3)/N </span><br><span class="line">NMSE_chebyshev = norm(y-P3*c3)/norm(y) </span><br><span class="line"> </span><br><span class="line">MSE_laguerre = norm(y-P4*c4)/N </span><br><span class="line">NMSE_laguerre = norm(y-P4*c4)/norm(y) </span><br><span class="line"> </span><br><span class="line">MSE_hermite = norm(y-P5*c5)/N </span><br><span class="line">NMSE_hermite = norm(y-P5*c5)/norm(y) </span><br><span class="line"> </span><br><span class="line"><span class="built_in">figure</span>(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">plot</span>(x,y,<span class="string">&#x27;r-&#x27;</span>,x,P1*c1,<span class="string">&#x27;b-&#x27;</span>,x,P2*c2,<span class="string">&#x27;k-&#x27;</span>,x,P3*c3,<span class="string">&#x27;y-&#x27;</span>,x,P4*c4,<span class="string">&#x27;g-&#x27;</span>,x,P5*c5,<span class="string">&#x27;m-&#x27;</span>)</span><br><span class="line"><span class="built_in">legend</span>(<span class="string">&#x27;original&#x27;</span>,<span class="string">&#x27;power&#x27;</span>,<span class="string">&#x27;legendre&#x27;</span>,<span class="string">&#x27;chebyshev&#x27;</span>,<span class="string">&#x27;laguerre&#x27;</span>,<span class="string">&#x27;hermite&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">power_p</span><span class="params">(x,M)</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> m = <span class="number">1</span>:M</span><br><span class="line">    P(:,m) = x.^(m<span class="number">-1</span>) ;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">legendre_p</span><span class="params">(N,NN)</span></span></span><br><span class="line"><span class="comment">% 本函数生成N*M的勒让德基矩阵</span></span><br><span class="line">s = <span class="built_in">linspace</span>(<span class="number">-1</span>,<span class="number">1</span>,N)&#x27; ;</span><br><span class="line">P = <span class="built_in">zeros</span>(N,NN) ;</span><br><span class="line">P(:,<span class="number">1</span>) = <span class="built_in">ones</span>(N,<span class="number">1</span>) ;</span><br><span class="line">P(:,<span class="number">2</span>) = s ;</span><br><span class="line"><span class="keyword">for</span> n = <span class="number">3</span> : NN</span><br><span class="line">    P(:,n) = ((<span class="number">2</span> * n - <span class="number">3</span>) * s .* P(:,n - <span class="number">1</span>) - (n - <span class="number">2</span>) * P(:,n - <span class="number">2</span>)) / ( n <span class="number">-1</span> ) ;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">chebyshev_p</span><span class="params">(N,M)</span></span></span><br><span class="line"><span class="comment">% 本函数生成N*M的切比雪夫基矩阵</span></span><br><span class="line">x = <span class="built_in">linspace</span>(<span class="number">-1</span>,<span class="number">1</span>,N)&#x27; ;</span><br><span class="line">P = <span class="built_in">zeros</span>(N,M) ;</span><br><span class="line">P(:,<span class="number">1</span>) = <span class="built_in">ones</span>(N,<span class="number">1</span>) ;</span><br><span class="line">P(:,<span class="number">2</span>) = x ;</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">3</span>:M</span><br><span class="line">    P(:,k) = <span class="number">2</span>*x.*P(:,k<span class="number">-1</span>) - P(:,k<span class="number">-2</span>) ;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">laguerre_p</span><span class="params">(N,M)</span></span></span><br><span class="line"><span class="comment">% 本函数生成N*M的拉盖尔基矩阵</span></span><br><span class="line">x  = <span class="built_in">linspace</span>(<span class="number">-2</span>,<span class="number">2</span>,N)&#x27; ;</span><br><span class="line">P = <span class="built_in">zeros</span>(N,M) ;</span><br><span class="line">P(:,<span class="number">1</span>) = <span class="built_in">ones</span>(N,<span class="number">1</span>) ;</span><br><span class="line">P(:,<span class="number">2</span>) = -x + <span class="built_in">ones</span>(N,<span class="number">1</span>) ;</span><br><span class="line"><span class="keyword">for</span> m = <span class="number">3</span>:M</span><br><span class="line">    P(:,m) = ((<span class="number">2</span>*(m<span class="number">-2</span>)+<span class="number">1</span>-x).*P(:,m<span class="number">-1</span>)-(m<span class="number">-2</span>)*P(:,m<span class="number">-2</span>))./(m<span class="number">-1</span>) ;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[P]</span> = <span class="title">hermite_p</span><span class="params">(N,M)</span> </span></span><br><span class="line"><span class="comment">% 本函数生成N*M的埃尔米特基矩阵</span></span><br><span class="line">x = <span class="built_in">linspace</span>(<span class="number">-2</span>,<span class="number">2</span>,N)&#x27; ;</span><br><span class="line">P = <span class="built_in">zeros</span>(N,M) ;</span><br><span class="line">P(:,<span class="number">1</span>) = <span class="built_in">ones</span>(N,<span class="number">1</span>) ;</span><br><span class="line">P(:,<span class="number">2</span>) = <span class="number">2</span>*x ;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> m = <span class="number">2</span>:M</span><br><span class="line">    P(:,m+<span class="number">1</span>) = <span class="number">2</span>*x.*P(:,m-) - <span class="number">2</span>*(m<span class="number">-1</span>)*P(:,m<span class="number">-1</span>) ;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      <categories>
          
          <category> 数值分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>伽马函数</title>
      <link href="/2021/10/17/%E4%BC%BD%E9%A9%AC%E5%87%BD%E6%95%B0/"/>
      <url>/2021/10/17/%E4%BC%BD%E9%A9%AC%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>遇到了这个伽马函数， 许多博客说对考研很重要，就浅浅记录一下</p><span id="more"></span><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>在数学中，$\Gamma$函数（伽玛函数；Gamma函数），是阶乘函数在实数与复数域上的扩展。如果$n$为正整数，则：</p><script type="math/tex; mode=display">\Gamma (n) = (n-1)!</script><p>对于实数部分为正的复数$\alpha$，伽玛函数为：</p><script type="math/tex; mode=display">\Gamma(\alpha) = \int_0^{+\infty} x^{\alpha-1}e^{-x}dx(a>0)</script><script type="math/tex; mode=display">\Gamma(\alpha+1) = \alpha \Gamma(\alpha)</script><h2 id="常用的数值"><a href="#常用的数值" class="headerlink" title="常用的数值"></a>常用的数值</h2><script type="math/tex; mode=display">\Gamma(1) = 1</script><script type="math/tex; mode=display">\Gamma(\frac{1}{2}) = \sqrt{\pi}</script><script type="math/tex; mode=display">\Gamma(n+1) = n!</script>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识点 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matlab画图功能再探</title>
      <link href="/2021/10/08/matlab%E7%94%BB%E5%9B%BE%E5%8A%9F%E8%83%BD%E5%86%8D%E6%8E%A2/"/>
      <url>/2021/10/08/matlab%E7%94%BB%E5%9B%BE%E5%8A%9F%E8%83%BD%E5%86%8D%E6%8E%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>复习了一下 matlab 里面的基本画图函数的属性</p><span id="more"></span><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">close all;</span><br><span class="line">th = <span class="built_in">linspace</span>()<span class="number">0</span>,<span class="number">2</span>*<span class="built_in">pi</span>;</span><br><span class="line">fig1 = <span class="built_in">plot</span>(th,<span class="built_in">sin</span>(th),<span class="string">&#x27;g&#x27;</span>,th,<span class="built_in">cos</span>(th),<span class="string">&#x27;b:&#x27;</span>);</span><br></pre></td></tr></table></figure><p><img src="/image/matlab使用方法再探/1.svg" alt="1"></p><p>plot 命令返回一个标识图形目标的向量，其元素称为句柄，这些句柄可以确定目标的特性.若想知道第一条曲线的特性，输入<code>get(fig1(1))</code>，不要关闭Figure 1窗口.</p><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">输出</summary><pre><code class="matlab">    AlignVertexCenters: off            Annotation: [1×1 matlab.graphics.eventdata.Annotation]          BeingDeleted: off            BusyAction: 'queue'         ButtonDownFcn: ''              Children: [0×0 GraphicsPlaceholder]              Clipping: on                 Color: [0 1 0]             ColorMode: 'manual'           ContextMenu: [0×0 GraphicsPlaceholder]             CreateFcn: ''       DataTipTemplate: [1×1 matlab.graphics.datatip.DataTipTemplate]             DeleteFcn: ''           DisplayName: ''      HandleVisibility: 'on'               HitTest: on         Interruptible: on              LineJoin: 'round'             LineStyle: '-'         LineStyleMode: 'auto'             LineWidth: 0.5000                Marker: 'none'       MarkerEdgeColor: 'auto'       MarkerFaceColor: 'none'         MarkerIndices: [1×100 uint64]            MarkerMode: 'auto'            MarkerSize: 6                Parent: [1×1 Axes]         PickableParts: 'visible'              Selected: off    SelectionHighlight: on           SeriesIndex: 1                   Tag: ''                  Type: 'line'              UserData: []               Visible: on                 XData: [1×100 double]             XDataMode: 'manual'           XDataSource: ''                 YData: [1×100 double]           YDataSource: ''                 ZData: [1×0 double]           ZDataSource: ''</code></pre></details><p>若想了解第二第曲线的特性，输入<code>get(fig1(2))</code>，不要关闭Figure 1窗口.</p><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">输出</summary><pre><code class="matlab">    AlignVertexCenters: off            Annotation: [1×1 matlab.graphics.eventdata.Annotation]          BeingDeleted: off            BusyAction: 'queue'         ButtonDownFcn: ''              Children: [0×0 GraphicsPlaceholder]              Clipping: on                 Color: [0 0 1]             ColorMode: 'manual'           ContextMenu: [0×0 GraphicsPlaceholder]             CreateFcn: ''       DataTipTemplate: [1×1 matlab.graphics.datatip.DataTipTemplate]             DeleteFcn: ''           DisplayName: ''      HandleVisibility: 'on'               HitTest: on         Interruptible: on              LineJoin: 'round'             LineStyle: ':'         LineStyleMode: 'manual'             LineWidth: 0.5000                Marker: 'none'       MarkerEdgeColor: 'auto'       MarkerFaceColor: 'none'         MarkerIndices: [1×100 uint64]            MarkerMode: 'manual'            MarkerSize: 6                Parent: [1×1 Axes]         PickableParts: 'visible'              Selected: off    SelectionHighlight: on           SeriesIndex: 2                   Tag: ''                  Type: 'line'              UserData: []               Visible: on                 XData: [1×100 double]             XDataMode: 'manual'           XDataSource: ''                 YData: [1×100 double]           YDataSource: ''                 ZData: [1×0 double]           ZDataSource: ''</code></pre></details><p>对于图形本身，由于是第一的图形，所以句柄是1，输入<code>get(1)</code></p><details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">输出</summary><pre><code class="matlab">                 Alphamap: [1×64 double]             BeingDeleted: off               BusyAction: 'queue'            ButtonDownFcn: ''                 Children: [1×1 Axes]                 Clipping: on          CloseRequestFcn: 'closereq'                    Color: [0.9400 0.9400 0.9400]                 Colormap: [256×3 double]              ContextMenu: [0×0 GraphicsPlaceholder]                CreateFcn: ''              CurrentAxes: [1×1 Axes]         CurrentCharacter: ''            CurrentObject: [1×1 Line]             CurrentPoint: [408 289]                DeleteFcn: ''             DockControls: on                 FileName: '/home/pacaep/aepBlog/source/image/plot使用方法再探/1.svg'        GraphicsSmoothing: on         HandleVisibility: 'on'                     Icon: ''            InnerPosition: [200 200 600 472]            IntegerHandle: on            Interruptible: on           InvertHardcopy: on              KeyPressFcn: ''            KeyReleaseFcn: ''                  MenuBar: 'figure'                     Name: ''                 NextPlot: 'add'                   Number: 1              NumberTitle: on            OuterPosition: [200 200 600 557]         PaperOrientation: 'portrait'            PaperPosition: [1.1250 3.0417 6.2500 4.9167]        PaperPositionMode: 'auto'                PaperSize: [8.5000 11]                PaperType: 'usletter'               PaperUnits: 'inches'                   Parent: [1×1 Root]                  Pointer: 'arrow'        PointerShapeCData: [16×16 double]      PointerShapeHotSpot: [8 8]                 Position: [200 200 600 472]                 Renderer: 'opengl'             RendererMode: 'auto'                   Resize: on               Scrollable: off            SelectionType: 'alt'           SizeChangedFcn: ''                      Tag: ''                  ToolBar: 'auto'                     Type: 'figure'                    Units: 'pixels'                 UserData: []                  Visible: on      WindowButtonDownFcn: ''    WindowButtonMotionFcn: ''        WindowButtonUpFcn: ''        WindowKeyPressFcn: ''      WindowKeyReleaseFcn: ''     WindowScrollWheelFcn: ''              WindowState: 'normal'              WindowStyle: 'normal'                 XDisplay: ':0'</code></pre></details>为了解图形窗口的设置，输入`set(1)``set(H,Name,Value)` 为 H 标识的对象指定其 Name 属性的值从图像中删去余弦曲线，输入`delete(fig1(2))``refresh`命令可以刷新图像.建立一个新的图形窗口，保留前一个图形窗口，输入`H=figure`，将打开新的图形窗口并设定为活动，输入`gcf`获得当前图形的句柄.<details class="custom-block details" style="display: block; position: relative; border-radius: 2px; margin: 1.6em 0px; padding: 1.6em; background-color: rgb(238, 238, 238); color: rgb(44, 62, 80); font-family: -apple-system, BlinkMacSystemFont, &quot;Segoe UI&quot;, Roboto, Oxygen, Ubuntu, Cantarell, &quot;Fira Sans&quot;, &quot;Droid Sans&quot;, &quot;Helvetica Neue&quot;, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"><summary style="outline: none; cursor: pointer;">输出</summary><pre><code class="matlab">  Figure (2) - 属性:      Number: 2        Name: ''       Color: [0.9400 0.9400 0.9400]    Position: [200 200 600 472]       Units: 'pixels'</code></pre></details>]]></content>
      
      
      <categories>
          
          <category> 使用教程 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>拉格朗日插值多项式</title>
      <link href="/2021/10/06/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E5%A4%9A%E9%A1%B9%E5%BC%8F/"/>
      <url>/2021/10/06/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E6%8F%92%E5%80%BC%E5%A4%9A%E9%A1%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在数值分析中，多项式插值是最常提到的内容</p><span id="more"></span><h2 id="多项式插值"><a href="#多项式插值" class="headerlink" title="多项式插值"></a>多项式插值</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><blockquote><p> 给定有序点集 ${x_0,x_1,\cdots,x_n}$ 且 $x_0 \lt x_1\lt \cdots\lt x_n$ 以及定义在 $x_0,x_n$ 上的连续函数$f(x)$或对应于 $x$ 值的 $y$ 值集合 $y_0,y_1,\cdots,y_n$ (即 $(x_i,y_i)$) 对.<br>多项式插值就是要求出对数据 $(x_0,y_0),(x_1,y_1),\cdots,(x_n,y_n)$ 进行插值的次数最多为 $n$ 的多项式 $p(x)$ ,即</p><script type="math/tex; mode=display">y_0 = p(x_0)</script><script type="math/tex; mode=display">y_1 = p(x_1)</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">y_n = p(x_n)</script><p> 称 $p$ 在 ${x_0,x_1,\cdots,x_n}$ 上插值 $f$ ，且 $p$ 是插值. 插值必须在相应点 ${x_0,x_1,\cdots,x_n}$ 上与函数 $f$ 或  $y_0,y_1,\cdots,y_n$ 取值相同，这些点通常称为节点(或分割点).<br> 则插值多项式为</p><script type="math/tex; mode=display">p(x) = a_n x^n + a_n-1 x^n-1 + \cdots + a_1 x + a_0</script><p> 插值条件为</p><script type="math/tex; mode=display">a_n x_0^n + a_n-1 x_0^n-1 + \cdots +a_1 x_0 + a_0 = y_0</script><script type="math/tex; mode=display">a_n x_1^n + a_n-1 x_1^n-1 + \cdots +a_1 x_1 + a_0 = y_1</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">a_n x_n^n + a_n-1 x_n^n-1 + \cdots +a_1 x_n + a_0 = y_n</script><p> 其中矩阵</p><script type="math/tex; mode=display">V = \begin{bmatrix}1&x_0 &x_0^2 & \cdots & x_0^n \\1&x_1 &x_1^2 & \cdots & x_1^n \\ 1&x_2 &x_2^2 & \cdots & x_2^n \\ \vdots & \vdots & \vdots & \ddots & \vdots \\ 1&x_n &x_n^2 & \cdots & x_n^n \end{bmatrix}</script><p> 由 $x$ 构成，形如 $V$ 的矩阵称为 $Vandermonde$ 矩阵，如果 $x_i$ 互异，则 $Vandermonde$ 矩阵非奇异，因此多项式插值存在且唯一. 插值多项式的系数由方程</p><script type="math/tex; mode=display">V \begin{bmatrix} a_0 \\ a_1 \\ \vdots \\ a_n\end{bmatrix} = \begin{bmatrix} y_0 \\ y_1 \\ \vdots \\ y_n\end{bmatrix}</script><p> 的解给出，可以采用Vandermonde方程组的专门解法解出.</p></blockquote><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><blockquote><p>对正弦曲线上的数据点 $(0,0),( \pi / 2,1),( \pi ,0),(3 \pi / 2, -1)$进行多项式插值，这里 ${x_0,x_1,x_2,x_3 } = { 0,\pi / 2,\pi ,3 \pi /2 } , {y_0,y_1,y_2,y_3 } = { 0,1,0,-1 } $,求解</p><script type="math/tex; mode=display">\begin{bmatrix} 1&0&0&0 \\ 1&\frac{\pi}{2} & \frac{\pi^2}{4} & \frac{\pi^3}{8} \\ 1 & \pi & \pi^2 & \pi^3 \\ 1 & \frac{3\pi}{2} & \frac{9\pi^2}{4} & \frac{27\pi^3}{8}\end{bmatrix}  \begin{bmatrix} a_0 \\ a_1 \\ a_2 \\ a_3\end{bmatrix} = \begin{pmatrix} 0 \\ 1 \\ 0 \\ -1 \end{pmatrix}</script><p>得 $ a_0,a_1,a_2,a_3 $<br><code>V\y</code></p><script type="math/tex; mode=display">\begin{bmatrix} a_0 \\ a_1 \\ a_2 \\ a_3 \end{bmatrix} = \begin{pmatrix} 0 \\ 1.6977 \\ -0.8106 \\ 0.0860 \end{pmatrix}</script><p>得</p><script type="math/tex; mode=display">p(x) = 0.0860 x^3 - 0.8106 x^2 +1.6997 x</script><p>验证<br><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="built_in">linspace</span>(<span class="number">0</span>,<span class="number">3</span>/<span class="number">2</span>*<span class="built_in">pi</span>);</span><br><span class="line">y = <span class="built_in">sin</span>(x);</span><br><span class="line">a = [<span class="number">0.0860</span> <span class="number">-0.8106</span> <span class="number">1.6977</span> <span class="number">0</span>];</span><br><span class="line">p = polyval(a,x);</span><br><span class="line"><span class="built_in">plot</span>(x,y,<span class="string">&#x27;k-&#x27;</span>,x,p,<span class="string">&#x27;r--&#x27;</span>);</span><br><span class="line"><span class="built_in">legend</span>(<span class="string">&#x27;sin(x)&#x27;</span>,<span class="string">&#x27;polyval(a,x)&#x27;</span>)</span><br></pre></td></tr></table></figure></p></blockquote><p><img src="/image/拉格朗日插值多项式/1.svg" alt="1"></p><h3 id="Runge函数"><a href="#Runge函数" class="headerlink" title="Runge函数"></a>Runge函数</h3><blockquote><p>多项式插值近似函数的效果似乎不错，但有时很差，因为Vandermonde矩阵一般是病态的，即使求解过程是精确的.<br>比如高阶多项式插值的振荡问题，即在节点间出现大的迂回.<br><strong>Runge函数</strong></p><script type="math/tex; mode=display">f(x) = \frac{1}{1 + 25 x^2}</script><p>图中画出了Runge函数以及它的10次插值多项式，函数的振幅会随着n的增大而增大，近似程度将越来越差， $x \in (-1,1)$ 时</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = <span class="number">-1</span>:<span class="number">0.2</span>:<span class="number">1</span>;</span><br><span class="line">x_ = <span class="built_in">linspace</span>(<span class="number">-1</span>,<span class="number">1</span>);</span><br><span class="line">x = x&#x27;;</span><br><span class="line">y = <span class="number">1.</span>/(<span class="number">1</span>+<span class="number">25.</span>*(x.^<span class="number">2</span>));</span><br><span class="line">y_ = <span class="number">1.</span>/(<span class="number">1</span>+<span class="number">25.</span>*(x_.^<span class="number">2</span>));</span><br><span class="line">A = <span class="built_in">ones</span>(<span class="number">11</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">10</span></span><br><span class="line">A = [A x.^<span class="built_in">i</span>];</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">a = A\y;</span><br><span class="line">a = a&#x27;;</span><br><span class="line">a = a(:,<span class="keyword">end</span>:<span class="number">-1</span>:<span class="number">1</span>);</span><br><span class="line">x = <span class="number">-1</span>:<span class="number">0.001</span>:<span class="number">1</span>;</span><br><span class="line">p=<span class="built_in">zeros</span>(<span class="number">11</span>,<span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">10</span>:<span class="number">-1</span>:<span class="number">0</span></span><br><span class="line">p = p+a(<span class="number">1</span>,<span class="number">11</span>-<span class="built_in">i</span>)*(x.^<span class="built_in">i</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">plot</span>(x_,y_,<span class="string">&#x27;k-&#x27;</span>,x,p,<span class="string">&#x27;r--&#x27;</span>),<span class="built_in">legend</span>(<span class="string">&#x27;1/(1+25*(x^2))&#x27;</span>,<span class="string">&#x27;p(x)&#x27;</span>),xlim([<span class="number">-1</span>,<span class="number">1</span>]),ylim([<span class="number">-0.5</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure></blockquote><p>​    <img src="/image/拉格朗日插值多项式/2.svg" alt="2"></p><blockquote><p>因此需要推导和理解其他方法.</p></blockquote><h2 id="拉格朗日插值多项式"><a href="#拉格朗日插值多项式" class="headerlink" title="拉格朗日插值多项式"></a>拉格朗日插值多项式</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><blockquote><p>假定要对两个互异点 $x_a,x_b$ 进行直线插值，先定义函数 $ L_a(x) $ 和 $ L_b(x) $</p><script type="math/tex; mode=display">L_a(x) = \frac{x - x_b }{x_a - x_b}</script><script type="math/tex; mode=display">L_b(x) = \frac{x - x_a }{x_b - x_a}</script><p>则</p><script type="math/tex; mode=display">L_a(x_a ) = 1,\quad L_a(x_b = 0)</script><script type="math/tex; mode=display">L_b(x_a ) = 1,\quad L_b(x_b = 0)</script><p>且<br>$ L_a(x) $和 $ L_b(x) $ 都是 $x$ 的线性函数.给定 $y_a = f(x_a )$ 和 $y_b = f(x_b )$，则</p><script type="math/tex; mode=display">l(x) = y_a L_a (x) + y_b L_b (x)</script><p>满足</p><script type="math/tex; mode=display">\begin{aligned} l(x_a ) ={}& y_a L_a (x_a) + y_b L_b (x_a) {} \\ ={}& y_a \cdot 1 + y_b \cdot 0 \\ ={}& y_a \end{aligned}</script><script type="math/tex; mode=display">\begin{aligned} l(x_b ) ={}& y_a L_a (x_b) + y_b L_b (x_b) {} \\ ={}& y_a \cdot 1 + y_b \cdot 0 \\ ={}& y_b \end{aligned}</script><p>即 $l(x)$ 是 $f$ 在 $(x_a , y_a )$ 和 $(x_b,y_b)$ 上的插值.由于 $l(x)$ 是两个线性函数的和，所以是线性的，又因为过互异两点的直线只有一条，所以 $l(x)$ 是唯一的.<br>将这种做法推广到 $n+1$ 个互异点 $x_0,x_1,\cdots,x_n $(有序且 $ x_0 \lt x_1 \lt \cdots \lt x_n $ )，定义函数</p><script type="math/tex; mode=display">\begin{aligned} L_{n,i} (x) ={}& \frac{(x-x_0)(x-x_1) \cdots (x-x_{i-1})(x-x_{i+1}) \cdots (x-x_n)}{(x_i-x_0)(x_i-x_1) \cdots (x_i-x_{i-1})(x_i-x_{i+1}) \cdots (x_i-x_n)} \\ ={}& \prod_{k=0 \\ k\neq i}^{n} \frac{(x-x_k)}{(x_i - x_k)} \end{aligned}</script><p>当 $n \neq i$ 时，有</p><script type="math/tex; mode=display">L_{n,i} (x_n) = 0</script><script type="math/tex; mode=display">L_{n,i} (x_i) = 1</script><p>上式为Lagrange多项式.<br>由于 $n$ 次多项式的和是次数最多为 $n$ 的多项式，所以用Lagrange多项式可以得到过点 $(x_0,y_0),(x_1,y_1), \cdots , (x_n,y_n) $ 的次数不超过 $n$ 的插值多项式 $p(x)$ </p><script type="math/tex; mode=display">\begin{aligned} p(x) ={}& y_0 L_{n,0} (x) + y_1 L_{n,1} (x) + \cdots + y_n L_{n,n} (x) \\ ={}& \sum_{i=1}^{n} y_i L_n (x) \end{aligned}</script><p>有 $L_i(x_j) = 0 $</p><script type="math/tex; mode=display">\begin{aligned} p(x) ={}& \sum_{i=0}^{n} y_i L_i (x_j) \\ ={}& y_0 L_0 (x_j) + y_1 L_1 (x_j) + \cdots + y_n L_n (x_j) \\ ={}& y_j \cdot 1 \\ ={}& y_j \end{aligned}</script><p>$p$ 就是所求的多项式插值，与Vandermonde矩阵的结果相同.</p></blockquote><h3 id="例题-1"><a href="#例题-1" class="headerlink" title="例题"></a>例题</h3><blockquote><p>对正弦曲线上的数据点 $(0,0),( \pi / 2,1),( \pi ,0),(3 \pi / 2, -1)$进行多项式插值，这里 ${x_0,x_1,x_2,x_3 } = { 0,\pi / 2,\pi ,3 \pi /2 } , {y_0,y_1,y_2,y_3 } = { 0,1,0,-1 } $,Lagrange多项式</p><script type="math/tex; mode=display">\begin{aligned}L_0(x) ={}& \frac{(x-x_1)(x-x_2)(x-x_3)}{(x_0-x_1)(x_0-x_2)(x_0-x_3)} \\ ={}& - \frac{4}{3\pi^3} (x-\frac{\pi}{2})(x-\pi)(x-\frac{3\pi}{2}) \end{aligned}</script><script type="math/tex; mode=display">L_1(x) = \frac{4}{\pi^3}x(x-\pi)(x-\frac{3\pi}{2})</script><script type="math/tex; mode=display">L_2(x) = - \frac{4}{3\pi^3} (x-\frac{\pi}{2})(x-\frac{3\pi}{2})</script><script type="math/tex; mode=display">L_3(x) = \frac{4}{3\pi^3} (x-\frac{\pi}{2})(x-\pi)</script><p>插值多项式为</p><script type="math/tex; mode=display">\begin{aligned} p(x) ={}& 0 \cdot L_0(x) + 1 \cdot L_1(x) + 0 \cdot L_2(x) + (-1) \cdot L_3(x) \\ ={}& 0.0860 x^3 - 0.8106 x^2 +1.6997 x \end{aligned}</script><p>同前</p></blockquote><h3 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h3><h5 id="Cauchy余项定理"><a href="#Cauchy余项定理" class="headerlink" title="Cauchy余项定理"></a>Cauchy余项定理</h5><blockquote><p>给定 $a \leq x_0 \lt x_1 \lt \cdots \lt x_n \leq b$ 及 $f \in C^{n+1}[a,b]$，则对任意 $x \in [a,b]$，都存在 $\xi = \xi(x) \in [a,b]$，使$f$在$x_0,x_1,\cdots,x_n$ 上的插值多项式 $p(x)$ 满足</p><script type="math/tex; mode=display">f(x) - p(x) = \frac{f^{n+1}(\xi)}{(n+1)!}(x-x_0)(x-x_1)\cdots(x-x_n)</script><p>证明略</p></blockquote><h4 id="Weierstrass定理"><a href="#Weierstrass定理" class="headerlink" title="Weierstrass定理"></a>Weierstrass定理</h4><blockquote><p>若$f \in C[a,b]$，则对任意给定的 $\varepsilon \gt 0$ 都存在多项式 $p(x)$ ,使对任意 $x \in [a,b]$，有</p><script type="math/tex; mode=display">\left | f(x) - p(x) \right | \leq \varepsilon</script><p>说明任何连续函数都可以被高次多项式任意逼近.</p></blockquote><h2 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h2><p>Lagrange只讨论了求出插值多项式系数的方法，区间外，Lagrange插值是不准确的，如果问题中要求多项式的值，可以使用更为有效和精确的方法.</p>]]></content>
      
      
      <categories>
          
          <category> 数值分析 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多项式插值 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
