<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>CUDA编程: CUDA内存管理（三） | Aeeeeeep Blog | The Gleaners</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.0.0"></head><body><main class="content"><section class="outer"><article id="post-CUDA编程-CUDA内存管理（三）" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">CUDA编程: CUDA内存管理（三）</h1></header><div class="article-meta"><a href="/2024/01/23/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%89%EF%BC%89/" class="article-date"><time datetime="2024-01-23T13:34:14.000Z" itemprop="datePublished">2024-01-23</time></a><div class="article-category"><a class="article-category-link" href="/categories/CUDA-%E7%BC%96%E7%A8%8B/">CUDA 编程</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>CUDA C 编程中的简单的内存管理，线程操作，如何编写核函数，使用 Thrust 库，并行计算，性能分析工具。</p><span id="more"></span><h2 id="获取-GPU-信息"><a href="#获取-GPU-信息" class="headerlink" title="获取 GPU 信息"></a>获取 GPU 信息</h2><p>CUDA 提供了几种获取 GPU 信息的方法，这里介绍一下通过调用 <code>cuda_runtime.h</code>中的 API 得到 GPU 的一些属性。</p><blockquote><p>在编写 CUDA C 程序时， 要将文件命名为 <code>*.cu</code>，一般使用 nvcc 命令编译运行，为 CUDA程序文件，支持 C/C++ 语法。</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    cudaDeviceProp devProp;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;devProp, dev);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;GPU Device Name&quot;</span> &lt;&lt; dev &lt;&lt; <span class="string">&quot;: &quot;</span> &lt;&lt; devProp.name &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;SM Count: &quot;</span> &lt;&lt; devProp.multiProcessorCount &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Shared Memory Size per Thread Block: &quot;</span> &lt;&lt; devProp.sharedMemPerBlock / <span class="number">1024.0</span> &lt;&lt; <span class="string">&quot; KB&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per Thread Block: &quot;</span> &lt;&lt; devProp.maxThreadsPerBlock &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Threads per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Warps per SM: &quot;</span> &lt;&lt; devProp.maxThreadsPerMultiProcessor / <span class="number">32</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编译命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc checkDeviceInfor.cu -o checkDeviceInfor</span><br></pre></td></tr></table></figure><p>输出如下，SM 数量为 30，每个线程块的共享内存为 48KB，每个线程块有 1024 个线程，每个 SM 有 1536 个线程，每个 SM 有 48 个线程束</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU Device Name0: NVIDIA GeForce RTX 3060 Laptop GPU</span><br><span class="line">SM Count: 30</span><br><span class="line">Shared Memory Size per Thread Block: 48 KB</span><br><span class="line">Threads per Thread Block: 1024</span><br><span class="line">Threads per SM: 1536</span><br><span class="line">Warps per SM: 48</span><br></pre></td></tr></table></figure><h2 id="初步内存管理"><a href="#初步内存管理" class="headerlink" title="初步内存管理"></a>初步内存管理</h2><p>主机和设备各自拥有独立的内存，C 拥有标准库可以管理主机的内存，CUDA 提供的 API 管理设备的内存，下面是 C 和 CUDA 的部分内存管理函数</p><div class="table-container"><table><thead><tr><th>C</th><th>CUDA</th><th>功能</th></tr></thead><tbody><tr><td>malloc</td><td>cudaMalloc</td><td>分配内存</td></tr><tr><td>memcpy</td><td>cudaMemcpy</td><td>复制内存</td></tr><tr><td>memset</td><td>cudaMemset</td><td>设置内存</td></tr><tr><td>free</td><td>cudaFree</td><td>释放内存</td></tr></tbody></table></div><h3 id="主机与设备的数据拷贝"><a href="#主机与设备的数据拷贝" class="headerlink" title="主机与设备的数据拷贝"></a>主机与设备的数据拷贝</h3><p>下面的程序举例了如何使用进行主机与设备的数据拷贝，使用了 <code>cudaMalloc</code>，<code>cudaMemcpy</code> 和 <code>cudaFree</code> 函数，函数形参如下</p><ul><li><p><code>__host__ cudaError_t cudaMalloc (void** devPtr, size_t size)</code></p><ul><li><code>devPtr</code>: 开辟数据的首指针</li><li><code>size</code>: 开辟的设备内存空间长度</li></ul></li><li><p><code>__host__ cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind)</code></p><ul><li><code>dst</code>: 目的数据内存首指针</li><li><code>src</code>: 源数据首指针</li><li><code>count</code>: 数据长度</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li><li><p><code>__host__ cudaError_t cudaFree (void* devPtr)</code></p></li><li><code>devPtr</code>: 设备变量指针</li></ul><p>上述函数的返回值类型都是 <code>cudaError_t</code>，以枚举形式保存各种错误类型</p><p>更多运行时函数详解见<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-runtime-api/">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;dev_dets, <span class="built_in">sizeof</span>(dets));</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(dev_dets, dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(&amp;host_dets, dev_dets, <span class="built_in">sizeof</span>(dets), cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>);i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>上面的程序使用<code>cudaMalloc</code>来申请设备内存，但二维数组不推荐这么做，在 kernel 运算时较高的性能损失，CUDA 给出了二维数组专用的内存申请函数<code>cudaMallocPitch</code>，在设备间内存拷贝时，也要使用<code>cudaMemcpy2D</code>函数，形参如下</p><ul><li><code>__host__cudaError_t cudaMallocPitch ( void** devPtr, size_t* pitch, size_t width, size_t height )</code><ul><li><code>devPtr</code>: 开辟矩阵的数据的首指针</li><li><code>pitch</code>: 分配存储器的宽度</li><li><code>width</code>: 二维数组列数</li><li><code>height</code>: 二维数组行数</li></ul></li><li><code>__host__ cudaError_t cudaMemcpy2D ( void* dst, size_t dpitch, const void* src, size_t spitch, size_t width, size_t height, cudaMemcpyKind kind )</code><ul><li><code>dst</code>: 目的矩阵内存首指针</li><li><code>dpitch</code>: dst指向的 2D 数组中的内存宽度，以字节为单位，是cuda为了读取方便，对齐过的内存宽度，可能大于一行元素占据的实际内存</li><li><code>src</code>: 源矩阵内存首指针</li><li><code>spitch</code>: src 指向的 2D 数组中的内存宽度</li><li><code>width</code>: src指向的2D数组中一行元素占据的实际宽度，为 <code>width*sizeof(type)</code></li><li><code>height</code>: src指向的2D数组的行数</li><li><code>kind</code>: 拷贝类型，<code>cudaMemcpyDeviceToHost</code>: 从设备向主机拷贝 | <code>cudaMemcpyDeviceToHost</code>: 从主机向设备拷贝 | <code>cudaMemcpyHostToHost</code>: 从主机向主机拷贝 | <code>cudaMemcpyDeviceToDevice</code>: 从设备向设备拷贝</li></ul></li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> dets[<span class="number">6</span>][<span class="number">4</span>] = &#123;</span><br><span class="line">        &#123;<span class="number">23</span>, <span class="number">34</span>, <span class="number">56</span>, <span class="number">76</span>&#125;,</span><br><span class="line">        &#123;<span class="number">11</span>, <span class="number">23</span>, <span class="number">45</span>, <span class="number">45</span>&#125;,</span><br><span class="line">        &#123;<span class="number">12</span>, <span class="number">22</span>, <span class="number">47</span>, <span class="number">47</span>&#125;,</span><br><span class="line">        &#123;<span class="number">9</span>, <span class="number">45</span>, <span class="number">56</span>, <span class="number">65</span>&#125;,</span><br><span class="line">        &#123;<span class="number">20</span>, <span class="number">37</span>, <span class="number">55</span>, <span class="number">75</span>&#125;,</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="type">size_t</span> width = <span class="number">4</span>;</span><br><span class="line">    <span class="type">size_t</span> height = <span class="number">6</span>;</span><br><span class="line">    <span class="type">size_t</span> pitch;</span><br><span class="line">    </span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">sizeof</span>(dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> *dev_dets;</span><br><span class="line">    cudaError_t err = cudaSuccess;</span><br><span class="line">    err = <span class="built_in">cudaMallocPitch</span>((<span class="type">void</span> **)&amp;dev_dets, &amp;pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height);</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess) &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;cudaMalloc failed!&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// copy data to gpu</span></span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(dev_dets, pitch, dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyHostToDevice);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied data to GPU.\n&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// get back copied cuda data</span></span><br><span class="line">    <span class="type">float</span> host_dets[<span class="built_in">sizeof</span>(dets)/<span class="built_in">sizeof</span>(<span class="type">float</span>)];</span><br><span class="line">    <span class="built_in">cudaMemcpy2D</span>(&amp;host_dets, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, dev_dets, pitch, <span class="built_in">sizeof</span>(<span class="type">float</span>)*width, height,cudaMemcpyDeviceToHost);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Copied from cuda back to host.\n&quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;host_dets size: &quot;</span> &lt;&lt; <span class="built_in">sizeof</span>(host_dets) &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>;i&lt;width*height;i++) &#123;</span><br><span class="line">        std::cout &lt;&lt; host_dets[i] &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaFree</span>(dev_dets);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;done.\n&quot;</span>;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">96</span></span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: <span class="number">96</span></span><br><span class="line"><span class="number">23</span> <span class="number">34</span> <span class="number">56</span> <span class="number">76</span> <span class="number">11</span> <span class="number">23</span> <span class="number">45</span> <span class="number">45</span> <span class="number">12</span> <span class="number">22</span> <span class="number">47</span> <span class="number">47</span> <span class="number">9</span> <span class="number">45</span> <span class="number">56</span> <span class="number">65</span> <span class="number">20</span> <span class="number">37</span> <span class="number">55</span> <span class="number">75</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> </span><br><span class="line">done.</span><br></pre></td></tr></table></figure><p>这两个函数应该会使 kernel 的运行时间变短，因为 pitch 对齐后可实现 global 内存联合访问，但<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>会变慢，因为比一维的操作多了对齐的考虑</p><h2 id="Kernel-函数"><a href="#Kernel-函数" class="headerlink" title="Kernel 函数"></a>Kernel 函数</h2><h3 id="kernel-限定词"><a href="#kernel-限定词" class="headerlink" title="kernel 限定词"></a>kernel 限定词</h3><ul><li><code>__device__</code>: 在设备上执行，只能在设备上调用；</li><li><code>__global__</code>: 在设备上执行，只能在主机上调用；</li><li><code>__host__</code>: 在主机上执行，只能在主机上调用。</li></ul><p><code>__device__</code>和<code>__global__</code>代表函数在设备上执行，不支持递归，不能在函数体内声明静态变量，静态变量对应于CPU的整个程序生命过程，不能有可变长参数；</p><p><code>__global__</code>和<code>__host__</code>不能一起使用，而<code>__device__</code>和<code>__host__</code>可以一起使用，编译器会在 CPU 和 GPU 各复制一份函数。</p><p>不添加限定词时，函数默认为<code>__host__</code>，也就是在主机上执行。</p><p>所有的 kernel 函数返回类型都是 void，且 kernel 函数都是异步执行。</p><h3 id="kernel-调用方式"><a href="#kernel-调用方式" class="headerlink" title="kernel 调用方式"></a>kernel 调用方式</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel_func</span> <span class="params">(param list)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line">kernel_func &lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt; (param list);</span><br></pre></td></tr></table></figure><ul><li><code>&lt;&lt;&lt;Dg, Db, Ns, S&gt;&gt;&gt;</code>: 是运算符内是核函数的执行参数，告诉编译器运行时如何启动核函数</li><li><code>Dg</code>: grid 的维度和尺寸，dim3 类型，意为一个 grid 有多少个 block</li><li><code>Db</code>: block 的维度和尺寸， dim3 类型，意为一个 block 有多少个 thread</li><li><code>Ns</code>: （可选）用于设置每个block除了静态分配的 shared Memory 以外，最多能动态分配的 shared Memory 大小，单位为 byte 不需要动态分配时该值为0或省略不写</li><li><code>S</code>: （可选） cudastream 类型的参数，表示该核函数处在哪个流之中</li></ul><p>这里我们实现一下第二章最后的例子，下面的程序使用了<code>cudaDeviceSynchronize</code>和<code>cudaDeviceReset</code>函数，解释如下</p><ul><li><code>__host__ __device__ cudaDeviceSynchronize</code>: 使设备阻塞到完成所有前面请求的任务，CUDA 11.6 后已弃用</li><li><code>__host__ cudaDeviceReset</code>: 显式销毁并清除当前进程中与设备关联的所有资源，资源不能再被访问，可能导致未定义的行为</li></ul><p>由于 CUDA printf 的输出存储在缓冲中，后台同步机制会有延时，需要使用上面两个同步函数中任意一个使 printf 函数的内容与主机同步，即可输出</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">printThreadIndex</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> ix = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="type">int</span> iy = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> idx = iy*blockDim.x * gridDim.x + ix;</span><br><span class="line">    <span class="keyword">if</span>(threadIdx.x == <span class="number">3</span> &amp;&amp; threadIdx.y == <span class="number">1</span> &amp;&amp; blockIdx.x == <span class="number">0</span> &amp;&amp; blockIdx.y == <span class="number">1</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;thread_id (%d,%d) block_id (%d,%d) coordinate (%d, %d), global index %2d \n&quot;</span>, threadIdx.x, threadIdx.y, blockIdx.x, blockIdx.y, ix, iy, idx);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)</span>, <span class="title">block</span><span class="params">(<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    printThreadIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line">    <span class="comment">// cudaDeviceSynchronize(); </span></span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="built_in">thread_id</span> (<span class="number">3</span>,<span class="number">1</span>) <span class="built_in">block_id</span> (<span class="number">0</span>,<span class="number">1</span>) <span class="built_in">coordinate</span> (<span class="number">3</span>, <span class="number">3</span>), global index <span class="number">27</span></span><br></pre></td></tr></table></figure><blockquote><p>在输出时不能使用 std::cout, std 命名空间不能使用到 GPU 上</p></blockquote><h2 id="CUDA-的-Thrust-库"><a href="#CUDA-的-Thrust-库" class="headerlink" title="CUDA 的 Thrust 库"></a>CUDA 的 Thrust 库</h2><p>CUDA 的 Thrust 库是基于标准模板库 STL 的 CUDA 的 C++ 模板库， 通过与 CUDA C 配合使用，节省了大量优化算法的时间，保证了性能与开发效率，在 CUDA Toolkit 中包含 Thrust，无需额外安装，只需导入相应头文件，在调用时使用 <code>thrust</code> 命名空间，并尽量不要使用 <code>using namespace std;</code> 语句，因为 thrust 库和 STL 库非常多的重名</p><h3 id="Vector-容器"><a href="#Vector-容器" class="headerlink" title="Vector 容器"></a>Vector 容器</h3><p>Thrust 中定义了主机端和设备端的两种 vector，分别定义在 host_vector.h 和 device_vector.h 中，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// H has storage for 4 integers</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(<span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize individual elements</span></span><br><span class="line">    H[<span class="number">0</span>] = <span class="number">14</span>;</span><br><span class="line">    H[<span class="number">1</span>] = <span class="number">20</span>;</span><br><span class="line">    H[<span class="number">2</span>] = <span class="number">38</span>;</span><br><span class="line">    H[<span class="number">3</span>] = <span class="number">46</span>;</span><br><span class="line">    H.<span class="built_in">push_back</span>(<span class="number">52</span>);</span><br><span class="line">    <span class="comment">// H.size() returns the size of vector H</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// print contents of H</span></span><br><span class="line">    <span class="comment">// for(int i = 0; i &lt; H.size(); i++)</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:H)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// resize H</span></span><br><span class="line">    H.<span class="built_in">resize</span>(<span class="number">2</span>);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;H now has size &quot;</span> &lt;&lt; H.<span class="built_in">size</span>() &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// Copy host_vector H to device_vector D</span></span><br><span class="line">    thrust::device_vector&lt;<span class="type">int</span>&gt; D = H; </span><br><span class="line">    <span class="comment">// elements of D can be modified</span></span><br><span class="line">    D[<span class="number">0</span>] = <span class="number">99</span>;</span><br><span class="line">    D[<span class="number">1</span>] = <span class="number">88</span>;</span><br><span class="line">    <span class="comment">// print contents of D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">    <span class="comment">// H and D are automatically deleted when the function returns</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">H has size <span class="number">5</span></span><br><span class="line"><span class="number">14</span></span><br><span class="line"><span class="number">20</span></span><br><span class="line"><span class="number">38</span></span><br><span class="line"><span class="number">46</span></span><br><span class="line"><span class="number">52</span></span><br><span class="line">H now has size <span class="number">2</span></span><br><span class="line"><span class="number">99</span></span><br><span class="line"><span class="number">88</span></span><br></pre></td></tr></table></figure><p>Thrust 允许使用 <code>=</code> 运算符对 <code>host_vector</code> 和 <code>device_vector</code> 的相互拷贝，也允许使用 <code>[i]</code> 下标访问 <code>device_vector</code> 的各个元素，但是用这种方法访问每一次都需要调用 <code>cudaMemcpy</code>，性能损失较大，应谨慎使用。 下面我们将介绍一些更有效的技术</p><p>下面展示 Thrust 提供的几种对 vector 操作的方法，包括初始化，赋值，<code>iterator</code></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize all ten integers of a device_vector to 1</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">D</span><span class="params">(<span class="number">10</span>, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the first seven elements of a vector to 9</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">begin</span>() + <span class="number">7</span>, <span class="number">9</span>);</span><br><span class="line">    <span class="comment">// initialize a host_vector with the first five elements of D</span></span><br><span class="line">    <span class="function">thrust::host_vector&lt;<span class="type">int</span>&gt; <span class="title">H</span><span class="params">(D.begin(), D.begin() + <span class="number">5</span>)</span></span>;</span><br><span class="line">    <span class="comment">// set the elements of H to 0, 1, 2, 3, ...</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// copy all of H back to the beginning of D</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(H.<span class="built_in">begin</span>(), H.<span class="built_in">end</span>(), D.<span class="built_in">begin</span>());</span><br><span class="line">    <span class="comment">// print D</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">auto</span> i:D)dd</span><br><span class="line">        std::cout &lt;&lt; i &lt;&lt; <span class="string">&quot; &quot;</span>;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">9</span> <span class="number">9</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br></pre></td></tr></table></figure><p>上面的程序使用了<code>thrust::fill</code>，当它对 <code>device_vector iterator</code> 操作时，会在编译时检查 <code>iterator</code> 在主机上还是在设备上，这个过程被称为静态调度，意味着调度过程没有运行时开销</p><h3 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h3><p>thrust 中定义了 <code>device_ptr</code> 数据类型，当传入函数的指针指向设备端内存时，需要用<code>device_ptr</code>进行封装</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">size_t</span> N = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// raw pointer to device memory</span></span><br><span class="line"><span class="type">int</span> * raw_ptr;</span><br><span class="line"><span class="built_in">cudaMalloc</span>((<span class="type">void</span> **) &amp;raw_ptr, N * <span class="built_in">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"><span class="comment">// wrap raw pointer with a device_ptr </span></span><br><span class="line"><span class="function">thrust::device_ptr&lt;<span class="type">int</span>&gt; <span class="title">dev_ptr</span><span class="params">(raw_ptr)</span></span>;</span><br><span class="line"><span class="comment">// use device_ptr in thrust algorithms</span></span><br><span class="line">thrust::<span class="built_in">fill</span>(dev_ptr, dev_ptr + N, (<span class="type">int</span>) <span class="number">0</span>);</span><br></pre></td></tr></table></figure><h3 id="数值操作"><a href="#数值操作" class="headerlink" title="数值操作"></a>数值操作</h3><h4 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h4><p>Transformations 是对一个输入范围中的每一个元素应用操作，将结果存储在给定范围中的方法，上面程序中已经 <code>thrust::fill</code> 就是一个 Transformations，它将范围内的所有元素设置为指定值。下面的程序用到了 <code>thrust::sequence</code>，<code>thrust::replace</code>，<code>thrust::transform</code>，更多 Transformations 请查看<a target="_blank" rel="noopener" href="https://thrust.github.io/doc/group__transformations.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sequence.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/copy.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/fill.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/replace.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// allocate three device_vectors with 10 elements</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">X</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Y</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">int</span>&gt; <span class="title">Z</span><span class="params">(<span class="number">10</span>)</span></span>;</span><br><span class="line">    <span class="comment">// initialize X to 0,1,2,3, ....</span></span><br><span class="line">    thrust::<span class="built_in">sequence</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>());</span><br><span class="line">    <span class="comment">// compute Y = -X</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">negate</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// fill Z with twos</span></span><br><span class="line">    thrust::<span class="built_in">fill</span>(Z.<span class="built_in">begin</span>(), Z.<span class="built_in">end</span>(), <span class="number">2</span>);</span><br><span class="line">    <span class="comment">// compute Y = X mod 2</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Z.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), thrust::<span class="built_in">modulus</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line">    <span class="comment">// replace all the ones in Y with tens</span></span><br><span class="line">    thrust::<span class="built_in">replace</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), <span class="number">1</span>, <span class="number">10</span>);</span><br><span class="line">    <span class="comment">// print Y</span></span><br><span class="line">    thrust::<span class="built_in">copy</span>(Y.<span class="built_in">begin</span>(), Y.<span class="built_in">end</span>(), std::<span class="built_in">ostream_iterator</span>&lt;<span class="type">int</span>&gt;(std::cout, <span class="string">&quot; &quot;</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> <span class="number">0</span> <span class="number">10</span> </span><br></pre></td></tr></table></figure><h4 id="SAXPY"><a href="#SAXPY" class="headerlink" title="SAXPY"></a>SAXPY</h4><p>SAXPY（Scalar Alpha X Plus Y）是一个在 BLAS（Basic Linear Algebra Subprograms）函数库提供中的函数，并且是一个并行向量处理机（vector processor）中常用的计算操作指令，为标量乘法和向量加法的组合，如 $y = a*x + y$，其中 $x$ 和 $y$ 为向量，$a$ 为标量常数。下面的程序定义了一个 functor 实现 SAXPY</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">saxpy_functor</span> &#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> a;</span><br><span class="line">    <span class="built_in">saxpy_functor</span>(<span class="type">float</span> _a) : <span class="built_in">a</span>(_a) &#123;&#125;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        <span class="type">float</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> <span class="type">float</span>&amp; x, <span class="type">const</span> <span class="type">float</span>&amp; y)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> a * x + y;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">saxpy</span><span class="params">(<span class="type">float</span> A, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; X, thrust::device_vector&lt;<span class="type">float</span>&gt;&amp; Y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// y = a * x + y</span></span><br><span class="line">    thrust::<span class="built_in">transform</span>(X.<span class="built_in">begin</span>(), X.<span class="built_in">end</span>(), Y.<span class="built_in">begin</span>(), Y.<span class="built_in">begin</span>(), <span class="built_in">saxpy_functor</span>(A));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Reductions"><a href="#Reductions" class="headerlink" title="Reductions"></a>Reductions</h4><p>使用 <code>thrust::reduce</code> 函数对一组数据进行操作，返回值为一个具体数值，下例就是对一组数据求和</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>, thrust::<span class="built_in">plus</span>&lt;<span class="type">int</span>&gt;());</span><br></pre></td></tr></table></figure><p>上列中<code>(int) 0</code>为计算的初始值，<code>thrust::plus&lt;int&gt;()</code>为操作符，当没有定义初始值和操作符时，它们是默认值，因此下面的两条语句和上面的等价，更多操作符请查看<a target="_blank" rel="noopener" href="https://nvidia.github.io/thrust/api/groups/group__reductions.html">官方文档</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>(), (<span class="type">int</span>) <span class="number">0</span>);</span><br><span class="line"><span class="type">int</span> sum = thrust::<span class="built_in">reduce</span>(D.<span class="built_in">begin</span>(), D.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p><code>thrust::transform_reduce</code>允许接受多个操作符来对一组数据求值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/transform_reduce.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/device_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/host_vector.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// square&lt;T&gt; computes the square of a number f(x) -&gt; x*x</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">square</span> &#123;</span><br><span class="line">    <span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">        T <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> T&amp; x)</span> <span class="type">const</span> </span>&#123; </span><br><span class="line">            <span class="keyword">return</span> x * x;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// initialize host array</span></span><br><span class="line">    <span class="type">float</span> x[<span class="number">4</span>] = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;;</span><br><span class="line">    <span class="comment">// transfer to device</span></span><br><span class="line">    <span class="function">thrust::device_vector&lt;<span class="type">float</span>&gt; <span class="title">d_x</span><span class="params">(x, x + <span class="number">4</span>)</span></span>;</span><br><span class="line">    <span class="comment">// setup arguments</span></span><br><span class="line">    square&lt;<span class="type">float</span>&gt; unary_op;</span><br><span class="line">    thrust::plus&lt;<span class="type">float</span>&gt; binary_op;</span><br><span class="line">    <span class="type">float</span> init = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// compute norm</span></span><br><span class="line">    <span class="type">float</span> norm = std::<span class="built_in">sqrt</span>( thrust::<span class="built_in">transform_reduce</span>(d_x.<span class="built_in">begin</span>(), d_x.<span class="built_in">end</span>(), unary_op, init, binary_op));</span><br><span class="line">    std::cout &lt;&lt; norm &lt;&lt; std::endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line"><span class="number">5.47723</span></span><br></pre></td></tr></table></figure><p>上面的程序对一组数据计算平方和再开方，这种写法会大大优化性能。</p><h4 id="Sorting"><a href="#Sorting" class="headerlink" title="Sorting"></a>Sorting</h4><p>对数据进行排序，很常用的排序功能，举例如下</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/sort.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/functional.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort</span>(A, A + N);</span><br><span class="line"><span class="comment">// A is now &#123;1, 2, 4, 5, 7, 8&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span>    keys[N] = &#123;  <span class="number">1</span>,   <span class="number">4</span>,   <span class="number">2</span>,   <span class="number">8</span>,   <span class="number">5</span>,   <span class="number">7</span>&#125;;</span><br><span class="line"><span class="type">char</span> values[N] = &#123;<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;f&#x27;</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">sort_by_key</span>(keys, keys + N, values);</span><br><span class="line"><span class="comment">// keys is now   &#123;  1,   2,   4,   5,   7,   8&#125;</span></span><br><span class="line"><span class="comment">// values is now &#123;&#x27;a&#x27;, &#x27;c&#x27;, &#x27;b&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;d&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"><span class="type">const</span> <span class="type">int</span> N = <span class="number">6</span>;</span><br><span class="line"><span class="type">int</span> A[N] = &#123;<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">5</span>, <span class="number">7</span>&#125;;</span><br><span class="line">thrust::<span class="built_in">stable_sort</span>(A, A + N, thrust::<span class="built_in">greater</span>&lt;<span class="type">int</span>&gt;());</span><br><span class="line"><span class="comment">// A is now &#123;8, 7, 5, 4, 2, 1&#125;</span></span><br></pre></td></tr></table></figure><p>上例中的 <code>thrust::stable_sort</code>接受用户自定义比较运算符</p><h4 id="max-element-min-element"><a href="#max-element-min-element" class="headerlink" title="max_element(min_element)"></a>max_element(min_element)</h4><p>求最大（小）值</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/extrema.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line">thrust::device_vector&lt;type&gt;::iterator iter = thrust::<span class="built_in">max_element</span>(dvec.<span class="built_in">begin</span>()，dvec.<span class="built_in">end</span>());</span><br><span class="line"><span class="type">int</span> position = iter - dvec.<span class="built_in">begin</span>();</span><br><span class="line">type max_val = *iter;</span><br></pre></td></tr></table></figure><p>其返回值是一个迭代器，需要获取最大（小）值所在位置，再得到结果</p><h4 id="unique"><a href="#unique" class="headerlink" title="unique"></a>unique</h4><p>将一组数据中满足条件的数据筛选出来，可自定义筛选条件</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;thrust/unique.h&gt;</span></span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">is_same</span> &#123;</span><br><span class="line">	<span class="function">__host__ __device__</span></span><br><span class="line"><span class="function">		<span class="type">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="type">const</span> float3 &amp;p1, <span class="type">const</span> float3 &amp;p2)</span></span></span><br><span class="line"><span class="function">	</span>&#123;</span><br><span class="line">		<span class="keyword">return</span> (p1.x==p2.x) &amp;&amp; (p1.y==p2.y) &amp;&amp; (p1.z==p2.z);</span><br><span class="line">	&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_same</span>()),p.<span class="built_in">end</span>();</span><br><span class="line">p.<span class="built_in">erase</span>(thrust::<span class="built_in">unique</span>(p.<span class="built_in">begin</span>(), p.<span class="built_in">end</span>(),<span class="built_in">is_sam</span>()),p.<span class="built_in">end</span>());</span><br></pre></td></tr></table></figure><p>unique 函数的功能只是将满足条件的数据筛选出来，无法直接删除，需要结合 vector 的 erase 函数进行删除</p><h2 id="建立-CUDA-的并行线程计算"><a href="#建立-CUDA-的并行线程计算" class="headerlink" title="建立 CUDA 的并行线程计算"></a>建立 CUDA 的并行线程计算</h2><p>下面的程序为大家演示以结构体类型存储的矩阵计算，后续章节会教大家使用 cuBLAS 库进行并行计算</p><h3 id="矩阵加法"><a href="#矩阵加法" class="headerlink" title="矩阵加法"></a>矩阵加法</h3><p>下面的程序进行了 $C = A + B$ 矩阵加法运算，下面的程序中使用了<code>cudaMallocManaged</code>函数，简单来说，就是结合了之前讲到的<code>cudaMalloc</code>和<code>cudaMemcpy</code>等内存迁移拷贝的操作，自动内存管理，方便代码编写，弊端是在 kernel 执行时会降低 kernel 的执行效率，在后续章节，我们会详细讲解有关 CUDA 的内存管理</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="type">int</span> w;</span><br><span class="line">    <span class="type">int</span> h;</span><br><span class="line">    <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">        A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><p>下面的程序进行了 $C = A * B$ 矩阵乘法运算</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMul</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">float</span> k = <span class="number">0.0</span>;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i&lt;A-&gt;w; i++)</span><br><span class="line">                k += <span class="built_in">getValue</span>(A, row, i) * <span class="built_in">getValue</span>(B, i, col);</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">...</span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="为运行程序计时"><a href="#为运行程序计时" class="headerlink" title="为运行程序计时"></a>为运行程序计时</h2><h3 id="nvprof"><a href="#nvprof" class="headerlink" title="nvprof"></a>nvprof</h3><p>nvprof 是过去比较常用的命令行工具，但在终端直接输入<code>nvprof ./*.o</code>会得到以下 Warning</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">======== Warning: nvprof is not supported on devices with compute capability 8.0 and higher.</span><br><span class="line">                  Use NVIDIA Nsight Systems for GPU tracing and CPU sampling and NVIDIA Nsight Compute for GPU profiling.</span><br><span class="line">                  Refer https://developer.nvidia.com/tools-overview for more details.</span><br></pre></td></tr></table></figure><p>目前主流的 CUDA 驱动不再支持<code>nvprof</code>命令，但我们仍可以在 NVIDIA Nsight Systems 中使用，在终端输入 <code>nsys nvprof ./*.o</code>就可以看到CUDA 程序执行的具体内容</p><p>这里我们以主机与设备的数据拷贝的两个程序为例</p><p>使用<code>cudaMalloc</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 1d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-01f4.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report5.nsys-rep</span><br><span class="line">[2/7] [========================100%] report5.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)     Name   </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ----------</span><br><span class="line">     99.9        137542088          1  137542088.0  137542088.0  137542088  137542088          0.0  cudaMalloc</span><br><span class="line">      0.1           163239          1     163239.0     163239.0     163239     163239          0.0  cudaFree  </span><br><span class="line">      0.0            36460          2      18230.0      18230.0      18070      18390        226.3  cudaMemcpy</span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report5.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     51.6             1504      1    1504.0    1504.0      1504      1504          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     48.4             1408      1    1408.0    1408.0      1408      1408          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report5.nsys-rep</span><br><span class="line">    /root/report5.sqlite</span><br></pre></td></tr></table></figure><p>使用<code>cudaMallocPitch</code>函数的程序</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">WARNING: 2d.o and any of its children processes will be profiled.</span><br><span class="line"></span><br><span class="line">96</span><br><span class="line">Copied data to GPU.</span><br><span class="line">Copied from cuda back to host.</span><br><span class="line">host_dets size: 96</span><br><span class="line">23 34 56 76 11 23 45 45 12 22 47 47 9 45 56 65 20 37 55 75 0 0 0 0 </span><br><span class="line">done.</span><br><span class="line">Generating &#x27;/tmp/nsys-report-6614.qdstrm&#x27;</span><br><span class="line">[1/7] [========================100%] report6.nsys-rep</span><br><span class="line">[2/7] [========================100%] report6.sqlite</span><br><span class="line">[3/7] Executing &#x27;nvtxsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain NV Tools Extension (NVTX) data.</span><br><span class="line">[4/7] Executing &#x27;cudaapisum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)       Name      </span><br><span class="line"> --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------</span><br><span class="line">    100.0        745692893          1  745692893.0  745692893.0  745692893  745692893          0.0  cudaMallocPitch</span><br><span class="line">      0.0           161820          1     161820.0     161820.0     161820     161820          0.0  cudaFree       </span><br><span class="line">      0.0            39090          2      19545.0      19545.0      16590      22500       4179.0  cudaMemcpy2D   </span><br><span class="line"></span><br><span class="line">[5/7] Executing &#x27;gpukernsum&#x27; stats report</span><br><span class="line">SKIPPED: /root/report6.sqlite does not contain CUDA kernel data.</span><br><span class="line">[6/7] Executing &#x27;gpumemtimesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)      Operation     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">     64.8             2880      1    2880.0    2880.0      2880      2880          0.0  [CUDA memcpy HtoD]</span><br><span class="line">     35.2             1567      1    1567.0    1567.0      1567      1567          0.0  [CUDA memcpy DtoH]</span><br><span class="line"></span><br><span class="line">[7/7] Executing &#x27;gpumemsizesum&#x27; stats report</span><br><span class="line"></span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)      Operation     </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy DtoH]</span><br><span class="line">      0.000      1     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy HtoD]</span><br><span class="line"></span><br><span class="line">Generated:</span><br><span class="line">    /root/report6.nsys-rep</span><br><span class="line">    /root/report6.sqlite</span><br></pre></td></tr></table></figure><p>这里我们可以看到<code>Total Time</code>中<code>cudaMallocPitch</code>函数用时几乎是<code>cudaMalloc</code>的 5 倍，更加肯定了我们的说法，<code>cudaMallocPitch</code>和<code>cudaMemcpy2D</code>需要额外对二维数据进行对齐操作</p><h3 id="cudaEvent-计时函数"><a href="#cudaEvent-计时函数" class="headerlink" title="cudaEvent 计时函数"></a>cudaEvent 计时函数</h3><p>以前面的矩阵乘法为例，分别计算 CUDA 开辟内存时间和矩阵乘法运算时间</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">20</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C-&gt;v, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;cudaMalloc cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    </span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line"></span><br><span class="line">    elapsedTime = <span class="number">0.0</span>;</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Matrix multiplication cost: &quot;</span> &lt;&lt; elapsedTime &lt;&lt; <span class="string">&quot;s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">cudaMalloc cost: <span class="number">0.161696</span>s</span><br><span class="line">Matrix multiplication cost: <span class="number">0</span>s</span><br></pre></td></tr></table></figure></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2024/01/30/CUDA%E7%BC%96%E7%A8%8B-CUDA%E6%B5%81-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E5%90%8C%E6%AD%A5/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">CUDA编程: CUDA流,事件与同步</div></a><a href="/2024/01/16/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">CUDA编程: CUDA内存管理（二）</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li><a target="_blank" href=https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024 style="display:inline-block;text-decoration:none;"> <img src="/images/备案图标.webp" style="float:left;width:14px;height:14px;margin-top:2px;margin-right:-3px"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备32090202001024号</p><a target="_blank" href="https://beian.miit.gov.cn" style="display:inline-block;text-decoration:none"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">&nbsp苏ICP备2023045098号</p></a></li><li>Aeeeeeep Blog | The Gleaners &copy; 2024</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>theme <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes...</span><script>var now=new Date,grt=new Date("11/11/2021 00:08:39");function daysInYear(e){return e%4==0&&e%100!=0||e%400==0?366:365}function createTime(){now.setTime(now.getTime()+250);for(var e=0,t=new Date(grt.getTime());t<=now;){var n=daysInYear(t.getFullYear());t.setDate(t.getDate()+n),t<=now&&e++}t.setDate(t.getDate()-daysInYear(t.getFullYear()));var a=Math.floor((now-t)/864e5),r=now.getHours(),o=now.getMinutes(),i=r<10?"0"+r:r,g=o<10?"0"+o:o;document.getElementById("timeDate").innerHTML="Site has been running for "+e+"y "+a+"d ",document.getElementById("times").innerHTML=i+"h "+g+"m "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog | The Gleaners"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop" style="font-size:24px"><i class="fe fe-drop-up"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>