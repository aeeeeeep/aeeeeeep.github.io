<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>CUDA编程: CUDA内存管理（一） | Aeeeeeep Blog</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.0.0"></head><body><main class="content"><section class="outer"><article id="post-CUDA编程-CUDA内存管理（一）" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">CUDA编程: CUDA内存管理（一）</h1></header><div class="article-meta"><a href="/2024/01/09/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="article-date"><time datetime="2024-01-09T13:34:06.000Z" itemprop="datePublished">2024-01-09</time></a><div class="article-category"><a class="article-category-link" href="/categories/CUDA-%E7%BC%96%E7%A8%8B/">CUDA 编程</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>各种内存的对齐访问以及实验，避免带宽浪费。还对比了 AoS 与 SoA 结构体在 GPU 上的性能表现。</p><span id="more"></span><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="固定内存"><a href="#固定内存" class="headerlink" title="固定内存"></a>固定内存</h3><p>分配的主机内存默认是可分页的（pageable），操作系统会分配给程序一个很大的虚拟内存，但实际的物理内存会小很多，为了让程序正常运行，操作系统对物理内存以页为单位组织，将数据存储到不同的页中，这些页是不连续的，程序只可以看到虚拟内存地址，而操作系统可能随时更换数据的在物理内存中的页，但是在使用 GPU 时，如果从主机传输到设备上的时候，页被更改了，对于传输数据而言是致命的，所以在传输之前，CUDA 驱动会锁定页面，或者直接分配固定的主机内存，将主机数据复制到固定内存上，再从固定内存传输数据到设备上，如下图左边所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/1.png" alt=""></p><p>CUDA 运行时可以使用以下指令直接分配固定主机内存，会使传输带宽高很多，如上图右边所示</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMallocHost</span> <span class="params">( <span class="type">void</span>** ptr, <span class="type">size_t</span> size )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>ptr</code>: 指针，指向要分配的内存的位置</li><li><code>size</code>: 要分配的内存大小</li></ul><p>分配后的内存必须使用下面的命令释放</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaFreeHost</span> <span class="params">( <span class="type">void</span>* ptr )</span></span></span><br></pre></td></tr></table></figure><p>我们对第三章中的矩阵相加代码作修改，对比固定内存和分页内存的传输效率，也就是对比<code>cudaMalloc</code>和<code>cudaMallocHost</code>函数所开辟内存空间的传输效率，下面是<code>cudaMalloc</code>版本</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">   <span class="type">int</span> w;</span><br><span class="line">   <span class="type">int</span> h;</span><br><span class="line">   <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">   A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    Matrix *A = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    Matrix *B = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    Matrix *C = (Matrix*)<span class="built_in">malloc</span>(<span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    A-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    B-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    C-&gt;v = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">        C-&gt;v[i] = <span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Matrix *A_d, *B_d, *C_d;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *B_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *C_d_v = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d_v, A-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d_v, B-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d_v, C-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_d_v_t = A-&gt;v;</span><br><span class="line">    <span class="type">float</span> *B_d_v_t = B-&gt;v;</span><br><span class="line">    <span class="type">float</span> *C_d_v_t = C-&gt;v;</span><br><span class="line"></span><br><span class="line">    A-&gt;v = A_d_v;</span><br><span class="line">    B-&gt;v = B_d_v;</span><br><span class="line">    C-&gt;v = C_d_v;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d, A, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d, B, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d, C, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    A-&gt;v = A_d_v_t;</span><br><span class="line">    B-&gt;v = B_d_v_t;</span><br><span class="line">    C-&gt;v = C_d_v_t;</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A_d, B_d, C_d);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面是<code>cudaMallocHost</code>版本</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">   <span class="type">int</span> w;</span><br><span class="line">   <span class="type">int</span> h;</span><br><span class="line">   <span class="type">float</span> *v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">return</span> A-&gt;v[row * A-&gt;w + col];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">   A-&gt;v[row * A-&gt;w + col] = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixAdd</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">        <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">        <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">        <span class="built_in">setValue</span>(C, row, col, <span class="built_in">getValue</span>(A, row, col) + <span class="built_in">getValue</span>(B, row, col));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">11</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;A, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;B, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;C, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    A-&gt;h = h;</span><br><span class="line">    A-&gt;w = w;</span><br><span class="line">    B-&gt;h = h;</span><br><span class="line">    B-&gt;w = w;</span><br><span class="line">    C-&gt;h = h;</span><br><span class="line">    C-&gt;w = w;</span><br><span class="line">    <span class="type">int</span> nBytes = w * h * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;A-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;B-&gt;v, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocHost</span>((<span class="type">void</span> **)&amp;C-&gt;v, nBytes);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; w * h; ++i) &#123;</span><br><span class="line">        A-&gt;v[i] = <span class="number">1.0</span>;</span><br><span class="line">        B-&gt;v[i] = <span class="number">2.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Matrix *A_d, *B_d, *C_d;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d, <span class="built_in">sizeof</span>(Matrix));</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *B_d_v = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="type">float</span> *C_d_v = <span class="literal">NULL</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;A_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;B_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">void</span> **)&amp;C_d_v, <span class="built_in">sizeof</span>(nBytes));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d_v, A-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d_v, B-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d_v, C-&gt;v, <span class="built_in">sizeof</span>(nBytes), cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_d_v_t = A-&gt;v;</span><br><span class="line">    <span class="type">float</span> *B_d_v_t = B-&gt;v;</span><br><span class="line">    <span class="type">float</span> *C_d_v_t = C-&gt;v;</span><br><span class="line"></span><br><span class="line">    A-&gt;v = A_d_v;</span><br><span class="line">    B-&gt;v = B_d_v;</span><br><span class="line">    C-&gt;v = C_d_v;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_d, A, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_d, B, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(C_d, C, <span class="built_in">sizeof</span>(Matrix), cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    A-&gt;v = A_d_v_t;</span><br><span class="line">    B-&gt;v = B_d_v_t;</span><br><span class="line">    C-&gt;v = C_d_v_t;</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixAdd &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A_d, B_d, C_d);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>nsys nvprof &#123;&#125;.o</code>得到两个版本的数据传输时间</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cudaMalloc 版本</span></span><br><span class="line"> <span class="title class_">Time</span> (%)  <span class="title class_">Total</span> <span class="title class_">Time</span> (ns)  <span class="title class_">Count</span>  <span class="title class_">Avg</span> (ns)  <span class="title class_">Med</span> (ns)  <span class="title class_">Min</span> (ns)  <span class="title class_">Max</span> (ns)  <span class="title class_">StdDev</span> (ns)      <span class="title class_">Operation</span>     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">    <span class="number">100.0</span>             <span class="number">6817</span>      <span class="number">6</span>    <span class="number">1136.2</span>     <span class="number">992.5</span>       <span class="number">960</span>      <span class="number">1536</span>        <span class="number">243.4</span>  [<span class="variable constant_">CUDA</span> memcpy <span class="title class_">HtoD</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">// cudaMallocHost 版本</span></span><br><span class="line"> <span class="title class_">Time</span> (%)  <span class="title class_">Total</span> <span class="title class_">Time</span> (ns)  <span class="title class_">Count</span>  <span class="title class_">Avg</span> (ns)  <span class="title class_">Med</span> (ns)  <span class="title class_">Min</span> (ns)  <span class="title class_">Max</span> (ns)  <span class="title class_">StdDev</span> (ns)      <span class="title class_">Operation</span>     </span><br><span class="line"> --------  ---------------  -----  --------  --------  --------  --------  -----------  ------------------</span><br><span class="line">    <span class="number">100.0</span>             <span class="number">6304</span>      <span class="number">6</span>    <span class="number">1050.7</span>     <span class="number">992.0</span>       <span class="number">928</span>      <span class="number">1344</span>        <span class="number">153.4</span>  [<span class="variable constant_">CUDA</span> memcpy <span class="title class_">HtoD</span>]</span><br></pre></td></tr></table></figure><p>可以看到使用<code>cudaMallocHost</code>函数相比<code>cudaMalloc</code>，因其固定内存的特性，传输效率高了一些</p><blockquote><p>上面程序中用<code>cudaMalloc</code>将结构体从主机传输到设备的方法十分繁琐且已经过时了，在这里只作测试，建议使用第三章中提到的<code>cudaMallocManaged</code>函数，后续会详细介绍该函数</p></blockquote><h3 id="零拷贝内存"><a href="#零拷贝内存" class="headerlink" title="零拷贝内存"></a>零拷贝内存</h3><p>在 GPU 编程中，主机变量和设备变量之间一般不能直接相互访问，但主机变量和设备变量都可以访问零拷贝内存。GPU线程可以直接访问主机里的零拷贝内存，当有以下几种情况时核函数会使用零拷贝内存</p><ul><li>当设备内存不足时</li><li>避免主机和设备之间的显式内存传输</li><li>为了提高PCIe传输率</li></ul><p>当使用零拷贝内存时要同步设备和主机间的内存访问，避免内存竞争。零拷贝内存是固定内存，不可分页。使用以下函数可以创建零拷贝内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaHostAlloc</span> <span class="params">( <span class="type">void</span>** pHost, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span>  flags )</span></span></span><br></pre></td></tr></table></figure><ul><li><p><code>pHost</code>: 指向将被分配的内存地址</p></li><li><p><code>size</code>: 要分配的内存块的大小</p></li><li><p><code>flags</code>: 用于控制函数行为，可选如下</p><ul><li><p><code>cudaHostAllocDefault</code>: 默认，和<code>cudaMallocHost</code>一致，使用系统的默认内存类型分配内存，</p></li><li><p><code>cudaHostAllocPortable</code>: 分配固定内存，可以被所有 CUDA 上下文使用</p></li><li><p><code>cudaHostAllocMapped</code>: 分配零拷贝内存，将内存分配映射到设备的地址空间，必须使用以下函数得到指向主机上零拷贝内存的设备指针 <code>pDevice</code>，设备才能访问零拷贝内存</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaHostGetDevicePointer</span><span class="params">(<span class="type">void</span> ** pDevice,<span class="type">void</span> * pHost,<span class="type">unsigned</span> flags)</span></span>;</span><br></pre></td></tr></table></figure><ul><li><code>pDevice</code>: 访问主机零拷贝内存的设备指针</li><li><code>pHost</code>: 同上<code>pHost</code></li><li><code>flags</code>: 此处必须置0，后续介绍原因</li></ul></li><li><p><code>cudaHostAllocWriteCombined</code>: 将内存分配为写组合（Write-combined memory），可以在某些系统配置上更快地通过 PCIe 传输，提高设备写入内存的性能</p></li></ul></li></ul><p>主机上固定内存的释放</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaFreeHost</span> <span class="params">( <span class="type">void</span>* ptr )</span></span></span><br></pre></td></tr></table></figure><p>下面的程序计算了向量乘法，对比了<code>cudaMallocHost</code>和<code>cudaHostAlloc</code>的传输效率</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A_hp,*B_hp;</span><br><span class="line">    <span class="type">float</span> *A_dp,*B_dp,*C_dp;</span><br><span class="line">    A_hp = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    B_hp = (<span class="type">float</span> *)<span class="built_in">malloc</span>(nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(A_hp,<span class="number">1</span>,nBytes);</span><br><span class="line">    <span class="built_in">memset</span>(B_hp,<span class="number">2</span>,nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;A_dp,nBytes);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;B_dp,nBytes);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;C_dp,nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(A_dp,A_hp,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(B_dp,B_hp,nBytes,cudaMemcpyHostToDevice);</span><br><span class="line">    </span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_hm, *B_hm, *C_hm;</span><br><span class="line">    <span class="type">float</span> *A_dm, *B_dm, *C_dm;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;A_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;B_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;C_hm, nBytes, cudaHostAllocMapped);</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A_hm[i]=<span class="number">1.0</span>;</span><br><span class="line">        B_hm[i]=<span class="number">2.0</span>;</span><br><span class="line">        C_hm[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;A_dm, (<span class="type">void</span>*)A_hm, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;B_dm, (<span class="type">void</span>*)B_hm, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaHostGetDevicePointer</span>((<span class="type">void</span>**)&amp;C_dm, (<span class="type">void</span>*)C_hm, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_dp, B_dp, C_dp);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;cudaMallocHost: %f ms\n&quot;</span>,  elapsedTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_dm, B_dm, C_dm);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;cudaHostAlloc: %f ms\n&quot;</span>,  elapsedTime);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(start);</span><br><span class="line">    <span class="built_in">cudaEventDestroy</span>(stop);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">free</span>(A_hp);</span><br><span class="line">    <span class="built_in">free</span>(B_hp);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(A_hm);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(B_hm);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(C_hm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(A_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C_dp);</span><br><span class="line">    <span class="built_in">cudaFree</span>(A_dm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B_dm);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C_dm);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出为</span></span><br><span class="line">cudaMallocHost: <span class="number">0.050720</span> ms</span><br><span class="line">cudaHostAlloc: <span class="number">1.258496</span> ms</span><br></pre></td></tr></table></figure><p>可以看到零拷贝内存的内核函数执行效率会大大降低，因为其数据的传输是在执行内核函数时，这种执行方式不适合离散架构，如我们平时用的通过 PCIe 总线传输数据的独显 + CPU 架构，而非常适合在 Nvidia 集成架构的设备上使用，也就是共享物理内存的架构</p><h3 id="统一虚拟寻址"><a href="#统一虚拟寻址" class="headerlink" title="统一虚拟寻址"></a>统一虚拟寻址</h3><p>统一虚拟寻址（UVA）是 Nvidia 在计算能力 2.0 之后的设备支持的特殊寻址方式，设备内存和主机内存被映射到统一虚拟内存地址中，共享一个虚拟地址空间，如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/2.png" alt=""></p><p>在 UVA 出来之前，代码中的指针变量需要区分指向主机内存还是设备内存，通过UVA，可以通过<code>cudaHostAlloc</code> 函数分配固定主机内存，指针指向相同的主机和设备地址，可以直接将指针传递给核函数，相当于省略了零拷贝内存的<code>cudaHostGetDevicePointer</code>步骤</p><p>删除前面的零拷贝内存代码中<code>cudaHostGetDevicePointer</code>函数和设备指针部分，如下代码就使用了 UVA 的寻址方式</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A_h, *B_h, *C_h;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;A_h, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;B_h, nBytes, cudaHostAllocMapped);</span><br><span class="line">    <span class="built_in">cudaHostAlloc</span>((<span class="type">float</span>**)&amp;C_h, nBytes, cudaHostAllocMapped);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A_h[i]=<span class="number">1.0</span>;</span><br><span class="line">        B_h[i]=<span class="number">2.0</span>;</span><br><span class="line">        C_h[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A_h, B_h, C_h);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(A_h);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(B_h);</span><br><span class="line">    <span class="built_in">cudaFreeHost</span>(C_h);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="统一内存寻址"><a href="#统一内存寻址" class="headerlink" title="统一内存寻址"></a>统一内存寻址</h3><p>在 CUDA 6.0 中，又引入了统一内存寻址的特性，用于简化内存管理。统一内存中创建了一个托管内存池，内存池中已分配的空间可以用相同的指针在 CPU 和 GPU 上进行访问。底层系统在统一的内存空间中自动的进行设备和主机间的传输</p><p>统一内存寻址依赖于前面提到的 UVA，不同之处在于</p><ul><li>统一内存寻址提供了一个“单指针到数据”的编程模型，通过底层系统进行统一内存管理，被称为托管内存，自己分配的内存称为未托管内存，两种类型的内存可以同时传递给核函数</li><li>UVA 的分配是先在主机上完成，在核函数运行前才传输数据给设备</li></ul><p>托管内存可以被静态分配或动态分配，在定义设备变量时添加 <code>__managed__</code> 关键字修饰静态托管内存变量，如下代码所示，静态声明的托管内存作用域是文件，该变量可以在主机或设备代码中直接引用</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">__managed__ <span class="type">float</span> x;</span><br></pre></td></tr></table></figure><p>第三章使用过的<code>cudaMallocManaged</code>函数就是动态分配托管内存变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMallocManaged</span> <span class="params">( <span class="type">void</span>** devPtr, <span class="type">size_t</span> size, <span class="type">unsigned</span> <span class="type">int</span>  flags = cudaMemAttachGlobal )</span></span></span><br></pre></td></tr></table></figure><ul><li><code>devPtr</code>: 开辟数据的首指针</li><li><code>size</code>: 开辟的设备内存空间长度</li><li><code>flags</code>: 默认为<code>cudaMemAttachGlobal</code><ul><li><code>cudaMemAttachGlobal</code>: 开辟的内存可以被任何设备上的任何流访问，流的概念将在下一章节介绍</li><li><code>cudaMemAttachHost</code>: 开辟的内存不能被任何设备上的任何流访问</li></ul></li></ul><p>使用<code>cudaMallocManaged</code>函数动态声明数组，分配托管内存变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc,<span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="内存访问模式"><a href="#内存访问模式" class="headerlink" title="内存访问模式"></a>内存访问模式</h2><p>多数 GPU 程序容易受到内存带宽的限制，所以最大程度的利用全局内存带宽，提高全局加载效率，是调控内核函数性能的基本条件。如果不能正确管理全局内存，那么优化方案可能也收效甚微<br>在 CUDA 执行模型中得知 CUDA 执行的基本单位是线程束，所以内存访问和数据存储也是以线程束为基本单位发布和执行的，在线程束的 32 个线程中，每个线程都会提出一个包含请求地址的单一内存访问请求，根据线程束中内存地址的分布，内存访问会被分成不同的模式，下面将介绍这些不同的内存访问模式，并学习实现最佳的全局内存访问</p><h3 id="对齐与合并访问"><a href="#对齐与合并访问" class="headerlink" title="对齐与合并访问"></a>对齐与合并访问</h3><p>全局内存（DRAM）是一个逻辑内存空间，可以通过核函数访问它，所有的程序数据都是储在物理设备内存上，也就是 DRAM 设备上，核函数的内存请求通常是在 DRAM 设备和片上内存间上以 32 字节或 128 字节粒度的内存事务来实现，内存事务就是从内核函数发起请求，到硬件响应返回数据过程</p><p>所有对全局内存的访问都会通过二级缓存，也有许多访问会通过一级缓存，这取决于访问类型和 GPU 架构</p><p>称 L1 为一级缓存，L2 为二级缓存，如下图所示，每个 SM 都有自己 L1，但是 L2 是所有 SM 公用的，核函数运行时需要从 DRAM 中读取数据，读取时如果使用 L1 则从 DRAM 上一次加载的数据是 128 字节，如果不使用 L1 则从 DRAM 上一次加载的数据是 32 字节</p><p>一行一级缓存是 128 字节，映射到设备内存中一个 128 字节的对齐段，如果线程束中的每个线程请求一个 4 字节的值，那么每次请求都会获取 4 x 32 = 128 字节的数据，这恰好与缓存行和设备内存段的大小相契合，因此我们要注意设备内存访问的：</p><ul><li>对齐内存访问</li><li>合并内存访问</li></ul><p><img src="/image/CUDA编程-CUDA内存管理（一）/3.png" alt=""></p><p>当设备内存事务的第一个地址是用于事务服务的缓存粒度的偶数倍数时（32或128字节），称为对齐内存访问，当一个线程束内的线程访问的内存都在一个内存块里的时候，就会出现合并访问。对齐与合并访问的情况如下图所示，只需要 128 字节的内存事务从设备内存中读取数据</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/4.png" alt=""></p><p>但如果一个内存事务加载的数据分布在不一个对齐的地址段上，就会有以下两种情况</p><ul><li>内存地址连续，但不在一个对齐的段上，如请求访问的数据分布在内存地址 1-128，那么 0-127 和 128-255 这两段数据要传递两次到 SM</li><li>内存地址不连续，也不在一个对齐的段上，如请求访问的数据分布在内存地址 0-63 和 128-191 上，那么这两段数据也要传递两次</li></ul><p>上述两种情况会使内存事务获取的大部分字节不能使用，造成带宽的浪费，如下图所示</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/5.png" alt=""></p><p>我们需要优化内存事务的效率，提高吞吐量</p><h3 id="全局内存读取"><a href="#全局内存读取" class="headerlink" title="全局内存读取"></a>全局内存读取</h3><p>SM 中的数据根据不同的设备和类型以 3 种不同的路径进行传输</p><ul><li>L1 和 L2</li><li>常量缓存</li><li>只读缓存</li></ul><p>默认路径是 L1 和 L2，需要使用常量和只读缓存的需要在代码中显式声明。但是提高性能还是要取决于访问模式，全局加载操作是否通过 L1 可以通过编译选项来控制，在计算能力为 2.X 和 3.5 以上的 GPU 中，可以通过以下编译器标识符禁用 L1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=cg</span><br></pre></td></tr></table></figure><p>通过以下编译器标识符启用 L1</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=ca</span><br></pre></td></tr></table></figure><p>当 SM 有全局加载请求会首先尝试通过 L1，如果 L1 被禁用，请求转向 L2，如果 L2 缺失，则由 DRAM 完成请求，在这种情况下，内存加载请求由 128 字节的设备内存事务实现</p><blockquote><p>Kepler K10, K20, K20X GPU 中，L1 不用来缓存全局内存访问，只用来存储寄存器溢出的本地数据</p></blockquote><h4 id="缓存加载"><a href="#缓存加载" class="headerlink" title="缓存加载"></a>缓存加载</h4><p>缓存加载是指经过 L1，在粒度为 128 字节的 L1 上由设备内存事务进行传输。缓存加载可以分为对齐/非对齐但合并/非合并</p><p>下图所示为对齐合并的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/6.png" alt=""></p><p>对齐但不连续的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/7.png" alt=""></p><p>非对齐但连续的情况，因为线程束请求的 32 个连续的 4 字节元素在两个 128 字节段内，所以利用率为 50%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/8.png" alt=""></p><p>线程束中所有线程请求同一个地址的情况，利用率为 4/128 = 3.125%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/9.png" alt=""></p><p>最坏的情况，每个线程束内的线程请求的都是不同的缓存行内，比较坏的情况就是所有数据分布在 N 个缓存行上，其中 1≤N≤32，请求 32 个 4 字节的数据就需要 N 个事务来完成，利用率为 1/N</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/10.png" alt=""></p><blockquote><p>CPU 和 GPU 的 L1 有明显的差异，CPU 的 L1 优化了时间和空间局部性，GPU 的 L1 是专为空间局部性设计的，频繁访问 L1 中内存位置不会增加数据留存缓存中的概率</p></blockquote><h4 id="没有缓存的加载"><a href="#没有缓存的加载" class="headerlink" title="没有缓存的加载"></a>没有缓存的加载</h4><p>没有缓存的加载是指不经过 L1，只经过 L2，在粒度为 32 字节的 L2 上由设备内存事务进行传输，更细的粒度代表更高的利用率</p><p>下图所示为对齐合并的情况，128 字节请求的地址占用了 4 个内存段，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/11.png" alt=""></p><p>对齐但不连续的情况，利用率为 100%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/12.png" alt=""></p><p>非对齐但连续的情况，因为线程束请求的 32 个连续的 4 字节元素但加载没有对齐到 128 个字节的边界，请求的地址最多落在 5 个内存段内，所以利用率为 4/5 = 80%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/13.png" alt=""></p><p>线程束中所有线程请求同一个地址的情况，利用率为 4/32 = 12.5%</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/14.png" alt=""></p><p>最坏的情况，每个线程束内的线程请求的都是不同的缓存行内，由于请求的 128 个字节最多落在 N 个 32 字节的内存段内而不是 N 个 128 字节的缓存行内，所以相比缓存加载，即使是最坏的情况也有所改善</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/15.png" alt=""></p><h4 id="非对齐读取示例"><a href="#非对齐读取示例" class="headerlink" title="非对齐读取示例"></a>非对齐读取示例</h4><p>对之前的<code>ArraySum</code>核函数代码执行加上偏移量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> offset, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">int</span> k = i+offset;</span><br><span class="line">    <span class="keyword">if</span>(k &lt; n)</span><br><span class="line">        c[i] = a[k]+b[k];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> dev = <span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaSetDevice</span>(dev);</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">20</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> offset=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(argc&gt;=<span class="number">2</span>)</span><br><span class="line">        offset = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid, block&gt;&gt;&gt;(A, B, C, offset, nElem);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed: %f ms offset: %d \n&quot;</span>, grid.x, block.x, elapsedTime, offset);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们这里使用 Nsight Compute 交互式内核分析工具，一般安装完 CUDA Toolkit 会在安装目录中，我们只需要添加环境变量即可，如没有请在<a target="_blank" rel="noopener" href="https://developer.nvidia.com/gameworksdownload#?dn=nsight-compute-2022-4-0">官网下载</a>，详细的使用方法请查看<a target="_blank" rel="noopener" href="https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html">官网文档</a></p><blockquote><p>注意类似 Docker 之类的虚拟机用户（如租赁的 GPU）没有权限使用 Nsight Compute 访问 GPU，需要在物理机上以管理员权限使用</p></blockquote><p>编译命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc -Xptxas -dlcm=cg &#123;&#125;.cu -o &#123;&#125;_cg.out	<span class="comment"># 禁用 L1</span></span><br><span class="line">nvcc -Xptxas -dlcm=ca &#123;&#125;.cu -o &#123;&#125;_ca.out	<span class="comment"># 启用 L1</span></span><br></pre></td></tr></table></figure><p>测试不同的偏移量获得的全局加载效率</p><blockquote><p>全局加载效率 = 请求的全局内存加载吞吐量 / 所需的全局内存加载吞吐量</p></blockquote><p>启用 L1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 0</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 11</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_ca.out 128</span><br></pre></td></tr></table></figure><ul><li>offset=0 : gld_efficiency 100%</li><li>offset=11 : gld_efficiency 40%</li><li>offset=128 : gld_efficiency 100%</li></ul><p>可以看出偏移量会直接导致性能损失</p><p>禁用 L1</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 0</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 11</span><br><span class="line">ncu --metrics smsp__sass_average_data_bytes_per_sector_mem_global_op_ld.pct ./&#123;&#125;_cg.out 128</span><br></pre></td></tr></table></figure><ul><li>offset=0 : gld_efficiency 100%</li><li>offset=11 : gld_efficiency 80%</li><li>offset=128 : gld_efficiency 100%</li></ul><p>禁用 L1 后同样偏移 11 的全局加载效率提高了不多，也验证了上面提到的更细的词粒度会带来更好的性能</p><h4 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h4><p>只读缓存最初是留给纹理内存加载用的，在 计算能力 3.5 以上的设备，只读缓存也支持使用全局内存加载代替一级缓存</p><p>只读缓存粒度为 32 字节，对于分散读取，细粒度优于一级缓存，有两种方法让内存从只读缓存读取</p><ul><li>使用函数<code>__ldg</code></li><li>在间接引用的指针上使用修饰符</li></ul><p>考虑如下代码</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用<code>__ldg</code>通过只读缓存对数组进行读取访问</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a,<span class="type">float</span>*b,<span class="type">float</span>*res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]= __ldg(&amp;a[i]) + __ldg(&amp;b[i]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也可以常量<code>__restrict__</code>修饰符应用到指针上，nvcc 将自动通过只读缓存指导无别名指针的加载</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">const</span> <span class="type">float</span>* __restrict__ a, <span class="type">const</span> <span class="type">float</span>* __restrict__ b, <span class="type">float</span>* __restrict__ res)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i=blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    res[i]=a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="全局内存写入"><a href="#全局内存写入" class="headerlink" title="全局内存写入"></a>全局内存写入</h3><p>内存的存储和加载是完全不同的，并且存储相对简单很多。存储操作在32个字节的粒度上被执行，内存事务也被分为一段、两端或者四段，例如两个地址在一个 128 字节的段内但不在对齐的 64 字节区域内，则会产生一个四段的事务，执行四段的事务比执行两个一段事务的效果更好</p><blockquote><p>Fermi 和 Kepler 架构的存储操作不经过 L1 ，只经过 L2</p></blockquote><p>如下图所示，内存访问是对齐的，访问一个连续的 128 字节范围，存储请求使用一个四段事务完成，这里最理想的情况</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/16.png" alt=""></p><p>数据分散在一个 192 字节的范围内，存储不连续，使用三个一段事务实现</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/17.png" alt=""></p><p>内存访问是对齐的，访问一个连续的 64 字节范围，存储请求使用一个两段事务完成</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/18.png" alt=""></p><h4 id="非对齐写入示例"><a href="#非对齐写入示例" class="headerlink" title="非对齐写入示例"></a>非对齐写入示例</h4><p>只需要对核函数作如下修改即可</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> offset, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">    <span class="type">int</span> k = i+offset;</span><br><span class="line">    <span class="keyword">if</span>(k &lt; n)</span><br><span class="line">        c[k] = a[i]+b[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试结论与非对齐读取示例相同</p><h3 id="结构体数组与数组结构体（SoA-和-AoS）"><a href="#结构体数组与数组结构体（SoA-和-AoS）" class="headerlink" title="结构体数组与数组结构体（SoA 和 AoS）"></a>结构体数组与数组结构体（SoA 和 AoS）</h3><p>在 C 语言中，结构体是一种强大的数据组织方式，结构体中的成员在内存里对齐的依次排开，但是我们保存一组数据有两种方式，可以定义如下结构体</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> x;</span><br><span class="line">    <span class="type">float</span> y;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>再定义一个数组结构体（AoS）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> _<span class="title">Aos</span>[<span class="title">N</span>];</span></span><br></pre></td></tr></table></figure><p>也可以定义一个结构体数组（SoA）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> x[N];</span><br><span class="line">    <span class="type">float</span> y[N];</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">S</span> _<span class="title">SoA</span>[<span class="title">N</span>];</span></span><br></pre></td></tr></table></figure><p>AoS 方式组织的数据在空间上是相邻的，这在 CPU 上会有良好的缓存局部性，但是在 GPU 的并行架构中，读写一个结构体字段 x 时会同时加载 x 和 y 两个字段，这就导致有 50 % 的带宽损失，再看 SoA，访问一个 SoA 布局的结构体时，由于没有交叉存储的字段，所以是合并内存访问，可以充分利用带宽，图示如下</p><p><img src="/image/CUDA编程-CUDA内存管理（一）/19.png" alt=""></p><p>前面我们介绍的矩阵相乘中定义矩阵的结构体就是 SoA 方式，下面是 AoS 方式的代码，与 SoA 方式的代码作对比，只做测试用，无实际意义</p><blockquote><p>这里指出第三章中矩阵相乘代码的错误，w 和 h 设置为 <code>1 &lt;&lt; 20</code> 会导致数值溢出，这里改为<code>1 &lt;&lt; 12</code></p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">Matrix</span> &#123;</span><br><span class="line">    <span class="type">int</span> w;</span><br><span class="line">    <span class="type">int</span> h;</span><br><span class="line">    <span class="type">float</span> v;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">__device__ <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">__device__ <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line"><span class="function">__device__ <span class="type">float</span> <span class="title">getValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> A[row * w + col].v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__device__ <span class="type">void</span> <span class="title">setValue</span><span class="params">(Matrix *A, <span class="type">int</span> row, <span class="type">int</span> col, <span class="type">float</span> v)</span> </span>&#123;</span><br><span class="line">    A[row * w + col].v = v;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">MatrixMul</span><span class="params">(Matrix *A, Matrix *B, Matrix *C)</span> </span>&#123;</span><br><span class="line">    <span class="type">float</span> k = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">int</span> row = threadIdx.y + blockIdx.y * blockDim.y;</span><br><span class="line">    <span class="type">int</span> col = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i=<span class="number">0</span>; i &lt; w; i++)</span><br><span class="line">        k += <span class="built_in">getValue</span>(A, row, i) * <span class="built_in">getValue</span>(B, i, col);</span><br><span class="line">    <span class="built_in">setValue</span>(C, row, col, k);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> w = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">    <span class="type">int</span> h = <span class="number">1</span> &lt;&lt; <span class="number">12</span>;</span><br><span class="line">    Matrix *A, *B, *C;</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;A, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;B, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">void</span>**)&amp;C, <span class="built_in">sizeof</span>(Matrix) * w * h);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i &lt; w*h; i++) &#123;</span><br><span class="line">        A[i].w = w;</span><br><span class="line">        A[i].h = h;</span><br><span class="line">        B[i].w = w;</span><br><span class="line">        B[i].h = h;</span><br><span class="line">        C[i].w = w;</span><br><span class="line">        C[i].h = h;</span><br><span class="line">        A[i].v = <span class="number">1.0</span>;</span><br><span class="line">        B[i].v = <span class="number">2.0</span>;</span><br><span class="line">        C[i].v = <span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">blockSize</span><span class="params">(<span class="number">32</span>, <span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">gridSize</span><span class="params">((w + blockSize.x - <span class="number">1</span>) / blockSize.x, (h + blockSize.y - <span class="number">1</span>) / blockSize.y)</span></span>;</span><br><span class="line">    MatrixMul &lt;&lt; &lt; gridSize, blockSize &gt;&gt; &gt;(A, B, C);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用 nsys 对比主机到设备的数据传输</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># AoS 方式</span><br><span class="line">Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------</span><br><span class="line">    402.653   6951     0.058     0.016     0.004     1.040        0.143  [CUDA Unified Memory memcpy HtoD]</span><br><span class="line"></span><br><span class="line"># SoA 方式</span><br><span class="line"> Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            </span><br><span class="line"> ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------</span><br><span class="line">    134.283   2772     0.048     0.008     0.004     1.028        0.142  [CUDA Unified Memory memcpy HtoD]</span><br></pre></td></tr></table></figure><p>可以直观看到，AoS 对某个字段读写时会同时加载所有字段，<code>Matrix</code>结构体有三个字段，134 * 3 = 402，与数据显示一致</p><h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><p>优化设备内存带宽利用率有两个目标</p><ul><li>对齐合并内存访问，减少带宽浪费</li><li>足够的并发内存操作，降低内存延迟</li></ul><p>我们已经了解了如何组织内存访问以对内存对齐的内存访问，这可以在 DRAM 和 SM 片上内存或寄存器之间确保有效利用字节移动，实现内存访问最大化一般通过增加每个线程中执行独立内存操作的数量，以及对核函数启动的执行配置进行试验</p><h4 id="展开循环"><a href="#展开循环" class="headerlink" title="展开循环"></a>展开循环</h4><p>我们使用 8 循环展开技术对<code>ArraySum</code>核函数作修改</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">ArraySum</span><span class="params">(<span class="type">float</span>*a, <span class="type">float</span>*b, <span class="type">float</span>*c, <span class="type">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="type">int</span> i = blockIdx.x*blockDim.x*<span class="number">4</span> +threadIdx.x;</span><br><span class="line">    c[i] = a[i]+b[i];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x &lt; n)</span><br><span class="line">        c[i+blockDim.x] = a[i+blockDim.x]+b[i+blockDim.x];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">2</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">2</span>] = a[i+blockDim.x *<span class="number">2</span>]+b[i+blockDim.x *<span class="number">2</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">3</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">3</span>] = a[i+blockDim.x *<span class="number">3</span>]+b[i+blockDim.x *<span class="number">3</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">4</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">4</span>] = a[i+blockDim.x *<span class="number">4</span>]+b[i+blockDim.x *<span class="number">4</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">5</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">5</span>] = a[i+blockDim.x *<span class="number">5</span>]+b[i+blockDim.x *<span class="number">5</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">6</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">6</span>] = a[i+blockDim.x *<span class="number">6</span>]+b[i+blockDim.x *<span class="number">6</span>];</span><br><span class="line">    <span class="keyword">if</span>(i + blockDim.x *<span class="number">7</span> &lt; n)</span><br><span class="line">        c[i+blockDim.x *<span class="number">7</span>] = a[i+blockDim.x *<span class="number">7</span>]+b[i+blockDim.x *<span class="number">7</span>];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> nElem=<span class="number">1</span>&lt;&lt;<span class="number">28</span>;</span><br><span class="line">    <span class="type">int</span> nBytes=<span class="built_in">sizeof</span>(<span class="type">float</span>)*nElem;</span><br><span class="line"></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">1024</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(nElem/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *A, *B, *C;</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;A, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;B, nBytes);</span><br><span class="line">    <span class="built_in">cudaMallocManaged</span>((<span class="type">float</span>**)&amp;C, nBytes);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>; i&lt;nElem; i++) &#123;</span><br><span class="line">        A[i]=<span class="number">1.0</span>;</span><br><span class="line">        B[i]=<span class="number">2.0</span>;</span><br><span class="line">        C[i]=<span class="number">0.0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;start);</span><br><span class="line">    <span class="built_in">cudaEventCreate</span>(&amp;stop);</span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(start, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    ArraySum&lt;&lt;&lt;grid.x/<span class="number">8</span>, block&gt;&gt;&gt;(A, B, C, nElem);</span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaEventRecord</span>(stop, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">cudaEventSynchronize</span>(stop);</span><br><span class="line">    <span class="type">float</span> elapsedTime=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">cudaEventElapsedTime</span>(&amp;elapsedTime, start, stop);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;%d,%d&gt;&gt;&gt; Time elapsed: %f ms \n&quot;</span>, grid.x/<span class="number">8</span>, block.x, elapsedTime);</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaFree</span>(A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(C);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">32768</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">353.544189</span> ms</span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">262144</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">739.468262</span> ms</span><br></pre></td></tr></table></figure><p>与最初的版本对比，循环展开技术节省了大半的运算时间</p><h4 id="增大并行性"><a href="#增大并行性" class="headerlink" title="增大并行性"></a>增大并行性</h4><p>对数组求和所用的<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>测试，寻找最佳执行配置</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">262144</span>,<span class="number">1024</span>&gt;&gt;&gt; Time elapsed: <span class="number">865.561584</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">524288</span>,<span class="number">512</span>&gt;&gt;&gt; Time elapsed: <span class="number">757.686401</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">1048576</span>,<span class="number">256</span>&gt;&gt;&gt; Time elapsed: <span class="number">687.030212</span> ms </span><br><span class="line">&lt;&lt;&lt;grid, block&gt;&gt;&gt;: &lt;&lt;&lt;<span class="number">2097152</span>,<span class="number">128</span>&gt;&gt;&gt; Time elapsed: <span class="number">786.416626</span> ms </span><br></pre></td></tr></table></figure><p>可以看到在<code>block.x</code>为 256 时，执行效率最高，这是因为较高的<code>block.x</code>会使 SM 的并行性降低，而过低的<code>block.x</code>不能充分利用 SM 的计算资源，具体的<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>大小取决于当前的 GPU 架构中的 SM 配置，需要实验得出</p></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2024/01/16/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%88%E4%BA%8C%EF%BC%89/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">CUDA编程: CUDA内存管理（二）</div></a><a href="/2024/01/02/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">CUDA编程: CUDA内存模型概述</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li><a target="_blank" href=https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024 style="display:inline-block;text-decoration:none;"> <img src="/images/备案图标.webp" style="float:left;width:14px;height:14px;margin-top:2px;margin-right:-3px"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备32090202001024号</p><a target="_blank" href="https://beian.miit.gov.cn" style="display:inline-block;text-decoration:none"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">&nbsp苏ICP备2023045098号</p></a></li><li>Aeeeeeep Blog &copy; 2024</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>theme <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes...</span><script>var now=new Date,grt=new Date("11/11/2021 00:08:39");function daysInYear(e){return e%4==0&&e%100!=0||e%400==0?366:365}function createTime(){now.setTime(now.getTime()+250);for(var e=0,t=new Date(grt.getTime());t<=now;){var n=daysInYear(t.getFullYear());t.setDate(t.getDate()+n),t<=now&&e++}t.setDate(t.getDate()-daysInYear(t.getFullYear()));var a=Math.floor((now-t)/864e5),r=now.getHours(),o=now.getMinutes(),i=r<10?"0"+r:r,g=o<10?"0"+o:o;document.getElementById("timeDate").innerHTML="Site has been running for "+e+"y "+a+"d ",document.getElementById("times").innerHTML=i+"h "+g+"m "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop" style="font-size:24px"><i class="fe fe-drop-up"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>