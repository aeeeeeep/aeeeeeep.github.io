<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>论文阅读: ZeRO++: Extremely Eficient Collective Communication for Giant Model Training | Aeeeeeep Blog | The Gleaners</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><main class="content"><section class="outer"><article id="post-论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">论文阅读: ZeRO++: Extremely Eficient Collective Communication for Giant Model Training</h1></header><div class="article-meta"><a href="/2023/11/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/" class="article-date"><time datetime="2023-11-25T08:12:55.000Z" itemprop="datePublished">2023-11-25</time></a><div class="article-category"><a class="article-category-link" href="/categories/%E6%87%B5%E9%80%BC%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">懵逼的深度学习</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>原文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2306.10209.pdf">https://arxiv.org/pdf/2306.10209.pdf</a></p><p>开源代码：<a target="_blank" rel="noopener" href="https://github.com/microsoft/deepspeed">https://github.com/microsoft/deepspeed</a></p><span id="more"></span><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>面对 LLM，3D并行工程实现复杂</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>通过以下 3 种方法减少通信</p><ul><li>基于块量化的 all-gather</li><li>通过数据重映射用通信减少内存开销</li><li>基于 all-to-all 的量化梯度平均方法，替代 reduce-scatter</li></ul><p>ZeRO++将 ZeRO 的通信量减少了 4 倍，并在低精度下保持准确性，使得在 384 张 GPU下，吞吐量可以提高 2.16 倍</p><h3 id="Limitations-of-ZeRO"><a href="#Limitations-of-ZeRO" class="headerlink" title="Limitations of ZeRO"></a>Limitations of ZeRO</h3><p>在低带宽集群中，单卡的吞吐量只有高带宽集群的一半，即使在高带宽集群中，使用数千个 GPU 进行训练，单卡的 batch size 也受到 global batch size 的限制（global batch size 不能无限增加，否则会降低模型收敛效率）</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/1.webp" width="500"></p><p>所以在千卡训练时，单卡的 batch size 必须非常小，这会降低计算与通信的比率</p><p>ZeRO 由于对模型状态进行分区，无法直接对模型状态复制，所以与之前的节省通信工作不兼容</p><h3 id="ZeRO"><a href="#ZeRO" class="headerlink" title="ZeRO++"></a>ZeRO++</h3><p>假设模型参数量为 M</p><p>FWD：参数 all gather</p><p>BWD：参数 all gather，梯度 reduce scatter</p><p>通信量总计 3M</p><h4 id="Quantized-Weight-Communication-qwZ"><a href="#Quantized-Weight-Communication-qwZ" class="headerlink" title="Quantized Weight Communication (qwZ)"></a>Quantized Weight Communication (qwZ)</h4><p>通信前，将参数从 fp16 量化为 int8，为了保证训练准确率，使用 block-based 量化的思想，并使用 CUDA kernel 保证高性能</p><h4 id="Hierarchical-Weight-Partition-hpZ"><a href="#Hierarchical-Weight-Partition-hpZ" class="headerlink" title="Hierarchical Weight Partition (hpZ)"></a>Hierarchical Weight Partition (hpZ)</h4><p>在单个节点中保存整个模型参数的副本，通过节点内的 all gather 代替节点间通信，牺牲显存，节省通信</p><h4 id="Quantized-Gradient-Communication-qgZ"><a href="#Quantized-Gradient-Communication-qgZ" class="headerlink" title="Quantized Gradient Communication  (qgZ)"></a>Quantized Gradient Communication (qgZ)</h4><p>直接在 reduce scatter 之前量化会影响精度，可以在通信过程中使用 block-based INT4 量化压缩梯度，并在发送后恢复保证训练精度</p><p>两步通信，先节点内通信，再节点间通信</p><p>节点间通信使用流水线策略，融合 CUDA Kernel</p><h4 id="Communication-Volume-Reduction"><a href="#Communication-Volume-Reduction" class="headerlink" title="Communication Volume Reduction"></a>Communication Volume Reduction</h4><p>qwZ: 1M → 0.5</p><p>hpZ: 1M → 0M</p><p>qgZ: 1M → 0.25M</p><p>all: 3M → 0.75M</p><h2 id="先前工作"><a href="#先前工作" class="headerlink" title="先前工作"></a>先前工作</h2><h3 id="3D-并行"><a href="#3D-并行" class="headerlink" title="3D 并行"></a>3D 并行</h3><p>3D 并行训练流程</p><ol><li>all gather 参数</li><li>FWD</li><li>partition</li><li>all gather</li><li>BWD</li><li>partition</li><li>reduce scatter</li><li>优化器更新</li></ol><h4 id="ZeRO-优化器"><a href="#ZeRO-优化器" class="headerlink" title="ZeRO 优化器"></a>ZeRO 优化器</h4><p>ZeRO-3 最高效利用内存，但需要多次通信解决分区问题</p><h4 id="Communication-Reduction-Techniques"><a href="#Communication-Reduction-Techniques" class="headerlink" title="Communication Reduction Techniques"></a>Communication Reduction Techniques</h4><p>量化问题：fp32/16 对比 int8 有数据范围和粒度的差异</p><p>改进方法：</p><p>过滤异常值，缩小数值范围差距，但准确性被滤波算法影响，且有时间开销<br>分块量化优化器状态，但需要修改模型结构</p><p>梯度压缩：1-bit adam/lamb 可以实现高效的通信，但必须保证每个 GPU 上都有梯度副本，由于 ZeRO-3 的分区策略，不能应用</p><h4 id="ZeRO-Communication-Reduction"><a href="#ZeRO-Communication-Reduction" class="headerlink" title="ZeRO Communication Reduction"></a>ZeRO Communication Reduction</h4><p>MiCS (基于 DeepSpeed-v0.4.9 和 PyTorch-v1.11)：对节点分组，每个组保存模型状态的完整副本。在每个组内，模型状态被分区，使最频繁的参数 all gather操作在每个组上进行，且并行多个节点间的集体通信，在组内的梯度达到边界，则进行组间通信，此外，还采用了细粒度同步、合并通信 API 和内存碎片整理等优化</p><p>hpZ 与 MiCS 类似，但只对权重分组，在每个 gpu 上保留对模型状态的分区，相比 MiCS 节省了内存</p><h2 id="设计"><a href="#设计" class="headerlink" title="设计"></a>设计</h2><h3 id="qwZ"><a href="#qwZ" class="headerlink" title="qwZ"></a>qwZ</h3><p>为了解决精度下降严重的问题，使用块量化的思想，将每个权重张量被划分为更小的块，然后使用独立的量化缩放系数进行对称量化为 INT8，减少了 3 倍量化误差</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/2.webp" width="500"></p><h3 id="hpZ"><a href="#hpZ" class="headerlink" title="hpZ"></a>hpZ</h3><p>采用两级分区策略</p><ul><li>全局主分区：所有模型状态在所有设备上全局分区（如ZeRO-3）</li><li>次级分区：在次全局级别（例如，计算节点）创建 FP16 参数的次级副本，并在多个次级分区中复制</li></ul><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/3.webp" width="500"></p><p>在 FWD 阶段，对权重在主分区上 all gather。权重在 FWD 后，根据次级分区进行分区。由于 FWD/BWD 之间模型参数的存在时序一致性，在 BWD 中再次需要权重时，基于次级分组进行 all gather</p><p>当次级分区设置为计算节点时，避免了 all gather 过程中的跨节点通信</p><p>在迭代结束时，在优化器步骤中，所有模型状态以及 FP16 参数的主副本都根据主分区更新</p><p>支持任何次级分区大小，并控制次级分区中的GPU数量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/4.webp" width="500"></p><h3 id="qgZ"><a href="#qgZ" class="headerlink" title="qgZ"></a>qgZ</h3><p>基于 all-to-all 的量化梯度通信策略，只在通信之前量化梯度，但在任何 reduce 操作之前将它们反量化到原有精度 ，功能上等同于压缩的 reduce-scatter 操作</p><p>解决了两个问题：</p><ol><li>简单地在 INT4/INT8 中实施 reduce-scatter 会导致显著精度损失</li><li>在传统 tree 或 ring-based reduce-scatter 中使用量化需要一长串量化和反量化步骤，这直接导致误差积累和显著的延迟</li></ol><p>qgZ 不使用tree或ring-based reduce-scatter算法，而是基于一种新的分层 all-to-all 方法</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/5.webp" width="500"></p><p>qgZ 中有三个主要步骤：</p><ol><li>梯度切片重新排序，在任何通信发生之前，对梯度进行切片并对张量切片重新排序，以保证通信结束时每个 GPU 上的最终梯度位置是正确的</li><li>节点内通信和 reduce，量化重新排序的梯度切片，在每个节点内进行 all-to-all 通信，从 all-to-all 中对接收到的梯度切片进行反量化，并进行局部 reduce</li><li>节点间通信和 reduce，再次量化局部reduce后的梯度，进行节点间的all-to-all通信，再次对接收到的梯度进行反量化，并计算最终的高精度梯度 reduce</li></ol><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/6.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/7.webp" width="500"></p><p>给定每个节点 N 个 GPU、M 的模型大小和 Z 的量化比率，NCCL all-to-all 将生成 M*N/Z 跨节点流量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/8.webp" width="1000"></p><p>相比之下，通过 qgZ，将每个 GPU 的跨节点流量从 M/Z 减少到 M/(Z<em>N)。 因此，总通信量从 M</em>N/Z 减少到 M<em>N/(Z</em>N) = M/Z</p><p>此外，通过重叠节点内和节点间通信以及融合 CUDA 内核来进一步优化 qgZ 的端到端延迟</p><ul><li>张量切片重新排序 (Tensor Slice Reordering)</li><li>节点内量化 (Intra-node quantization)</li><li>节点内反量化 (Intra-node Dequantization)</li><li>节点内梯度整合 (Intra-node Reduction)</li><li>节点间量化 (inter-node quantization)</li></ul><h2 id="优化实现"><a href="#优化实现" class="headerlink" title="优化实现"></a>优化实现</h2><h3 id="Overlap-Compute-and-Communication"><a href="#Overlap-Compute-and-Communication" class="headerlink" title="Overlap Compute and Communication"></a>Overlap Compute and Communication</h3><ul><li>根据模型层的执行顺序</li><li>保证量化异步执行</li></ul><p>获取每一层的参数，在不同的 CUDA 流上同时启动当前层的通信和下一层的量化。当下一层需要量化数据时，ZeRO++同步量化流以确保量化数据准备就绪</p><p>这可以在当前层的通信时间跨度下隐藏了下一层的量化成本，隐藏了量化开销</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/9.webp" width="500"></p><p>基于 all to all 的梯度通信分为两个阶段：节点内通信和节点间通信</p><p>为了利用节点间通信执行时，节点内通信的空闲，实现了输入梯度张量的分块和 pipeline 传输</p><p>pipeline 阶段越多，重新排序所需的细粒度张量切片就越多</p><p>提出了一种广义张量切片重新排序算法，涵盖了 w/ 和 w/o pipeline 数据传输的情况</p><p>这里的 stages 是指拥有的 pipeline stage 的数量，nodeSize 是每个节点的 GPU 数量，nodes 是节点的数量</p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/10.webp" width="500"></p><h3 id="CUDA-Kernels"><a href="#CUDA-Kernels" class="headerlink" title="CUDA Kernels"></a>CUDA Kernels</h3><p>为了最大化带宽利用和最小化内核开销，论文中实现并优化了自定义CUDA核心，用于实现量化操作</p><p>开发了一个可组合运算符的核心量化和反量化库，利用高效的向量化内存访问来满足给定GPU架构支持的最大粒度。此外，利用指令级并行性重叠多个内存事务</p><p>使用多种技术减少量化内核的总内存流量。例如，调整每个量化块的大小，将张量重塑和量化融合到同一内核中，避免从全局内存中重复加载数据</p><p>此外，将连续的反量化、减少和量化操作融合到单一内核实现中</p><p>减少了 9 倍的内存流量</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ul><li>硬件配置：使用包含16个V100 SXM3 32 GB GPU的24个NVIDIA DGX-2节点。这些节点通过具有NVIDIA SHARP支持的InfiniBand（IB）连接，实现超过800 Gbps的节点间带宽</li><li>测试环境：为了评估ZeRO++在不同网络环境下的性能，展示了通过启用1到8个IB连接（即100 Gbps到800 Gbps）的ZeRO++运行性能</li><li>基准设置：使用ZeRO-3作为基准，因其便于大规模训练巨型模型。同时，为了评估优化内核的性能，还使用了PyTorch量化和非融合内核实现的ZeRO++作为消融研究的基线</li><li>模型配置：基于Megatron-Turing-NLG训练530B模型在2000个GPU上使用每GPU 2000 token的设置，对ZeRO++使用相同的2000 token设置进行评估。还评估了每 GPU 1000 token的设置，以测试ZeRO++在更极端规模的场景下的性能。调整层数和隐藏层大小以构建不同大小的模型</li></ul><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/11.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/12.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/13.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/14.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/15.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/16.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/17.webp" width="500"></p><p><img src="/image/论文阅读-ZeRO-Extremely-Eficient-Collective-Communication-for-Giant-Model-Training/18.webp" width="500"></p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>合并上述三项优化，使用 384 V100 GPU 进行大规模模型训练时系统吞吐量提高 2.16 倍</p></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6/" rel="tag">分布式训练框架</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2023/12/05/%E8%AE%B0%E5%BD%95%E7%AC%ACn%E6%AC%A1%E5%88%9B%E5%BB%BA%E5%B9%B6%E5%90%AF%E7%94%A8%E4%B8%B4%E6%97%B6swap/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">记录第n次创建启用并清理临时swap</div></a><a href="/2023/11/09/%E5%BC%80%E4%B8%AA%E6%96%B0%E5%9D%91-%E7%B2%BE%E8%AF%BBpytorch%E6%BA%90%E7%A0%81/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">开个新坑: 精读pytorch源码</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li><a target="_blank" href=https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024 style="display:inline-block;text-decoration:none;"> <img src="/images/备案图标.webp" style="float:left;width:14px;height:14px;margin-top:2px;margin-right:-3px"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备32090202001024号</p><a target="_blank" href="https://beian.miit.gov.cn" style="display:inline-block;text-decoration:none"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">&nbsp苏ICP备2023045098号</p></a></li><li>Aeeeeeep Blog | The Gleaners &copy; 2023</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>theme <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes, and seconds...</span><script>var now=new Date;function createTime(){var n=new Date("11/11/2021 00:08:39");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Site has been running for "+dnum+"d ",document.getElementById("times").innerHTML=hnum+"h "+mnum+"m "+snum+"s "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog | The Gleaners"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop"><i class="fe fe-rocket"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>