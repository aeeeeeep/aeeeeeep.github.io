<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>CUDA编程: PyCUDA编程简介 | Aeeeeeep Blog</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 7.0.0"></head><body><main class="content"><section class="outer"><article id="post-CUDA编程-PyCUDA编程简介" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">CUDA编程: PyCUDA编程简介</h1></header><div class="article-meta"><a href="/2023/12/26/CUDA%E7%BC%96%E7%A8%8B-PyCUDA%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/" class="article-date"><time datetime="2023-12-26T13:33:47.000Z" itemprop="datePublished">2023-12-26</time></a><div class="article-category"><a class="article-category-link" href="/categories/CUDA-%E7%BC%96%E7%A8%8B/">CUDA 编程</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>PyCUDA GPU 编程。主要内容是掌握几种PyCUDA 的基本内核函数，在 Python 中生成和调用并行化的 CUDA C 内核函数，以及了解 PyCUDA 的并行前缀算法。</p><span id="more"></span><h2 id="获取-GPU-信息"><a href="#获取-GPU-信息" class="headerlink" title="获取 GPU 信息"></a>获取 GPU 信息</h2><p>首先，需要初始化 CUDA，使用 <code>pycuda.driver.init()</code>或通过<code>import pycuda.autoinit</code>使用<code>autoinit</code>子模块初始化，接下来使用<code>pycuda.driver.Device(i)</code>来访问不同的 GPU 设备，这里笔者的电脑为单 GPU</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line">drv.init()</span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line"><span class="comment"># import pycuda.autoinit</span></span><br><span class="line"><span class="comment"># 进行自动初始化</span></span><br><span class="line"></span><br><span class="line">gpu_device = drv.Device(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;GPU Device Name&#123;&#125;: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(<span class="number">0</span>, gpu_device.name()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;SM Count: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MULTIPROCESSOR_COUNT))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Shared Memory Size per Thread Block: &#123;&#125; KB&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_SHARED_MEMORY_PER_BLOCK / <span class="number">1024.0</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Threads per Thread Block: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_BLOCK))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Threads per SM: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_MULTIPROCESSOR))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Warps per SM: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(gpu_device.MAX_THREADS_PER_MULTIPROCESSOR / <span class="number">32</span>))</span><br></pre></td></tr></table></figure><p>运行命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python infogpu.py</span><br></pre></td></tr></table></figure><p>输出如下，SM 数量为 30，每个线程块的共享内存为 48KB，每个线程块有 1024 个线程，每个 SM 有 1536 个线程，每个 SM 有 48 个线程束</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">GPU Device Name0: NVIDIA GeForce RTX 3060 Laptop GPU</span><br><span class="line">SM Count: 30</span><br><span class="line">Shared Memory Size per Thread Block: 48.0 KB</span><br><span class="line">Threads per Thread Block: 1024</span><br><span class="line">Threads per SM: 1536</span><br><span class="line">Warps per SM: 48.0</span><br></pre></td></tr></table></figure><h2 id="使用-gpuarray-类"><a href="#使用-gpuarray-类" class="headerlink" title="使用 gpuarray 类"></a>使用 gpuarray 类</h2><p>PyCUDA 库中，gpuarray 为数据的存储类型，正如 Numpy 库中的 array 为数据的存储类型</p><h3 id="传输数据"><a href="#传输数据" class="headerlink" title="传输数据"></a>传输数据</h3><p>在 CUDA C 中，我们需要使用 <code>cudaMemcpyHostToDevice</code>和<code>cudaMemcpyDeviceToHost</code>等函数来进行主机与设备的数据传输，需要跟踪主机与设备内存空间中的多个指针，还要进行内存分配，会使代码变得复杂，PyCUDA 的 gpuarray 能够自动完成内存分配，并根据生命周期自动释放。举例如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x_host = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=np.float32)</span><br><span class="line">y_host = np.array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], dtype=np.float32)</span><br><span class="line">z_host = np.array([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>], dtype=np.float32)</span><br><span class="line"></span><br><span class="line">x_device = gpuarray.to_gpu(x_host)</span><br><span class="line">y_device = gpuarray.to_gpu(y_host)</span><br><span class="line">z_device = gpuarray.to_gpu(z_host)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>((x_device + y_device + z_device).get())</span><br><span class="line"><span class="built_in">print</span>((x_device ** y_device).get())</span><br><span class="line"><span class="built_in">print</span>((x_device / z_device).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">[<span class="number">12.</span> <span class="number">15.</span> <span class="number">18.</span>]</span><br><span class="line">[  <span class="number">1.</span>       <span class="number">32.</span>      <span class="number">729.00006</span>]</span><br><span class="line">[<span class="number">0.14285715</span> <span class="number">0.25</span>       <span class="number">0.33333334</span>]</span><br></pre></td></tr></table></figure><p>上面的程序使用<code>gpuarray.to_gpu()</code>将 array 转为 gpuarray，使用 gpuarray 的<code>get()</code>属性将 gpuarray 转为 array，除此之外，gpuarray 还具有如下属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pycuda.gpuarray.zeros(shape, dtype=np.float64, *, allocator=<span class="literal">None</span>, order=<span class="string">&#x27;C&#x27;</span>)	<span class="comment"># 开辟gpu内存空间，创建 (m,n) 的0矩阵</span></span><br><span class="line">pycuda.gpuarray.empty(shape, dtype, *, allocator=<span class="literal">None</span>, order=<span class="string">&#x27;C&#x27;</span>)	<span class="comment"># 开辟gpu内存空间，创建 (m,n) 的空矩阵</span></span><br><span class="line">pycuda.gpuarray.zeros_like(other_ary, dtype=<span class="literal">None</span>, order=<span class="string">&#x27;K&#x27;</span>)	<span class="comment"># 开辟gpu内存空间，创建 size 同 ary 的 0矩阵，因此ary最好</span></span><br><span class="line">pycuda.gpuarray.empty_like(other_ary, dtype=<span class="literal">None</span>, order=<span class="string">&#x27;K&#x27;</span>)	<span class="comment"># 开辟gpu内存空间，创建 size 同 ary 的空矩阵</span></span><br><span class="line">pycuda.gpuarray.arange(start, stop, step, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)	<span class="comment"># 创建顺序序列</span></span><br><span class="line">pycuda.gpuarray.take(a, indices, stream=<span class="literal">None</span>)	<span class="comment"># 返回 gpu_ary[a[index[0]], ..., a[index[n]]]</span></span><br><span class="line">pycuda.gpuarray.maximum(a, b, out=<span class="literal">None</span>, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 对应a和b中的较大元素</span></span><br><span class="line">pycuda.gpuarray.minimum(a, b, out=<span class="literal">None</span>, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 对应a和b中的较小元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">max</span>(a, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 所有元素的最大元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">min</span>(a, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 所有元素的最小元素</span></span><br><span class="line">pycuda.gpuarray.<span class="built_in">sum</span>(a, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 所有元素求和</span></span><br><span class="line">pycuda.gpuarray.dot(a, b, dtype=<span class="literal">None</span>, stream=<span class="literal">None</span>)	<span class="comment"># gpu_ary 求点积</span></span><br></pre></td></tr></table></figure><p>上述函数中的<code>stream=None</code>意为指定 CUDA 流，我们会在后面的章节详细讲解。</p><p>更多属性使用方法请查看<a target="_blank" rel="noopener" href="https://documen.tician.de/pycuda/array.html#constructing-gpuarray-instances">官方文档</a></p><h2 id="PyCUDA-生成并行化-Kernel-函数"><a href="#PyCUDA-生成并行化-Kernel-函数" class="headerlink" title="PyCUDA 生成并行化 Kernel 函数"></a>PyCUDA 生成并行化 Kernel 函数</h2><p>PyCUDA 提供了大量函数来生成内联的 CUDA C 内核函数，内联函数通过 NVIDIA NVCC 编译器在外部编译缓存，由 PyCUDA 程序在运行时调用，下面为大家举几个主要例子，更详细的 PyCUDA 内核函数介绍请看<a target="_blank" rel="noopener" href="https://documen.tician.de/pycuda/array.html#module-pycuda.elementwise">官方文档</a></p><h3 id="ElementwiseKernel-逐元素运算"><a href="#ElementwiseKernel-逐元素运算" class="headerlink" title="ElementwiseKernel 逐元素运算"></a>ElementwiseKernel 逐元素运算</h3><p><code>ElementwiseKernel</code>函数用于对 gpuarray 数组并行化逐元素计算，函数声明如下</p><p><code>class pycuda.elementwise.ElementwiseKernel(arguments, operation, name=&#39;kernel&#39;, keep=False, options=[], preamble=&#39;&#39;)</code></p><ul><li><code>arguments</code>: 意为参数列表，内容是声明变量的字符串，为 C/C++ 语法，末尾无分号</li><li><code>operation</code>: 为变量赋值，对数组的操作需要用 <code>i</code> 进行索引，类会自动 并行化 GPU 各核心中与 <code>i</code> 有关的计算，为 C/C++ 语法，末尾无分号</li><li><code>name</code>: 指定内核函数名称，命名空间在 CUDA C 中</li><li><code>options</code>: 构建时要使用的编译器选项，为<code>list</code>类型</li><li><code>keep</code>: 为<code>True</code>则保留编译器输出目录，并打印一行指示其所在目录以用于调试，<code>False</code>则不保留</li><li><code>preamble</code>: 指定内核构造之前包含的代码，可以使用它来包含其他文件和定义<code>operation</code>使用的函数</li></ul><p>下例的程序定义了一个名为 <code>gpu_x2_kernel</code>的内核函数，实现了输入数组<code>in</code>中每个元素的平方，将结果数组<code>out</code>中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pycuda.elementwise <span class="keyword">import</span> ElementwiseKernel</span><br><span class="line">host_data = np.float32(np.random.random(<span class="number">1</span>&lt;&lt;<span class="number">20</span>))</span><br><span class="line">gpu_x2_kernel = ElementwiseKernel(</span><br><span class="line">    <span class="string">&quot;float *in, float *out&quot;</span>,    <span class="comment"># arguments</span></span><br><span class="line">    <span class="string">&quot;out[i] = in[i] * in[i]&quot;</span>,   <span class="comment"># operation</span></span><br><span class="line">    <span class="string">&quot;gpu_x2_kernel&quot;</span>             <span class="comment"># name</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>下面的程序比较了 gpuarray 数组运算与<code>ElementwiseKernel</code>类运算的速度，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> pycuda.elementwise <span class="keyword">import</span> ElementwiseKernel</span><br><span class="line"></span><br><span class="line">host_data = np.float32(np.random.random(<span class="number">1</span>&lt;&lt;<span class="number">29</span>))</span><br><span class="line"></span><br><span class="line">gpu_x2_kernel = ElementwiseKernel(</span><br><span class="line">    <span class="string">&quot;float *in, float *out&quot;</span>,</span><br><span class="line">    <span class="string">&quot;out[i] = in[i] * in[i]&quot;</span>,</span><br><span class="line">    <span class="string">&quot;gpu_x2_kernel&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">speedcomparison</span>():</span><br><span class="line">    t1 = time()</span><br><span class="line">    host_data_x2 =  host_data * host_data</span><br><span class="line">    t2 = time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time to compute on CPU: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t2 - t1))</span><br><span class="line">    device_data = gpuarray.to_gpu(host_data)</span><br><span class="line">    <span class="comment"># allocate memory for output</span></span><br><span class="line">    device_data_2x = gpuarray.empty_like(device_data)</span><br><span class="line">    t1 = time()</span><br><span class="line">    gpu_x2_kernel(device_data, device_data_2x)</span><br><span class="line">    t2 = time()</span><br><span class="line">    from_device = device_data_2x.get()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;total time to compute on GPU: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(t2 - t1))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Is the host computation the same as the GPU computation? : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(from_device, host_data_x2)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    speedcomparison()</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 在 ipython 中运行输出为</span></span><br><span class="line">In [<span class="number">1</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.36846113204956055</span></span><br><span class="line">total time to compute on GPU: <span class="number">0.07349300384521484</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.368114709854126</span></span><br><span class="line">total time to compute on GPU: <span class="number">0.03887605667114258</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: run speedcomparison.py</span><br><span class="line">total time to compute on CPU: <span class="number">0.3670005798339844</span>dair_v2x_root</span><br><span class="line">total time to compute on GPU: <span class="number">0.039081573486328125</span></span><br><span class="line">Is the host computation the same <span class="keyword">as</span> the GPU computation? : <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>可以看到在首次运行 GPU 的内核函数后，运算时间会大幅减少，这正是因为 NVCC 编译器会编译好的内核函数缓存起来，以重复使用</p><p>其中在内核函数中定义的<code>float *out</code>为 C 语言的浮点型指针，所以会使用<code>gpuarray.empty_like</code>先在 GPU 开辟内存空间，将运算结果放入其中，再使用<code>get</code>函数拷贝回主机</p><h3 id="InclusiveScanKernel-扫描内核函数"><a href="#InclusiveScanKernel-扫描内核函数" class="headerlink" title="InclusiveScanKernel 扫描内核函数"></a>InclusiveScanKernel 扫描内核函数</h3><p><code>InclusiveScanKernel</code>函数通过指定输入类型<code>dtype</code>和<code>string</code>类型的算术表达式<code>scan_expr</code>进行内核函数构造，再对数组逐元素进行并行化运算，函数声明如下</p><p><code>class pycuda.scan.InclusiveScanKernel(dtype, scan_expr, neutral=None, name_prefix=&#39;scan&#39;, options=[], preamble=&#39;&#39;, devices=None)</code></p><ul><li><code>dtype</code>: 指定输入数组类型</li><li><code>scan_expr</code>: 任意符合结合律的二元运算表达式，必须是 C 语言编写</li><li><code>neutral</code>: 中性元，加法应为 0，乘法应为 1，本小节后面会详细讲解原因</li><li><code>name_prefix</code>: 用于内核名称以确保配置文件和日志中的可识别性</li><li><code>options</code> 与 <code>preamble</code> 含义同上</li></ul><p>下面的程序对向量逐元素作加法以及逐元素查找最大值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.scan <span class="keyword">import</span> InclusiveScanKernel</span><br><span class="line"></span><br><span class="line">seq1 = np.array(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">10</span>)),dtype=np.int32)</span><br><span class="line">seq2 = np.array([<span class="number">1</span>,<span class="number">100</span>,-<span class="number">3</span>,-<span class="number">1000</span>,<span class="number">4</span>,<span class="number">1000000</span>,<span class="number">65</span>,<span class="number">2454</span>],dtype=np.int32)</span><br><span class="line">seq1_gpu = gpuarray.to_gpu(seq1)</span><br><span class="line">seq2_gpu = gpuarray.to_gpu(seq2)</span><br><span class="line">sum_gpu = InclusiveScanKernel(np.int32, <span class="string">&quot;a + b&quot;</span>)</span><br><span class="line">max_gpu = InclusiveScanKernel(np.int32, <span class="string">&quot;a&gt;b ? a : b&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(sum_gpu(seq1_gpu).get())</span><br><span class="line"><span class="built_in">print</span>(max_gpu(seq2_gpu).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">[ <span class="number">0</span>  <span class="number">1</span>  <span class="number">3</span>  <span class="number">6</span> <span class="number">10</span> <span class="number">15</span> <span class="number">21</span> <span class="number">28</span> <span class="number">36</span> <span class="number">45</span>]</span><br><span class="line">[      <span class="number">1</span>     <span class="number">100</span>     <span class="number">100</span>     <span class="number">100</span>     <span class="number">100</span> <span class="number">1000000</span> <span class="number">1000000</span> <span class="number">1000000</span>]</span><br></pre></td></tr></table></figure><p>上例所示，<code>sum_gpu</code>为<code>&quot;a + b&quot;</code>，包含<code>a</code>和<code>b</code>代指的二元运算的两个元素，类似 Python 中的 <code>lambda a,b : a + b</code>，<code>max_gpu</code>类似<code>lambda a,b : max(a,b)</code></p><h3 id="ReductionKernel-规约内核函数"><a href="#ReductionKernel-规约内核函数" class="headerlink" title="ReductionKernel 规约内核函数"></a>ReductionKernel 规约内核函数</h3><p><code>ReductionKernel</code>函数对<code>arguments</code>并行执行 <code>map expr</code>，然后对其结果执行 <code>reduce expr</code>，相当于<code>InclusiveScanKernel</code>函数后接着<code>ElementwiseKernel</code>函数，函数声明如下</p><p><code>class pycuda.reduction.ReductionKernel(dtype_out, neutral, reduce_expr, map_expr=None, arguments=None, name=&#39;reduce_kernel&#39;, keep=False, options=[], preamble=&#39;&#39;, allocator=None)</code></p><ul><li><code>dtype_out</code>: 指定返回数据类型</li><li><code>neutral</code>: 中性元，加法应为 0，乘法应为 1，本小节后面会详细讲解原因</li><li><code>reduce_expr</code>: 相当于<code>InclusiveScanKernel</code>函数的<code>reduce_expr</code></li><li><code>map_expr</code>: 相当于<code>ElementwiseKernel</code>函数的<code>operation</code></li><li>其余关键字同上</li></ul><p>下面的程序计算了两个向量的点积</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.reduction <span class="keyword">import</span> ReductionKernel</span><br><span class="line"></span><br><span class="line">seq1 = np.array(<span class="built_in">range</span>(<span class="number">10</span>),dtype=np.float32)</span><br><span class="line">seq2 = np.array(<span class="built_in">range</span>(<span class="number">10</span>),dtype=np.float32)</span><br><span class="line">seq1_gpu = gpuarray.to_gpu(seq1)</span><br><span class="line">seq2_gpu = gpuarray.to_gpu(seq2)</span><br><span class="line">dot_prod = ReductionKernel(np.float32, neutral=<span class="string">&quot;0&quot;</span>, reduce_expr=<span class="string">&quot;a+b&quot;</span>, map_expr=<span class="string">&quot;x[i] * y[i]&quot;</span>,arguments=<span class="string">&quot;float *x, float *y&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.dot(seq1,seq2))</span><br><span class="line"><span class="built_in">print</span>(dot_prod(seq1_gpu, seq2_gpu).get())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line"><span class="number">285.0</span></span><br><span class="line"><span class="number">285.0</span></span><br></pre></td></tr></table></figure><h2 id="PyCUDA-SourceModule-并行函数"><a href="#PyCUDA-SourceModule-并行函数" class="headerlink" title="PyCUDA SourceModule 并行函数"></a>PyCUDA SourceModule 并行函数</h2><p><code>SourceModule</code>函数将原来的内联 CUDA C 代码编译成可以从 Python 启动的内核函数，与上述所有函数不同的是，<code>SourceModule</code>函数在<code>source</code>中直接编写 CUDA C 整体内核函数代码，函数声明如下</p><p><code>class pycuda.compiler.SourceModule(source, nvcc=&#39;nvcc&#39;, options=None, keep=False, no_extern_c=False, arch=None, code=None, cache_dir=None, include_dirs=[])</code></p><ul><li><code>source</code>: 内核函数代码，使用 CUDA C 编写</li><li><code>nvcc</code>: 编译器命令</li><li><code>no_extern_c</code>: 默认为 <code>False</code>，<code>source</code>会被包装在 extern “C” { … } 中以防止 C++ 名称重整，为<code>True</code>则不会被包装在 extern “C” { … } 中</li><li><code>arch</code>: 指定传递给 nvcc 命令上 <code>-arch</code> 选项的值</li><li><code>code</code>: 指定传递给 nvcc 命令上 <code>-code</code> 选项的值</li><li><code>cache_dir</code>: 指定用于编译器缓存的目录，默认为<code>PYCUDA_CACHE_DIR</code>所指目录，为<code>False</code>则禁用缓存，如果环境变量<code>PYCUDA_DISABLE_CACHE</code>被赋值，意为全局禁用缓存，<code>cache_dir</code>将被覆盖</li><li><code>include_dirs</code>: 将一个或多个外部类传递给<code>source</code>，类型为 string list</li><li>其余关键字同上</li></ul><p>需要使用<code>get_function</code>得到已编译好的内核函数的引用，才可使用内核函数，下例程序计算了矩阵乘法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> cuda</span><br><span class="line"><span class="keyword">import</span> pycuda.tools</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> numpy.linalg <span class="keyword">as</span> la</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"> </span><br><span class="line">mod = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void matrixMultiply(float * A, float * B, float * C,</span></span><br><span class="line"><span class="string">                   int A_shape_0,int A_shape_1,int B_shape_1) &#123;</span></span><br><span class="line"><span class="string">    float cValue = 0;</span></span><br><span class="line"><span class="string">    int Row = blockIdx.y * blockDim.y + threadIdx.y;</span></span><br><span class="line"><span class="string">    int Col = blockIdx.x * blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">    if ((Row &lt; A_shape_0) &amp;&amp; (Col &lt; B_shape_1)) &#123;</span></span><br><span class="line"><span class="string">        for (int k = 0; k &lt; A_shape_1; k++) &#123;</span></span><br><span class="line"><span class="string">            cValue += A[Row*A_shape_1 + k] * B[k*B_shape_1 + Col];</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        C[Row*B_shape_1 + Col] = cValue;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"> </span><br><span class="line">matrixMultiply = mod.get_function(<span class="string">&quot;matrixMultiply&quot;</span>)</span><br><span class="line">n = <span class="number">1000</span></span><br><span class="line">A = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1000000</span>).reshape(<span class="number">1000</span>,<span class="number">1000</span>).astype(np.float32)</span><br><span class="line">B = np.random.randint(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1000000</span>).reshape(<span class="number">1000</span>,<span class="number">1000</span>).astype(np.float32)</span><br><span class="line"></span><br><span class="line">BLOCK_SIZE = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">A_gpu = gpuarray.to_gpu(A)</span><br><span class="line">B_gpu = gpuarray.to_gpu(B)</span><br><span class="line">C_gpu = gpuarray.empty_like(A_gpu)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> n % BLOCK_SIZE != <span class="number">0</span>:</span><br><span class="line">    grid=(n // BLOCK_SIZE+<span class="number">1</span>, n // BLOCK_SIZE+<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    grid=(n // BLOCK_SIZE, n // BLOCK_SIZE,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;grid size: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(grid))</span><br><span class="line">start = time.time()</span><br><span class="line">matrixMultiply(A_gpu, B_gpu, C_gpu, np.int32(A.shape[<span class="number">0</span>]),np.int32(A.shape[<span class="number">1</span>]),np.int32(B.shape[<span class="number">1</span>]), block=(BLOCK_SIZE,BLOCK_SIZE,<span class="number">1</span>), grid=grid);</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;result:\n&quot;</span>, C_gpu.get())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;time cost: %ss&quot;</span> %(time.time() - start))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出为</span></span><br><span class="line">grid size: (<span class="number">32</span>, <span class="number">32</span>, <span class="number">1</span>)</span><br><span class="line">result:</span><br><span class="line"> [[<span class="number">19305.</span> <span class="number">18274.</span> <span class="number">18786.</span> ... <span class="number">19130.</span> <span class="number">19501.</span> <span class="number">18229.</span>]</span><br><span class="line"> [<span class="number">19945.</span> <span class="number">19823.</span> <span class="number">20158.</span> ... <span class="number">19853.</span> <span class="number">20397.</span> <span class="number">19848.</span>]</span><br><span class="line"> [<span class="number">20651.</span> <span class="number">19959.</span> <span class="number">20910.</span> ... <span class="number">20715.</span> <span class="number">21033.</span> <span class="number">20364.</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">20196.</span> <span class="number">19777.</span> <span class="number">20198.</span> ... <span class="number">20291.</span> <span class="number">20736.</span> <span class="number">19916.</span>]</span><br><span class="line"> [<span class="number">20239.</span> <span class="number">19820.</span> <span class="number">20210.</span> ... <span class="number">20171.</span> <span class="number">20685.</span> <span class="number">19244.</span>]</span><br><span class="line"> [<span class="number">19595.</span> <span class="number">19444.</span> <span class="number">19546.</span> ... <span class="number">19862.</span> <span class="number">20462.</span> <span class="number">19508.</span>]]</span><br><span class="line">time cost: <span class="number">0.003673076629638672</span>s</span><br></pre></td></tr></table></figure><h2 id="并行前缀算法"><a href="#并行前缀算法" class="headerlink" title="并行前缀算法"></a>并行前缀算法</h2><p>前面提到的<code>InclusiveScanKernel</code>函数和函数就是并行扫描算法之一，并行扫描算法定义如下</p><p>定义二元运算符 $\oplus$ 和一个有 $n$ 个元素的数组 $[x<em>0, x_1, x_2, \cdots, x</em>{n-1}]$</p><p>返回</p><script type="math/tex;mode=display">[x_0, (x_0 \oplus x_1), (x_0 \oplus x_1 \oplus x_2), \cdots, (x_0 \oplus x_1 \oplus \cdots \oplus x_{n-1})]</script><p>以加法举例，输入为 <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code>，返回为<code>[ 0 1 3 6 10 15 21 28 36 45]</code>，如果用串行的计算方式，需要计算 $n$ 次求和，时间复杂度为$O(n)$，我们要尽量降低时间复杂度</p><h3 id="朴素并行前缀算法"><a href="#朴素并行前缀算法" class="headerlink" title="朴素并行前缀算法"></a>朴素并行前缀算法</h3><p>朴素并行扫描算法是并行扫描算法的初始版本，我们假设输入元素为 n 个，且 n 为 2 的次幂，同时在 n 个线程上并行计算，时间复杂度为 $O(\log_{2} n)$，下图为算法示意图（<a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/hillis-steele-scan-parallel-prefix-scan-algorithm/">原图链接</a>）</p><p><img src="/image/CUDA编程-PyCUDA编程简介/1.png" alt=""></p><p>下面的程序以加法为例，计算了长度为 1024 的数组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">naive_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void naive_scan(double *vec, double *out) &#123;</span></span><br><span class="line"><span class="string">     __shared__ double sum_buf[1024];     </span></span><br><span class="line"><span class="string">     int tid = threadIdx.x;</span></span><br><span class="line"><span class="string">     sum_buf[tid] = vec[tid];</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     // begin parallel prefix sum algorithm</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int iter = 1;</span></span><br><span class="line"><span class="string">     for (int i=0; i &lt; 10; i++) &#123;</span></span><br><span class="line"><span class="string">         __syncthreads();</span></span><br><span class="line"><span class="string">         if (tid &gt;= iter ) &#123;</span></span><br><span class="line"><span class="string">             sum_buf[tid] = sum_buf[tid] + sum_buf[tid - iter];</span></span><br><span class="line"><span class="string">         &#125;</span></span><br><span class="line"><span class="string">         iter *= 2;</span></span><br><span class="line"><span class="string">     &#125;</span></span><br><span class="line"><span class="string">    __syncthreads();</span></span><br><span class="line"><span class="string">    out[tid] = sum_buf[tid];</span></span><br><span class="line"><span class="string">    __syncthreads();</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">naive_gpu = naive_ker.get_function(<span class="string">&quot;naive_scan&quot;</span>)</span><br><span class="line"></span><br><span class="line">testvec = np.random.randn(<span class="number">1024</span>).astype(np.float64)</span><br><span class="line">testvec_gpu = gpuarray.to_gpu(testvec)</span><br><span class="line">    </span><br><span class="line">outvec_gpu = gpuarray.empty_like(testvec_gpu)</span><br><span class="line">naive_gpu( testvec_gpu , outvec_gpu, block=(<span class="number">1024</span>,<span class="number">1</span>,<span class="number">1</span>), grid=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">total_sum = <span class="built_in">sum</span>( testvec)</span><br><span class="line">total_sum_gpu = outvec_gpu[-<span class="number">1</span>].get()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Does our kernel work correctly? : &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(total_sum_gpu , total_sum)))</span><br></pre></td></tr></table></figure><p>其中使用了<code>__syncthreads()</code>函数，保证了线程块中的所有线程在进入下一次迭代前，当前迭代里每一个线程的所有部分都被保存在了全局内存中</p><h3 id="高效并行前缀算法"><a href="#高效并行前缀算法" class="headerlink" title="高效并行前缀算法"></a>高效并行前缀算法</h3><p>上一例的算法只能计算长度为 1024 的数组进行计算，且需要与长度等量的线程数量，接下来我们实现支持长度超过 1024 的任意数组的算法，此算法需要两个内核函数和 python 函数，分加进行上行扫描和下行扫描。上行扫描阶段类似于一次规约操作，即$x<em>0 \oplus \cdots \oplus x</em>{n-1}$，下行扫描阶段将对上行扫描的返回值进行计算，返回最终结果，具体代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> pycuda.autoinit</span><br><span class="line"><span class="keyword">import</span> pycuda.driver <span class="keyword">as</span> drv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pycuda <span class="keyword">import</span> gpuarray</span><br><span class="line"><span class="keyword">from</span> pycuda.compiler <span class="keyword">import</span> SourceModule</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># this is a work-efficent parallel prefix-sum algorithm.</span></span><br><span class="line"><span class="comment"># written by Brian Tuomanen for &quot;Hands On GPU Programming with Python and CUDA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel for up-sweep phase</span></span><br><span class="line">up_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void up_ker(double *x, double *x_old, int k ) &#123;</span></span><br><span class="line"><span class="string">     int tid =  blockIdx.x*blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int _2k = 1 &lt;&lt; k;</span></span><br><span class="line"><span class="string">     int _2k1 = 1 &lt;&lt; (k+1);</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int j = tid* _2k1;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     x[j + _2k1 - 1] = x_old[j + _2k -1 ] + x_old[j + _2k1 - 1];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">up_gpu = up_ker.get_function(<span class="string">&quot;up_ker&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># implementation of up-sweep phase</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">up_sweep</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="comment"># let&#x27;s typecast to be safe.</span></span><br><span class="line">    x = np.float64(x)</span><br><span class="line">    x_gpu = gpuarray.to_gpu(np.float64(x) )</span><br><span class="line">    x_old_gpu = x_gpu.copy()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>( <span class="built_in">int</span>(np.log2(x.size) ) ) : </span><br><span class="line">        num_threads = <span class="built_in">int</span>(np.ceil( x.size / <span class="number">2</span>**(k+<span class="number">1</span>)))</span><br><span class="line">        grid_size = <span class="built_in">int</span>(np.ceil(num_threads / <span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            block_size = <span class="number">32</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            block_size = num_threads</span><br><span class="line">            </span><br><span class="line">        up_gpu(x_gpu, x_old_gpu, np.int32(k)  , block=(block_size,<span class="number">1</span>,<span class="number">1</span>), grid=(grid_size,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        x_old_gpu[:] = x_gpu[:]</span><br><span class="line">        </span><br><span class="line">    x_out = x_gpu.get()</span><br><span class="line">    <span class="keyword">return</span>(x_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># kernel for down-sweep phase</span></span><br><span class="line">down_ker = SourceModule(<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">__global__ void down_ker(double *y, double *y_old,  int k) &#123;</span></span><br><span class="line"><span class="string">     int tid =  blockIdx.x*blockDim.x + threadIdx.x;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int _2k = 1 &lt;&lt; k;</span></span><br><span class="line"><span class="string">     int _2k1 = 1 &lt;&lt; (k+1);</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     int j = tid*_2k1;</span></span><br><span class="line"><span class="string">     </span></span><br><span class="line"><span class="string">     y[j + _2k - 1 ] = y_old[j + _2k1 - 1];</span></span><br><span class="line"><span class="string">     y[j + _2k1 - 1] = y_old[j + _2k1 - 1] + y_old[j + _2k - 1];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">down_gpu = down_ker.get_function(<span class="string">&quot;down_ker&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># implementation of down-sweep phase</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_sweep</span>(<span class="params">y</span>):</span><br><span class="line">    y = np.float64(y)</span><br><span class="line">    y[-<span class="number">1</span>] = <span class="number">0</span></span><br><span class="line">    y_gpu = gpuarray.to_gpu(y)</span><br><span class="line">    y_old_gpu = y_gpu.copy()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">int</span>(np.log2(y.size)))):</span><br><span class="line">        num_threads = <span class="built_in">int</span>(np.ceil( y.size / <span class="number">2</span>**(k+<span class="number">1</span>)))</span><br><span class="line">        grid_size = <span class="built_in">int</span>(np.ceil(num_threads / <span class="number">32</span>))</span><br><span class="line">        <span class="keyword">if</span> grid_size &gt; <span class="number">1</span>:</span><br><span class="line">            block_size = <span class="number">32</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            block_size = num_threads</span><br><span class="line">        down_gpu(y_gpu, y_old_gpu, np.int32(k), block=(block_size,<span class="number">1</span>,<span class="number">1</span>), grid=(grid_size,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">        y_old_gpu[:] = y_gpu[:]</span><br><span class="line">    y_out = y_gpu.get()</span><br><span class="line">    <span class="keyword">return</span>(y_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># full implementation of work-efficient parallel prefix sum</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">efficient_prefix</span>(<span class="params">x</span>):</span><br><span class="line">	<span class="keyword">return</span>(down_sweep(up_sweep(x)))</span><br><span class="line"></span><br><span class="line">testvec = np.random.randn(<span class="number">32</span>*<span class="number">1024</span>).astype(np.float64)</span><br><span class="line">testvec_gpu = gpuarray.to_gpu(testvec)</span><br><span class="line"></span><br><span class="line">outvec_gpu = gpuarray.empty_like(testvec_gpu)</span><br><span class="line"></span><br><span class="line">prefix_sum = np.roll(np.cumsum(testvec), <span class="number">1</span>)</span><br><span class="line">prefix_sum[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">prefix_sum_gpu = efficient_prefix(testvec)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Does our work-efficient prefix work? &#123;&#125;&quot;</span>.<span class="built_in">format</span>(np.allclose(prefix_sum_gpu, prefix_sum)))</span><br></pre></td></tr></table></figure></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CUDA/" rel="tag">CUDA</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2024/01/02/CUDA%E7%BC%96%E7%A8%8B-CUDA%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%E6%A6%82%E8%BF%B0/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">CUDA编程: CUDA内存模型概述</div></a><a href="/2023/12/19/CUDA%E7%BC%96%E7%A8%8B-CUDA-C%E7%BC%96%E7%A8%8B%E7%AE%80%E4%BB%8B/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">CUDA编程: CUDA C编程简介</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li><a target="_blank" href=https://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024 style="display:inline-block;text-decoration:none;"> <img src="/images/备案图标.webp" style="float:left;width:14px;height:14px;margin-top:2px;margin-right:-3px"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备32090202001024号</p><a target="_blank" href="https://beian.miit.gov.cn" style="display:inline-block;text-decoration:none"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">&nbsp苏ICP备2023045098号</p></a></li><li>Aeeeeeep Blog &copy; 2024</li><li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li><li>theme <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes...</span><script>var now=new Date,grt=new Date("11/11/2021 00:08:39");function daysInYear(e){return e%4==0&&e%100!=0||e%400==0?366:365}function createTime(){now.setTime(now.getTime()+250);for(var e=0,t=new Date(grt.getTime());t<=now;){var n=daysInYear(t.getFullYear());t.setDate(t.getDate()+n),t<=now&&e++}t.setDate(t.getDate()-daysInYear(t.getFullYear()));var a=Math.floor((now-t)/864e5),r=now.getHours(),o=now.getMinutes(),i=r<10?"0"+r:r,g=o<10?"0"+o:o;document.getElementById("timeDate").innerHTML="Site has been running for "+e+"y "+a+"d ",document.getElementById("times").innerHTML=i+"h "+g+"m "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop" style="font-size:24px"><i class="fe fe-drop-up"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>