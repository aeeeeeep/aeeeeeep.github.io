<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>多目标优化笔记 | Aeeeeeep Blog</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><main class="content"><section class="outer"><article id="post-多目标优化笔记" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">多目标优化笔记</h1></header><div class="article-meta"><a href="/2022/04/19/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/" class="article-date"><time datetime="2022-04-19T01:58:03.000Z" itemprop="datePublished">2022-04-19</time></a><div class="article-category"><a class="article-category-link" href="/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/">数学建模</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>多目标优化 ( MOO ) - Multi-Objective Optimization</p><p>从问题定义，单目标、多目标，无约束、有约束方面了解多目标优化</p><span id="more"></span><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="无约束的单目标优化问题"><a href="#无约束的单目标优化问题" class="headerlink" title="无约束的单目标优化问题"></a>无约束的单目标优化问题</h3><script type="math/tex;mode=display">\min_{x} f(x),x \in R^{N}</script><h3 id="无约束的多目标优化问题"><a href="#无约束的多目标优化问题" class="headerlink" title="无约束的多目标优化问题"></a>无约束的多目标优化问题</h3><script type="math/tex;mode=display">\min_{x} F(x) = [f_1(x), f_2(x),\cdots, f_n(x)], x \in R^{N}</script><p>$ n $ 为子目标的数量，$ f_n(x) $为一阶可导目标函数 $F(x)$ 的子函数</p><h3 id="有约束的单目标优化问题"><a href="#有约束的单目标优化问题" class="headerlink" title="有约束的单目标优化问题"></a>有约束的单目标优化问题</h3><script type="math/tex;mode=display">\begin{array}{lcl}
\min_{x} & f(x) & \\
\text { s.t. } & g_{i}(x) \geq 0 & , i \in[1, M] \\
& h_{j}(x)=0 & , j \in[1, L]
\end{array}</script><p>$\text { s.t. }$ 为 <code>subject to</code> ，受限于的缩写，设 $ D $ 为可行域</p><script type="math/tex;mode=display">D = \{x | g_i(x) \geq 0, i \in [1, M], h_j(x) =0 , j \in [1,L]  \}</script><h3 id="有约束的多目标优化问题"><a href="#有约束的多目标优化问题" class="headerlink" title="有约束的多目标优化问题"></a>有约束的多目标优化问题</h3><script type="math/tex;mode=display">\begin{array}{lcl}
\min_x & F(x) = [f_1(x), f_2(x),\cdots, f_n(x)]\\
\text{s.t.} & g_i(x) \geq 0 ,\quad i\in(1, M) \\
& h_j(x) = 0 ,\quad j \in (1,L)
\end{array}</script><p>设 $ D $ 为可行域</p><script type="math/tex;mode=display">D = \{x | g_i(x) \geq 0, i \in [1, M], h_j(x) =0 , j \in [1,L]  \}</script><h2 id="MOO-的解集"><a href="#MOO-的解集" class="headerlink" title="MOO 的解集"></a>MOO 的解集</h2><p>对于 MOO，通常没有解 $ x^\ast \in D $, 使 $ f_i(x), \forall i \in [1,N] $ 同时处于最优解，因此单目标优化问题的解在 MOO 中通常不适用</p><p>MOO 中的解集分为 <strong>绝对有效解</strong>，<strong>有效解</strong>，<strong>弱有效解</strong></p><p>设 $R^N $为 $N$ 维的实向量空间，$y=(y_1,y_2,\cdots, y_N)^{T}$，$z=(z_1,z_2,\cdots, z_N)^{T}$</p><script type="math/tex;mode=display">\begin{cases}\text { 相等 } & y=z \Leftrightarrow y_{i}=z_{i}, i=1,2, \ldots, N \\ \text { 严格小于 } & y<z \Leftrightarrow y_{i}<z_{i}, i=1,2, \ldots, N \\ \text { 小于 } & y \leqq z \Leftrightarrow y_{i} \leqslant z_{i}, i=1,2, \ldots, N \\ \text { 小于且不相等(支配) } & y \leqslant z \Leftrightarrow y_{i} \leqslant z_{i}, i=1,2, \ldots, N, y \neq z\end{cases}</script><h3 id="Pareto-支配（Pareto-Dominance）"><a href="#Pareto-支配（Pareto-Dominance）" class="headerlink" title="Pareto 支配（Pareto Dominance）"></a>Pareto 支配（Pareto Dominance）</h3><p>$ \forall x_1, x_2 \in R^{N} $， 对于 $ k = 1,\cdots,K $，都有 $ f_k(x_1) \leqslant f_k(x_2)$，则 $ x_1 $ 支配 $ x_2 $</p><p><img src="/image/多目标优化笔记/D.jpg" style="zoom:67%"></p><h3 id="Pareto-解集（绝对最优解）"><a href="#Pareto-解集（绝对最优解）" class="headerlink" title="Pareto 解集（绝对最优解）"></a>Pareto 解集（绝对最优解）</h3><p>$ x^{\ast}{\in}{D} $ ， $ \forall x \in D,\quad f(x^\ast) {\leqq} f(x) $ ，即 $ \forall k \in 1,\cdots,K, \quad f_K(x^\ast) \leqq f_{K}(x) $ ，则 $ x^\ast $ 为 MOO 问题的最优解</p><h3 id="Pareto-解集（有效解）"><a href="#Pareto-解集（有效解）" class="headerlink" title="Pareto 解集（有效解）"></a>Pareto 解集（有效解）</h3><p>$x^\ast\in{D}$ ，若 $f_k(x)\leq f_k(x^\ast) \wedge \exists i,f_i(x) &lt; f_i(x^\ast),i\in [1,k]$ 不成立，则 $x^\ast$ 是 MOO 问题的有效解，也叫 Pareto 最优解，其含义是如果 $x^\ast$ 是 Pareto 最优解，则找不到这样的可行解 $x\in{D}$ ，使得 $f(x)$ 的每个目标值都不比 $f(x^\ast)$ 的目标值坏,并且 $ f (x) $ 至少有一个目标比 $f(x^\ast)$ 的相应目标值好，即 $ x^\ast $ 是最好的，不能再进行改进（Pareto 改进）</p><h3 id="Pareto-解集（弱有效解）"><a href="#Pareto-解集（弱有效解）" class="headerlink" title="Pareto 解集（弱有效解）"></a>Pareto 解集（弱有效解）</h3><p>$x^\ast\in{D}$ ，如果不存在 $x\in{D}$，使得 $f(x)&lt;f(x^\ast)$ ，即</p><script type="math/tex;mode=display">f_k(x) < f_k(x^*) \quad \wedge \quad \forall k \in [1,K]</script><p>则 $x^\ast$ 是 MOO 问题的有效解，其含义是如果 $x^\ast$ 是弱有效解,则找不到这样的可行解 $x\in{D}$，使得 $f(x)$ 的每个目标值都比 $f(x^\ast)$ 的目标值严格（ $&lt;$ ）的好</p><h3 id="Pareto-最优解集（Pareto-optimal-Set）"><a href="#Pareto-最优解集（Pareto-optimal-Set）" class="headerlink" title="Pareto 最优解集（Pareto-optimal Set）"></a>Pareto 最优解集（Pareto-optimal Set）</h3><p>给定问题的有效解集（Pareto 最优解）构成的解集，集合中的解是相互非支配的，两两非支配关系，简称 $PS$</p><h3 id="Pareto-最优前沿（Pareto-optimal-Front）"><a href="#Pareto-最优前沿（Pareto-optimal-Front）" class="headerlink" title="Pareto 最优前沿（Pareto-optimal Front）"></a>Pareto 最优前沿（Pareto-optimal Front）</h3><p>Pareto 每一个解对应的目标值向量组成的集合，简称 $PF$</p><script type="math/tex;mode=display">PF = \{F(x)|x\in PS\}</script><p><img src="/image/多目标优化笔记/PF.jpg" style="zoom:50%"></p><h3 id="MOO-的最优性条件"><a href="#MOO-的最优性条件" class="headerlink" title="MOO 的最优性条件"></a>MOO 的最优性条件</h3><p>约束规格定义：对优化问题的约束函数，附加某些限制条件，使得其最优解满足的最优性条件</p><p>下面给出一个严格条件下多目标优化的充分必要条件，给出的充要条件前，先引入了约束规格条件</p><script type="math/tex;mode=display">\begin{aligned}
\min _{x \in \hat{D}} F(x) &=\sum_{k=1}^{K} f_{k}(x) \\
\hat{D} &=x \in D \mid f(x) \leq f(\hat{x})
\end{aligned}</script><p>定理：设 $ f (x)$ ，$ g(x) $ 为凸函数,且在 $x \in D$ 处可微，$h(x)$ 为线性函数，且 $\hat{D} = x \in D|f (x) \leq f (\hat{x})$ 满足 $ KKT $ 约束规格，则 $x^\ast$ 是 MOO 的有效解的充分必要条件是存在 $ \lambda \in R^K , u \in R^M , v \in R^L$， 使得</p><script type="math/tex;mode=display">\left\{\begin{array}{l}
\nabla_{x} L\left(x^{*}, \lambda^{*}, u^{*}, v^{*}\right)=\nabla f\left(x^{*}\right) \lambda^{*}+\nabla g\left(x^{*}\right) u^{*}+\nabla h\left(x^{*}\right) v^{*}=0 \\
u^{* T} g\left(x^{*}\right)=0 \\
\lambda^{*}>0, u^{*} \geq 0
\end{array}\right.</script><h2 id="MOO-的经典算法"><a href="#MOO-的经典算法" class="headerlink" title="MOO 的经典算法"></a>MOO 的经典算法</h2><h3 id="线性加权法"><a href="#线性加权法" class="headerlink" title="线性加权法"></a>线性加权法</h3><p>根据 $f(x)$ 的重要程度，设定权重进行线性加权</p><script type="math/tex;mode=display">\begin{array}{r}
& \min _{x} \sum_{k=1}^{K} \lambda_{k} f_{k}(x) \\
\text { s.t. } & g_{i}(x) \geq 0,\quad i \in[1, M] \\
& h_{j}(x)=0,\quad j \in[1, L]
\end{array}</script><p><img src="/image/多目标优化笔记/weight.jpg" style="zoom:67%"></p><p>于是就变成了单目标优化问题，上述问题存在有效解的条件，对于给定的 $\lambda \in \Lambda^{++}$ ，则上述问题的最优解是 MOO 问题的有效解，其中</p><script type="math/tex;mode=display">\Lambda^{++}=\left\{\lambda \mid \lambda_{k}>0, k=1,2 \ldots K, \sum_{k=1}^{K} \lambda_{k}=1\right\}</script><ul><li>优点：实现简单，有成熟的算法求解</li><li>缺点：$\lambda_k$ 难以确定，求出的解的优劣无法确定</li></ul><h3 id="主要目标法"><a href="#主要目标法" class="headerlink" title="主要目标法"></a>主要目标法</h3><p>也称 $\epsilon$-约束方法</p><script type="math/tex;mode=display">\begin{array}{l}
& \min _{x} & f_{p}(x) \\
\text { s.t. } & f_{k}(x) &\leq \epsilon_{k} &,k=1, \ldots, K, k \neq p \\
& g_{i}(x) &\geq 0, i \in[1, M] \\
& h_{j}(x) &= 0, j \in[1, L]
\end{array}</script><p><img src="/image/多目标优化笔记/constraint.jpg" style="zoom:67%"></p><p>$\epsilon$-约束方法从 $K$ 个目标中选择最重要的子目标作为优化目标,其余的子目标作为约束条件。每个子目标,通过上界 $\epsilon_{K}$ 来约束</p><h4 id="主要目标法最优解和-MOO-解集的关系"><a href="#主要目标法最优解和-MOO-解集的关系" class="headerlink" title="主要目标法最优解和 MOO 解集的关系"></a>主要目标法最优解和 MOO 解集的关系</h4><ul><li><p>主要目标法最优解是 MOO 解的弱有效解</p></li><li><p>若主要目标 $f_p (x)$ 是严格凸函数，可行域为 $\hat{D}$ 的凸集，则主要目标法最优解是 MOO 解的有效解</p></li></ul><h4 id="界限值-epsilon-的选取"><a href="#界限值-epsilon-的选取" class="headerlink" title="界限值 $\epsilon$ 的选取"></a>界限值 $\epsilon$ 的选取</h4><p>可以取子目标函数的上限值</p><script type="math/tex;mode=display">\min \left\{f_{k} \mid f_{k}(x), k=1, \ldots, K, k \neq p\right\} \leq \epsilon_{k}</script><p>这种取法可以使得某些 $f_k(x)$ 留在可行域 $\hat{D}$ 内,并且 $\hat{D}$ 内有较多的点靠近 $f_k (x)$ 的最优解</p><ul><li>优点：简单，能应用到凸函数和非凸函数场景下</li><li>缺点：$\epsilon_k$ 如果取值不合适，可行域 $\hat{D}$ 可能为空值</li></ul><h3 id="逼近目标法"><a href="#逼近目标法" class="headerlink" title="逼近目标法"></a>逼近目标法</h3><p>提出一个目标值 $f^0 = (f_1^0,f_2^0,\cdots,f_k^0)$，使得每个目标函数 $f_k(x)$ 都逼近对应的目标值</p><script type="math/tex;mode=display">\begin{array}{l}
L = (f(x), f^0) & = ||f(x) - f^0 ||^{\lambda}_{2} \\
&= \sum_{k=1}^{K} \lambda_k(f_k(x)-f^0)^2,\lambda \in \Lambda^{++}
\end{array}</script><p>和机器学习中的损失函数类似，是一个单目标优化问题，可以通过经典的方法进行求解，这里求解的最优解和有效解及弱有效解没有直接的联系，反映了决策者希望的目标值</p><h3 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h3><p>这是一种直接优化的方法，而上面提到的算法都是采取先验的知识将多目标优化转化成单目标优化</p><h4 id="最速梯度下降"><a href="#最速梯度下降" class="headerlink" title="最速梯度下降"></a>最速梯度下降</h4><p>简单起见，将讨论问题限制在无约束的单目标优化问题，并要求无约束的单目标优化问题中的 $f (x)$ 具有一阶连续偏导数，对于这类问题，能够从某一点出发，选择目标函数 $f (x) $ 下降最快的方向进行搜索，尽快达到最小值，问题是如何选择下降最快的方向</p><script type="math/tex;mode=display">DF(x;d) = \nabla f(x)^T d</script><p>求 $f(x)$ 在点 $x$ 处的下降最快的方向导数，归结为求如下最优化问题</p><script type="math/tex;mode=display">\min \nabla f(x)^T d \\
\text{s.t.} \quad ||d|| \leq 1</script><p>其中 $||\cdot||$ 为欧式距离，上述问题的解为</p><script type="math/tex;mode=display">d = -\frac{\nabla f(x)}{||\nabla f(x)||}</script><p>负梯度方向为最速下降方向，最速下降法的迭代公式为</p><script type="math/tex;mode=display">x^{t+1} = x^t + \lambda_k d^{(k)}</script><p>其中，$\lambda _k$ 可以通过一维搜索来得到</p><h4 id="多目标梯度下降算法"><a href="#多目标梯度下降算法" class="headerlink" title="多目标梯度下降算法"></a>多目标梯度下降算法</h4><p>设当前为 $t+1$ 轮迭代，梯度迭代公式</p><script type="math/tex;mode=display">x^{t+1} = x^t + \lambda \cdot d^{t}</script><p>多目标优化的方向导数</p><script type="math/tex;mode=display">\nabla f_k(x)^T d,k = 1,2,\cdots,K</script><p>定义最大方向导数</p><script type="math/tex;mode=display">M_x(d^t) = max\{ \nabla f_k(x)^T d^t \mid k = 1,2,\cdots,K\}</script><p>多目标问题的最速梯度下降方向，可以归结为求解以下问题</p><script type="math/tex;mode=display">\begin{array}{l}
\min & M_x(d^t) + \frac{1}{2} \| d^t \| ^2 \\
\text{s.t.} & d^t \in R
\end{array}</script><p>上述优化问题是闭且强凸优化问题，一定存在最优解，令 $M_x (d^t) = \alpha$，可以将一阶偏导项消去</p><script type="math/tex;mode=display">\begin{array}{l}
\min & \alpha + \frac{1}{2} \|d^t \|^2 \\
\text{s.t.} & \nabla f_k(x)^T d^t \leq \alpha ,k = 1,2,\cdots,K \\
& d^t \in R
\end{array}</script><p>上述问题为带线性不等式约束的凸二次规划问题</p><p>令 $d^\ast$，$a^\ast$ 为上述优化问题的最优解，得到</p><ul><li>若 $x^\ast$ 为 Pareto 最优，则 $d^\ast=0,a^\ast=0$</li><li>若 $x^\ast$ 不为 Pareto 最优，则 $a^\ast&lt;0$</li></ul><p>且</p><script type="math/tex;mode=display">\alpha  \leq-\frac{1}{2} \|\left. d^{t}\right|^{2}<0 \\
\nabla f_{k}(x)^{T} d^{t}  \leq \alpha, k=1, \cdots, K</script><p>因此</p><ul><li>如果 $d^\ast = 0$，则说明此时不存在下降方向,使得所有的目标都下降</li><li>如果 $d^\ast \neq 0$，则有 $\nabla f_{k}(x)^{T} d^{t} &lt; 0$，则 $d^t$ 是一个有效的多目标搜索方向，按如下公式更新，即可以使目标函数下降</li></ul><script type="math/tex;mode=display">\begin{array}{l}
x^{(t+1)}&=& x^{t}+\lambda \cdot d^{t} \\
f_{k}\left(x^{(t+1)}\right) &\leq& f_{k}\left(x^{t}\right), k=1, \ldots, K
\end{array}</script><h2 id="多任务学习（MTL）"><a href="#多任务学习（MTL）" class="headerlink" title="多任务学习（MTL）"></a>多任务学习（MTL）</h2><p>多任务学习（MTL）- Multi-Task Learning</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在同一时间学习多个任务，求得最优解</p><p>设有 $N$ 个样本点 $\{ x,y_i^1,y_i^2,\cdots, y_i^T\},i \in N$，其中 $T$ 为任务数量，$y_i^t$ 是第 $t^{th}$ 个任务，第 $i^{th}$ 个样本点标签，定义为</p><script type="math/tex;mode=display">f^t(x;\theta^{sh},\theta^t):X \rightarrow Y^t</script><p>其中 $\theta^{sh}$ 为多个任务的共享参数，$\theta^t$ 为单个任务的独有参数</p><p>$Loss$ 为</p><script type="math/tex;mode=display">L^t(\_,\_) = Y^t \times Y^t \rightarrow R^+</script><script type="math/tex;mode=display">\min_{\theta^{sh} ,\theta} = \sum_{t=1}^{T} c^t \hat{L}^{t}(\theta^{sh},\theta)</script><p>$c^t$ 为每个具体任务的权重，每个具体任务 $t$ 的 $Loss$ 为</p><script type="math/tex;mode=display">\hat{L}^t(\theta^{sh},\theta) \triangleq \frac{1}{N} \sum_t L(f^t(x;\theta^{sh}, \theta^t), y^t_i)</script><h3 id="多任务学习转化为多目标优化"><a href="#多任务学习转化为多目标优化" class="headerlink" title="多任务学习转化为多目标优化"></a>多任务学习转化为多目标优化</h3><p>将多任务学习转换为 MOO 问题求解，定义</p><script type="math/tex;mode=display">\min _{\theta^{s h}, \theta} L\left(\theta^{s h}, \theta^{1}, \ldots, \theta^{T}\right)=\min _{\theta^{s h}, \theta}\left(\hat{L}^{1}\left(\theta^{s h}, \theta^{1}\right), \cdots, \hat{L}^{T}\left(\theta^{s h}, \theta^{T}\right)\right)</script><p>多目标优化的目的是求得 Pareto 最优解，多目标优化的 Pareto 最优解定义</p><p>一个解 $\theta$ 支配另一个解 $\bar{\theta}$ ，如果</p><script type="math/tex;mode=display">\hat{L}(\theta^{sh},\theta^t) \leq \hat{L}(\bar{\theta}^{sh},\bar{\theta}^t)</script><p>对于所有的任务 $t$ 都成立，且</p><script type="math/tex;mode=display">L(\theta^{sh},\theta^1,\theta^2,\cdots,\theta^{T}) \neq L(\bar{\theta}^{sh},\bar{\theta}^1,\bar{\theta}^2,\cdots, \bar{\theta}^T)</script><p>一个解 $\theta^{\ast}$ 称为 Pareto 最优解的集合称为 Pareto 最优解集，其图像称为 Pareto 前沿（Pareto Front）</p><h2 id="多任务求解：单个-Pareto-解"><a href="#多任务求解：单个-Pareto-解" class="headerlink" title="多任务求解：单个 Pareto 解"></a>多任务求解：单个 Pareto 解</h2><h3 id="问题转化"><a href="#问题转化" class="headerlink" title="问题转化"></a>问题转化</h3><p>单个 Pareto 解使用了多重梯度下降法，由多目标优化的 $KKT$ 条件，得</p><p>存在 $\alpha^1,\alpha^2,\cdots,\alpha^T \geq 0$，使得</p><script type="math/tex;mode=display">\begin{array}{r}
\sum_{t=1}^{T} \alpha^T = 1 \\
\sum_{t=1}^{T} \alpha^t \nabla_{\theta^{sh}} \hat{L}(\theta^{sh},\theta^{t}) = 0
\end{array}</script><p>对应所有的任务 $t$</p><script type="math/tex;mode=display">\nabla_{\theta^t}\hat{L}(\theta^{sh},\theta^t) = 0</script><p>满足上式的解称为 Pareto 平衡点（Pareto Stationary Point），Pareto 最优点都是 Pareto 平稳点，反之不一定成立，考虑如下的优化问题</p><script type="math/tex;mode=display">\begin{gathered}
\min _{\alpha^{1}, \cdots ,\alpha^{T}}\left\|\sum_{t=1}^{T} \alpha^{t} \nabla_{\theta s h} \hat{L}^{t}\left(\theta^{s h}, \theta^{t}\right)\right\| \\
\sum_{t=1}^{T} \alpha^{t}=1, \alpha^{t} \geq 0, \forall t
\end{gathered}</script><p>上述优化问题的解存在两种情况</p><ul><li>最优值 $=0$，则对应的解满足 $KKT$ 条件</li><li>最优值 $\neq 0$，则对应的解给出了下降方向，使得多任务目标函数提升（函数值下降）上述优化问题等价于在输入点集凸包中找到最小模点</li></ul><h3 id="两个任务的情形"><a href="#两个任务的情形" class="headerlink" title="两个任务的情形"></a>两个任务的情形</h3><p>由多目标优化的 $KKT$ 条件，得</p><script type="math/tex;mode=display">\begin{gathered}
\min _{\alpha^{1}, \ldots \alpha^{T}}\|\gamma \theta+(1-\gamma) \bar{\theta}\| \\
\gamma+(1-\gamma)=1, \gamma \geq 0
\end{gathered}</script><p>其中 $\theta,\bar{\theta}$ 定义为</p><script type="math/tex;mode=display">\begin{aligned}
&\theta \triangleq \nabla_{\theta s h} \hat{L}^{1}\left(\theta^{s h}, \theta^{1}\right) \\
&\bar{\theta} \triangleq \nabla_{\theta s h} \hat{L}^{2}\left(\theta^{s h}, \theta^{2}\right)
\end{aligned}</script><p>其解的情况枚举如下</p><ul><li>当 $\theta^T \bar{\theta} \geq \theta^T \theta,\gamma = 1$</li><li>当 $\theta^T \bar{\theta} \geq \bar{\theta}^T \bar{\theta},\gamma = 0$</li><li>$\text{otherwise}$</li></ul><script type="math/tex;mode=display">\gamma=\frac{(\bar{\theta}-\theta)^{T} \bar{\theta}}{\| \bar{\theta}-\theta) \|_{2}^{2}}p7</script><p>几何解释</p><p><img src="/image/多目标优化笔记/interpret.png" style="zoom:66%"></p><p><img src="/image/多目标优化笔记/algo1.png" style="zoom:60%"></p><p>基于 Frank-wolfe 算法，得求解 MTL 任务算法</p><p><img src="/image/多目标优化笔记/algo2.png" style="zoom:66%"></p><p><img src="/image/多目标优化笔记/frankwolfesolver.png" style="zoom:62%"></p><h2 id="多任务求解：多个-Pareto-解"><a href="#多任务求解：多个-Pareto-解" class="headerlink" title="多任务求解：多个 Pareto 解"></a>多任务求解：多个 Pareto 解</h2><p>上一节介绍的方法只能求得一个 pareto 解，有时需要多个 pareto 解才能做出更好的决策</p><h3 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h3><p>将多任务学习分解为多个带约束的多目标子问题，通过对子问题进行并行求解</p><p>原始多任务学习定义</p><script type="math/tex;mode=display">\min_{\theta} L(\theta) = (L_1(\theta),L_2(\theta),\cdots,L_i(\theta),\cdots,L_m(\theta))</script><p>$L_i(\theta)$ 是第 $i$ 个任务的损失函数</p><p>下图所示</p><p><img src="/image/多目标优化笔记/pareto1.png" style="zoom:60%"></p><p><img src="/image/多目标优化笔记/pareto2.png" style="zoom:60%"></p><p>用一组分布良好的 Preference Vectors（PV）将多任务学习的目标空间分解为 $K$ 个子区域</p><script type="math/tex;mode=display">PV = \{u_1,u_2,\cdots,u_k,\cdots,u_K\},u_k \in R_+^m</script><p>重新定义多任务学习</p><script type="math/tex;mode=display">\min_{\theta} L(\theta) = (L_1(\theta),L_2(\theta),L_m(\theta)) \\
s.t. \quad L(\theta) \in \Omega_k,k = 1,\cdots,K</script><p>$\Omega_k$ 是目标空间的子区域</p><script type="math/tex;mode=display">\Omega_k = \{v \in R_+^m | u_j^T v \leq u_k^Tv,\forall j = 1,\cdots,K \}</script><p>$\Omega_k$ 中的元素 $v$</p><script type="math/tex;mode=display">v \in \Omega_k \Leftrightarrow u_k^T v = \| u_k \| \cdot \| v \| \cos{\alpha}</script><p>$u_k^T v$ 为最大的内积</p><p>重新定义多任务学习</p><script type="math/tex;mode=display">\begin{array}{l}
& \min_\theta L(\theta) = (L_1(\theta),L_2(\theta),L_m(\theta)) \\
\text{s.t.} & G_j(\theta_t) = (u_j - u_k)^T \\
& L(\theta_t) \leq 0 \\
& j=1,\cdots,K
\end{array}</script><p>这样得到的解集将会分布在不同的子区域 $\Omega_k$</p><h3 id="子问题的梯度下降方法"><a href="#子问题的梯度下降方法" class="headerlink" title="子问题的梯度下降方法"></a>子问题的梯度下降方法</h3><h4 id="寻找初始解-theta-r"><a href="#寻找初始解-theta-r" class="headerlink" title="寻找初始解 $\theta_r$"></a>寻找初始解 $\theta_r$</h4><p>求解多任务学习，需要找到一个满足约束的基本可行解，对于随机产生的可行解 $\theta_r$ ，一种最直接的方法就是找到初始可行解 $\theta_0$</p><script type="math/tex;mode=display">\begin{array}{l}
& \min_{\theta_0} \| \theta_0 - \theta_r \|^2 \\
 \text{s.t.} & L(\theta_0) \in \Omega_k
\end{array}</script><p>上述问题投影方法求解的效率不高，特别是对于大规模的深度神经网络，改写为无约束优化问题，使用序列梯度方法找到初始解 $\theta_0$</p><p>定义活跃限制集合（activated constraints）</p><script type="math/tex;mode=display">I(\theta_r) = \{ j|G_j(\theta_r) \geq 0,j=1,\cdots,K \}</script><p>令 $I(\theta_r)$ 中所有的活跃限制函数值下降的方向</p><script type="math/tex;mode=display">(d_r,\alpha_r) = \text{argmin}_{d \in R^n,\alpha \in R} \quad \alpha + \frac{1}{2} \|d\|^2 \\
\text{s.t.} \quad \nabla G_j (\theta_r)^T d \leq \alpha,j\in I(\theta_r)</script><p>得到更新公式</p><script type="math/tex;mode=display">\theta_{r_{t+1}} = \theta_{r_t} + \eta_r d_{r_t}</script><p>上述方法能将活跃集内的约束目标值减少，使得越来越多的约束目标小于 0，$I(\theta_{r})$ 最后变为空集，则 $\theta_{r}$ 是可行解</p><h4 id="求解子问题"><a href="#求解子问题" class="headerlink" title="求解子问题"></a>求解子问题</h4><p>受限 pareto 最优：$\theta^{\ast}$ 是多任务 $L(\theta)$ 在子区域 $\Omega_k$ 的最优解，如果 $\theta^{\ast} \in \Omega_k$ 且不存在 $\hat{\theta} \in \Omega_k$，使得 $\hat{\theta} &lt; \theta^{\ast}$</p><p>考虑如下多目标优化问题</p><script type="math/tex;mode=display">\begin{array}{l}
& (d_t,\alpha_t)  = \text{argmin}_{d\in R^n,\alpha \in R} \quad \alpha + \frac{1}{2}\|d\|^2 \\

\text{s.t.} & \nabla L_i(\theta_t)^Td \leq \alpha,i=1,\cdots,m\\
& \nabla G_j(\theta_t)^Td \leq \alpha,j \in I_{\in}(\theta_t)
\end{array}</script><p>其中 $I_{\in}(\theta_t)$ 定义</p><script type="math/tex;mode=display">I_{\in} (\theta_t) = \{j \in I | G_j (\theta) \geq - \epsilon \}</script><p>令 $(d^k,a^k)$ 是多任务学习问题的解，则</p><ul><li><p>如果 $\theta_t$ 是严格受限于 $\Omega_k$，则 $d_t = 0 \in R^n$ 且 $\alpha_t = 0$</p></li><li><p>如果 $\theta_t$ 不是严格受限于 $\Omega_k$，则</p><script type="math/tex;mode=display">\begin{array}{l}
\alpha_{t} \leq-\frac{1}{2}\left\|d_{t}\right\|^{2}<0 \\
\nabla L_{i}\left(\theta_{t}\right)^{T} d \leq \alpha,& i=1, \ldots, m \\
\nabla G_{j}\left(\theta_{t}\right)^{T} d \leq \alpha,& j \in I_{\epsilon}\left(\theta_{t}\right)
\end{array}</script><p>迭代公式</p><script type="math/tex;mode=display">\theta_{t+1} = \theta_t + \eta d_t</script></li></ul><p>通过求解上述问题，能够获得一个有效的搜索方向</p><h4 id="大规模求解方法"><a href="#大规模求解方法" class="headerlink" title="大规模求解方法"></a>大规模求解方法</h4><p>上述方法能够获得一个有效的搜索方向，对于大规模的问题，会比较困难，这里将问题重写，将其表示为对偶形式</p><ul><li><p>$KKT$ 条件</p><script type="math/tex;mode=display">\begin{array}{l}
& d_{t} =-\sum_{i=1}^{m} \lambda_{i} \nabla L_{i}\left(\theta_{t}\right)+\sum_{j \in I_{\epsilon}(\theta)} \beta_{i} \nabla G_{j}\left(\theta_{t}\right) \\
\text{s.t.} & \sum_{i=1}^{m} \lambda_{i}+\sum_{j \in I_{\epsilon}(\theta)} \beta_{j}=1
\end{array}</script></li><li><p>对偶问题</p><script type="math/tex;mode=display">\begin{array}{l}
& \max_{\lambda_{i}, \beta_{j}}-\frac{1}{2}\left\|\sum_{i=1}^{m} \lambda_{i} \nabla L_{i}\left(\theta_{t}\right)+\sum_{j \in I_{\epsilon}(\theta)} \beta_{i} \nabla G_{j}\left(\theta_{t}\right)\right\|^{2} \\
\text { s.t. } & \sum_{i=1}^{m} \lambda_{i}+\sum_{j \in I_{\epsilon}(\theta)} \beta_{j}=1 \\ 
& \lambda_{i} \geq 0, \beta_{j} \geq 0 \\
& \forall i=1, \cdots, m\\
& \forall j \in I_{\epsilon}(\theta)
\end{array}</script></li></ul><p>将多任务学习转化为其对偶问题后，求解空间不再是参数空间，而是变成了任务个数和受限条件数，使得求解问题极大的减少了</p><p>Pareto MTL 算法如下图</p><p><img src="/image/多目标优化笔记/pareto_MTL.png" style="zoom:50%"></p><p>为表述方便，这里引用论文中关于多任务学习的定义</p><p>设 $f(x)$ 表面光滑</p><script type="math/tex;mode=display">f(x):\mathcal{R}^n \rightarrow \mathcal{R}^m \\
f_i(x):\mathcal{R}^n \rightarrow \mathcal{R},i=1,\cdots,m</script><h3 id="准备：-Krylov-子空间"><a href="#准备：-Krylov-子空间" class="headerlink" title="准备：$Krylov$ 子空间"></a>准备：$Krylov$ 子空间</h3><p>大规模稀疏线性方程组 $AX=b$ 求解的首先方法是 $krylov$ 子空间方法，基本思想是在一个较小的子空间 $\mathcal{K} \subset R_n$ 中寻找近似解</p><p>定义：设 $A \in R^{n \times n},r \in R^n$，则</p><script type="math/tex;mode=display">\mathcal{K}_{m}(A, r) \triangleq \text{span}\{r, Ar, \cdots, A^{m-1} r\} \subseteq R_{n}</script><p>是由 $A$ 和 $r$ 生成的 $Krylov$ 子空间，通常简记为 $\mathcal{K}_m$，$Krylov$ 子空间有如下的三个性质</p><ul><li>$Krylov$ 子空间嵌套性：$\mathcal{K}_{1} \subseteq \mathcal{K}_{2} \subseteq \cdots \subseteq \mathcal{K}_{m}$</li><li>$\mathcal{K}_m$ 的维数不超过 $m$</li><li>$\mathcal{K}_{m}(A, r)=\{x=p(A)\}$，$r$ 为次数小于 $m$ 的多项式</li></ul><p>求解 $Krylov$ 子空间的解来近似原始线性方程组的解</p><p><img src="/image/多目标优化笔记/krylov.png" style="zoom:50%"></p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li><p>Pareto 平稳点（Pareto Stationary）：设 $f_i(x)$ 连续可身微，点 $x$ 称为 Pareto 平稳点，如果存在 $\alpha \in \mathcal{R}^{m},a_i \geq 0$ 使得下式成立</p><script type="math/tex;mode=display">\sum_{i=1}^{m} \alpha_i \nabla f_i(x) = 0 \\
\sum_{i=1}^{m} \alpha_i = 1</script></li><li><p>Pareto 点都是 Pareto 平稳点</p></li><li><p>设 $f(x)$ 是光滑且 $x^{\ast}$ 是 Pareto 点，$x(t)$ 是过点 $x^{\ast}$ 的曲线</p><script type="math/tex;mode=display">x(t):t\in (-\epsilon,\epsilon) \rightarrow \mathcal{R}^n \\
x(0) = x^{\ast}</script><p>则存在 $\beta \in \mathcal{R}^m$ 使得</p><script type="math/tex;mode=display">H(x^{\ast})x'(t) = \nabla f(x^{\ast})^T \beta \\
H(x^{\ast}) = \sum_{i=1}^{m} \alpha_i \nabla^2 f_i(x^{\ast})</script><p>$x’(t)$ 为切线</p></li></ul><p>上式表明,算子 $H(x^{\ast})$ 将点 $x$ 处的切向量 $v = x’(t)$ 变换为由 $∇f_i(x^{\ast})$ 扩张成的 $Krylov$ 子空间的向量</p><p><img src="/image/多目标优化笔记/krylov_expand.png" style="zoom:70%"></p><h3 id="离散-Pareto-求解"><a href="#离散-Pareto-求解" class="headerlink" title="离散 Pareto 求解"></a>离散 Pareto 求解</h3><p>给定初始点 $x_0 \in \mathcal{R}^n,f_i(x)$ 光滑，可以从如下三步来获取连续 Pareto 解</p><ul><li>求解 Pareto 平稳点: 从初始点 $x_0$ 出发，通过梯度下降的方法求解 Pareto 平稳点 $x_0^{\ast}$</li><li>扩展 Pareto 平稳点，$f(x)$ 在点 $x_0^{\ast}$ 处光滑，如果 Pareto 前沿存在，则在点 $x_0^{\ast}$ 处的某个领域内存在 Pareto 平稳点，由此出发，可以求得一系列的 Pareto 平稳点 $x_i^{\ast}$</li><li>将已知的平稳点所在的局部 Pareto 前沿进行连接合并，扩充成更大的连续Pareto 前沿</li></ul><p><img src="/image/多目标优化笔记/pareto_front.png" style="zoom:50%"></p><p>接下来讨论获取 Pareto 平稳点的方法</p><h3 id="梯度求解法"><a href="#梯度求解法" class="headerlink" title="梯度求解法"></a>梯度求解法</h3><p>见上述</p><h3 id="一阶梯度求解法扩张"><a href="#一阶梯度求解法扩张" class="headerlink" title="一阶梯度求解法扩张"></a>一阶梯度求解法扩张</h3><p>通过梯度求解法求解出 Pareto 平稳点 $x_0^{\ast}$ 后，基于该点扩展出局部 Pareto 集 $\{x_i \}$，这一过程分解为两步</p><ul><li>计算权重 $\alpha$</li><li>求解搜索方向 $v$，估计梯度迭代的搜索方向 $v_i$</li></ul><p>通过如下更新公式求解</p><script type="math/tex;mode=display">x_i = x_0^{\ast} + sv_i</script><p>计算 $\alpha$ 可以归结为求解如下的约束问题</p><script type="math/tex;mode=display">\min_\alpha \| \sum_{i=1}^m \alpha_i \nabla f_i(x_0^{\ast}) \|^2 \\
\text{s.t.} \quad \alpha_i \geq 0,\sum_{i=1}^m \alpha_i = 1</script><p>上述问题规模为 $m$， 量级较小，可以很方便的求解出来</p><p>再由基本概念，得求解的线性方程组</p><script type="math/tex;mode=display">H(x_0^{\ast})v = \nabla f(x_0^{\ast})^T \beta</script><p>上述问题求解有两个难点</p><ul><li>$x_0^{\ast}$ 不一定为 Pareto 平稳点</li><li>当 $n$ 非常大时，求解起来非常困难</li></ul><p>为此引入校正向量 $c$（correction vector），约束问题改写为</p><script type="math/tex;mode=display">\begin{array}{l}
&\min_{a,c} \| c \|^2 \\
\text{s.t.} & \alpha_i \geq 0 \\
& \sum_{i=1}^m \alpha_i = 1 \\
& \sum_{i=1}^{m} \alpha_i(\nabla f_i(x_0^{\ast})-c)=0
\end{array}</script><p>用 $\nabla f_i(x_0^{\ast})-c$ 近似 $\nabla f_i(x_0^{\ast})$，$x_0^{\ast}$ 将会是 Pareto 平稳点</p><p>设 $a^{\ast}$ 是未引入校正向量约束问题的解，则引入 $c$ 后的解为</p><script type="math/tex;mode=display">(a,c) = (a^{\ast},\nabla f(x_0^{\ast})^T a^{\ast})</script><p>在计算出 $a^{\ast},x_0^{\ast},c$ 后，可以计算出 $\nabla f(x_0^{\ast})$，考虑如下稀疏线性方程组</p><script type="math/tex;mode=display">H(x_0^{\ast})v = (\nabla f(x_0^{\ast})^T - c^T) \beta</script><p>$\beta$ 为随机生成的向量，$v$ 为待求解的变量。上述式可以通过 $krylov$ 子空间，$MINERS$ 方法求解</p><p>寻找离散 Pareto 解集合的求解算法</p><ul><li>Input：随机初始化网络</li><li>ParetoExpand($x^{\ast}$) 生成点 $x^{\ast}$ 的 $K$ 个搜索方向 $v_i$</li><li>由 $K$ 个搜索方向扩展出 $K$ 个子网络</li><li>更新子网络节点 $x_i = x^{\ast} + sv_i$</li><li>ParetoExpand($x_i$) 输出 Pareto 平稳点 $x_i^{\ast}$</li><li>Output：$N$ 个 Pareto 平稳网络</li></ul><p><img src="/image/多目标优化笔记/efficient_Pareto.png" style="zoom:60%"></p><h3 id="连续-Pareto-解（Pareto-front）构建"><a href="#连续-Pareto-解（Pareto-front）构建" class="headerlink" title="连续 Pareto 解（Pareto front）构建"></a>连续 Pareto 解（Pareto front）构建</h3><p>通过前面求解出来 $N$ 个 Pareto 平稳网络（父节点及 $K$ 个子网络），由离散 Pareto 点合并成更大的连续 Pareto 前沿</p><p>给定 $x_i^{\ast}$ 及基其对应的 $K$ 个节点 $\{ {x_i^{\ast}}_1,\cdots,{x_i^{\ast}}_k \}$，定义连续变量 $r_{i \rightarrow {i}_j} \in [0,1]$ 以及搜索方向</p><script type="math/tex;mode=display">v_{i \rightarrow {i}_j} = {x_i^{\ast}}_j - x_i^{\ast},j = 1,2,\cdots,K</script><p>$x_i^{\ast}$ 处局部 Pareto 集可以通过下式进行构建</p><script type="math/tex;mode=display">S(x_i^{\ast}) = \{x_i^{\ast} + \sum_{i=1}^K r_{i \rightarrow {i}_j} u_{i \rightarrow {i}_j} | r_{i \rightarrow {i}_j} \geq 0,\sum_{i=1}^K r_{i \rightarrow {i}_j} \leq 1 \}</script><p>$S(x_i^{\ast})$ 是点 $x_i^{\ast}$ 及对应的 $K$ 个子节点 $\{ {x_i^{\ast}}_1,\cdots,{x_i^{\ast}}_K \}$ 构成的凸包，切平面中切向量的线性组合仍然在切平面</p><p>对于 $N$ 个局部 Pareto 集</p><script type="math/tex;mode=display">\{ S(x_1^{\ast}),\cdots,S(x_N^{\ast}) \}</script><p>可以将两两接壤处合并成一个更大的局部 Pareto 集合，全部合并完后，可以生成多个的连续 Pareto 前沿</p><p><img src="/image/多目标优化笔记/pareto_continuous.png" style="zoom:67%"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li><a target="_blank" rel="noopener" href="https://engineering.purdue.edu/~sudhoff/ee630/Lecture09.pdf">https://engineering.purdue.edu/~sudhoff/ee630/Lecture09.pdf</a></li><li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/352461938">https://zhuanlan.zhihu.com/p/352461938</a></li><li><a target="_blank" rel="noopener" href="https://hpzhao.github.io/2018/09/17/多目标优化四种方法/">https://hpzhao.github.io/2018/09/17/多目标优化四种方法/</a></li><li><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23311674">https://www.zhihu.com/question/23311674</a></li><li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions</a></li></ul></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96/" rel="tag">多目标优化</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%9C%80%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" rel="tag">最优化方法</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2022/04/25/%E7%AC%AC%E4%BA%8C%E5%B1%8A%E7%BD%91%E5%88%83%E6%9D%AFmisc%E6%89%80%E8%A7%81%E9%9D%9E%E6%89%80%E8%A7%81wp/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">第二届网刃杯misc所见非所见wp</div></a><a href="/2022/04/16/%E4%BD%BF%E7%94%A8%E5%B9%B3%E8%A1%8C%E7%BA%BF%E6%AE%B5%E7%9A%84%E7%9B%B8%E6%9C%BA%E6%A0%87%E5%AE%9A/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">使用平行线段的相机标定[计划更新]</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">访客数:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>文章访问量:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li>Aeeeeeep Blog &copy; 2023</li><li><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/images/备案图标.png" style="float:left"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备 32090202001024号</p></a></li></ul><div class="float-left"><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span><script>var now=new Date;function createtime(){var n=new Date("11/11/2021 00:08:39");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="本站已运行 "+dnum+" 天 ",document.getElementById("times").innerHTML=hnum+" 小时 "+mnum+" 分 "+snum+" 秒"}setInterval("createtime()",250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop"><i class="fe fe-rocket"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>