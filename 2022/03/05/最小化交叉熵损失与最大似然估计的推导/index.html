<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><title>最小化交叉熵损失与最大似然估计的推导 | Aeeeeeep Blog | The Gleaners</title><link rel="shortcut icon" href="/images/favicon64.ico"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css"><script src="/js/pace.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><main class="content"><section class="outer"><article id="post-最小化交叉熵损失与最大似然估计的推导" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal><div class="article-inner"><header class="article-header"><h1 class="article-title" itemprop="name">最小化交叉熵损失与最大似然估计的推导</h1></header><div class="article-meta"><a href="/2022/03/05/%E6%9C%80%E5%B0%8F%E5%8C%96%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E4%B8%8E%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E7%9A%84%E6%8E%A8%E5%AF%BC/" class="article-date"><time datetime="2022-03-05T13:10:51.000Z" itemprop="datePublished">2022-03-05</time></a><div class="article-category"><a class="article-category-link" href="/categories/%E6%87%B5%E9%80%BC%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">懵逼的深度学习</a></div></div><div class="tocbot"></div><div class="article-entry" itemprop="articleBody"><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>机器学习所使用的交叉熵损失函数与信息论里的交叉熵的推导与思考</p><span id="more"></span><h2 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h2><h3 id="信息论中的交叉熵定义"><a href="#信息论中的交叉熵定义" class="headerlink" title="信息论中的交叉熵定义"></a>信息论中的交叉熵定义</h3><script type="math/tex;mode=display">H(p,q) = H(p) + D_{KL}(p||q)</script><p>在 $ p,q $ 是离散分布时， 上式等价为</p><script type="math/tex;mode=display">H(p,q) = -\sum^K_{i=1} p(x_i) \log q(x_i)</script><p>其中， $x_i$ 是 $p,q$ 分布共同样本空间的同一个样本点，样本空间的大小为 $K$</p><h3 id="机器学习中的交叉熵定义"><a href="#机器学习中的交叉熵定义" class="headerlink" title="机器学习中的交叉熵定义"></a>机器学习中的交叉熵定义</h3><p>机器学习进行优化时， 会把所有样本的交叉熵值求平均，假设有 $N$ 个样本</p><script type="math/tex;mode=display">J(w) = \frac{1}{N} \sum^N_{n=1} H(p_n,q_n)</script><p>而信息论中的交叉熵仅仅是针对一个样本</p><p>因为交叉熵常用于解决分类问题，而分类问题的概率本质是计算类别变量的广义的伯努利分布，所以机器学习采用的是交叉熵的离散形式</p><script type="math/tex;mode=display">CE = -\sum^K_{i=1} t_i \log s_i</script><p>其中，$t_i$ 是期望的类别标签，$s_i$ 是模型对第 $i$ 个类别计算得到的 $score$ ，通常在计算损失之前，会用激活函数对 $score$ 加以转换，用 $f(s_i)$ 替代上式的 $s_i$</p><p>得到机器学习的交叉熵损失函数</p><script type="math/tex;mode=display">J(w) = - \frac{1}{N} \sum^N_{n=1} \sum^K_{i=1} t_i \log s_i</script><p>因为对于分类问题，假设模型的输出层上只有2个输出结点，而且是一个二分类单标签问题，如果输出层用符号 $Y$ 表示，那么 $Y$ 服从 $0-1$ 分布(是二项分布的特例，或称伯努利分布，二元分布)，即随机变量 $Y$ 的样本空间有两个样本点(分别对应输出层的两个输出结点)，每个样本点就是一个类别。我们希望机器学习模型训练出的分布是某个类别的概率为 $1$ ，另一个类别的概率为 $0$ 。推广到多分类单标签问题，那么 $Y$ 服从广义的伯努利分布(是多项式分布的特例，或称 $Category$ 分布，范畴分布，类别分布，$Multinoulli$ 分布(2012年在《Machine Learning - A Probabilistic Perspective》中正式提出))。</p><h3 id="最小化交叉熵损失与最大似然推导"><a href="#最小化交叉熵损失与最大似然推导" class="headerlink" title="最小化交叉熵损失与最大似然推导"></a>最小化交叉熵损失与最大似然推导</h3><p>先从一个直观的例子感受最小化交叉熵损失与最大似然的关系</p><script type="math/tex;mode=display">J(w)=-\frac{1}{N} \sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>去掉 $\frac{1}{N}$ 并不影响函数的单调性，机器学习任务的也可以是最小化下面的交叉熵损失</p><script type="math/tex;mode=display">J(w)=-\sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>等价于最大化</p><script type="math/tex;mode=display">J(w)=\sum_{n=1}^{N}\left[y_{n} \log \hat{y}_{n}+\left(1-y_{n}\right) \log \left(1-\hat{y}_{n}\right)\right]</script><p>这其实就是对伯努利分布求最大似然中的对数似然函数</p><h3 id="伯努利分布的最大似然推导"><a href="#伯努利分布的最大似然推导" class="headerlink" title="伯努利分布的最大似然推导"></a>伯努利分布的最大似然推导</h3><p>有二元随机变量 $Y \in{0,1}$ ，设 $p(Y=1) = \beta$ ，那么它的概率质量函数(PMF)为</p><script type="math/tex;mode=display">P(Y \mid \beta)=\beta^{Y}(1-\beta)^{1-Y}</script><p>现有 $D=\left{y<em>{1}, y</em>{2}, \ldots, y_{N}\right} $ 来自 $Y$ ，样本容量为 $N$ 的一个样本，似然函数为</p><script type="math/tex;mode=display">P(D \mid \beta)=\prod_{i=1}^{N} P\left(Y=y_{i} \mid \beta\right)=\prod_{i=1}^{N} \beta^{y_{i}}(1-\beta)^{1-y_{i}}</script><p>在机器学习中，对 $\beta$ 的定义为</p><script type="math/tex;mode=display">\beta = p_{\theta}(Y = 1|x_i)</script><p>其中，$X=\left{x<em>{1}, \ldots, x</em>{N}\right}$ ，$ x_{i} \in X$，$X$ 是 $D$ 中每个样本点对应类别的特征的集合。即给定模型参数 $\theta$ 和随机变量的样本点 $Y=1$ 的属性特征 $x_i$ ( $x_i$ 可以是一个向量)，让模型估计出事件 $Y=1$ 的概率(同时也是当前伯努利分布的参数)</p><p>故上述的似然函数的参数不再是伯努利分布的 $\beta$，而是模型的参数 $\theta$，有</p><script type="math/tex;mode=display">P(D \mid \theta, X)=\prod_{i=1}^{N} p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}}</script><p>易得对数似然函数</p><script type="math/tex;mode=display">\begin{aligned}
\mathcal{L}(\theta ; X, D) &= \log \prod_{i=1}^{N} p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\
&= \sum_{i=1}^{N} \log p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}\left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\
&= \sum_{i=1}^{N} \log p_{\theta}\left(Y=1 \mid x_{i}\right)^{y_{i}}+\log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)^{1-y_{i}} \\
&= \sum_{i=1}^{N} y_{i} \log p_{\theta}\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right)
\end{aligned}</script><p>以下给出最大似然估计与最小化交叉熵损失的转化过程，意在说明在伯努利分布下，最大似然估计与最小化交叉熵损失是同概念的</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{p} &= \arg \max _{\theta} \sum_{i=1}^{N} y_{i} \log p_{\theta}\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{\theta}\left(Y=1 \mid x_{i}\right)\right) \\
&= \arg \max_{\theta} \sum^N_{i=1}y_i \log \hat{y_i} + (1-y_i) \log (1- \hat{y_i}) \\
&= \arg \min_{\theta} - \sum^N_{i=1}y_i \log \hat{y_i} + (1-y_i) \log (1- \hat{y_i}) \\
&= \arg \min_{\theta} \sum^N_{i=1} H(y_i,\hat{y_i})
\end{aligned}</script><h3 id="广义伯努利分布的最大似然推导"><a href="#广义伯努利分布的最大似然推导" class="headerlink" title="广义伯努利分布的最大似然推导"></a>广义伯努利分布的最大似然推导</h3><p>单标签多分类任务的类别随机变量只服从多项式分布中试验次数为1的情况，广义伯努利分布（ $Category$ 分布）对应的是更常见的单标签多分类任务，以下讨论伯努利分布到广义伯努利分布的过渡以及与最大似然估计的关系</p><p>有 $K$ 元类别随机变量 $Y \in{1, \ldots, K} $ ，且 $p(Y=J)=\beta_{j}$，概率质量函数库为</p><script type="math/tex;mode=display">P(Y|B)=\prod^K_{i=1} \beta ^{I(Y=i)}_i</script><p>如果 $Y=i$，$I(Y=i) = 1$ ，否则， $I(Y=i)=0$</p><blockquote><p>这个概率质量函数之所以看上去有点奇怪，是因为它出现了 $Identity$ 函数。而符合我们直觉的是维基百科对广义伯努利分布的另一个PMF定义，即 $p(Y=i)=p_i$ ，即直接根据列联表，获得该事件发生的概率。如果想解释上面晦涩难懂的包含 $Identity$ 函数的PMF，可以用严格按照伯努利试验的描述来解释：在进行1次试验中抽到第 $j$ 个类别，其他概率被抽中0次的概率，即</p><script type="math/tex;mode=display">P(Y=j)=(1,1) \ldots p_{j-1}^{0} p_{j}^{1} p_{j+1}^{0} \ldots</script></blockquote><p>$D=\left{y<em>{1}, y</em>{2}, \ldots, y_{N}\right}$ 是来自 $Y$ 的，样本容量为 $N$，的一个样本，那么似然函数为</p><script type="math/tex;mode=display">P(D \mid \beta)=\prod_{i=1}^{N} \prod_{j=1}^{K} \beta_{j}^{I\left(y_{i}=j\right)}</script><p>同样地，在机器学习模型中，对上述关于对 $\beta$ 的定义做出转变</p><script type="math/tex;mode=display">\beta_{j}=p_{\theta}\left(Y=j \mid x_{i}\right)</script><p>$X=\left{x<em>{1}, \cdots, x</em>{N}\right}$，$x_i \in X$，$X$ 是 $D$ 中的每个样本点对应特征的集合</p><p>同理上述的似然函数的参数不再是广义伯努利分布的 $\beta_1,beta_2,\cdots,\beta_k$，而是模型的参数 $\theta$，所以似然函数有</p><script type="math/tex;mode=display">P(D \mid \theta, X)=\prod_{i=1}^{N} \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)}</script><p>易得对数似然函数</p><script type="math/tex;mode=display">\begin{aligned}
\mathcal{L}(\theta ; X, D) &= \log \prod_{i=1}^{N} \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)}\\
&= \sum_{i=1}^{N} \log \prod_{j=1}^{K} p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{K} \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)^{I\left(y_{i}=j\right)} \\
&= \sum_{i=1}^{N} \sum_{j=1}^{K} I\left(y_{i}=j\right) \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)
\end{aligned}</script><p>因为最大化上述式子具有约束条件 $\sum^K_{i=1} \beta_i = 1$，所以最大化上面的对数似然函数是一个条件极值问题，使用拉格朗日乘数法进行求解，得到下面的关于求解广义伯努利分布下的交叉熵的拉格朗日函数</p><script type="math/tex;mode=display">\tilde{\mathcal{L}}(\theta ; X, D, \lambda)=\sum_{i=1}^{N} \sum_{j=1}^{K} I\left(y_{i}=j\right) \log p_{\theta}\left(y_{i}=j \mid x_{i}\right)+\lambda\left(1-\sum_{k} \beta_{i}\right)</script><p>在没有使用机器学习模型的前提下，我们只需对分布的参数和 $\lambda$ 求偏导就能得到参数的估计值</p><p>用模型的参数记号 $\hat{\beta<em>j}$ 表示 $p</em>{\theta} (y_i = j | x_i)$，用 one-hot 向量表示 $Identity$ 函数值</p><p>得</p><script type="math/tex;mode=display">\mathcal{L}(\theta ; X, D)=\sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j}</script><p>下面同样给出极大似然估计与最小化广义伯努利分布的交叉熵损失函数的转化过程，意在说明在广义伯努利分布下，最大似然估计与最小化交叉熵损失也是同概念的</p><script type="math/tex;mode=display">\begin{aligned}
\theta_{p} &= \arg \max _{\theta} \sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j} \\
&= \arg \min _{\theta}-\sum_{i=1}^{N} \sum_{j=1}^{K} y_{j} \log \hat{\beta}_{j} \\
&= \arg \min _{\theta} \sum_{i=1}^{N} H\left(y_{j}, \hat{\beta}_{j}\right)
\end{aligned}</script><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>最小化交叉熵损失函数与最大似然估计之间的等价并非巧合，同是处理信息的公式，只是应用的方向不同</p><blockquote><p>参考：</p></blockquote></div><footer class="article-footer"><ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/" rel="tag">概率与信息论</a></li></ul><div style="text-align:center;color:#ccc;font-size:14px">- ETX &nbsp;<i class="fe fe-smile"></i>&nbsp;Thank you for reading -</div><div><ul class="post-copyright"><li class="post-copyright-license"><strong>Copyright: </strong>All posts on this blog except otherwise stated, All adopt <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">CC BY-NC-ND 4.0</a> license agreement. Please indicate the source of reprint!</li></ul><div></div></div></footer></div><nav class="article-nav"><a href="/2022/03/16/%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BAshape/" class="article-nav-link"><strong class="article-nav-caption">Newer</strong><div class="article-nav-title">卷积运算的输入输出shape</div></a><a href="/2022/02/23/%E4%BD%BF%E7%94%A8Opencv+Python%E7%9A%84AR%E5%B0%8Fdemo/" class="article-nav-link"><strong class="article-nav-caption">Older</strong><div class="article-nav-title">使用Opencv+Python的AR小demo</div></a></nav><div class="gitalk" id="gitalk-container"></div><link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css"><script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script><script src="https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"1eb16485d4cf892a21bb",clientSecret:"d134888956393ad07790db31a3c50eea40618d43",repo:"gitalkIssue",owner:"aeeeeeep",admin:["aeeeeeep"],id:md5(location.pathname),distractionFreeMode:!1,pagerDirection:"last"});gitalk.render("gitalk-container")</script></article><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></section><footer class="footer"><div class="outer"><div class="float-right"><div class="powered-by">&emsp;<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">Visitors:<span id="busuanzi_value_site_uv"></span></span>&emsp; <i class="fe fe-bookmark"></i>Article Views:<span id="busuanzi_value_page_pv"></span></div></div><ul class="list-inline"><li>Aeeeeeep Blog | The Gleaners &copy; 2023</li><li><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=32090202001024" style="display:inline-block;text-decoration:none;height:20px;line-height:20px"><img src="/images/备案图标.webp" style="float:left"><p style="float:left;height:20px;line-height:20px;margin:0 0 0 5px;color:#1e3e3f">苏公网安备 32090202001024号</p></a></li></ul><div class="float-left"><span id="timeDate">Loading days...</span><span id="times">Loading hours, minutes, and seconds...</span><script>var now=new Date;function createTime(){var n=new Date("11/11/2021 00:08:39");now.setTime(now.getTime()+250),days=(now-n)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-n)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-n)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-n)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("timeDate").innerHTML="Site has been running for "+dnum+" d ",document.getElementById("times").innerHTML=hnum+" h "+mnum+" m "+snum+" s "}setInterval(createTime,250)</script></div></div></footer></main><aside class="sidebar"><button class="navbar-toggle"></button><nav class="navbar"><div class="logo"><a href="/"><img src="/images/aepBlack.svg" alt="Aeeeeeep Blog | The Gleaners"></a></div><ul class="nav nav-main"><li class="nav-item"><a class="nav-item-link" href="/">Home</a></li><li class="nav-item"><a class="nav-item-link" href="/archives">Archives</a></li><li class="nav-item"><a class="nav-item-link" href="/links">Links</a></li><li class="nav-item"><a class="nav-item-link" href="/about">About</a></li><li class="nav-item"><a class="nav-item-link nav-item-search" title="Search"><i class="fe fe-search"></i> Search</a></li></ul></nav><nav class="navbar navbar-bottom"><ul class="nav"><li class="nav-item"><div class="totop" id="totop"><i class="fe fe-rocket"></i></div></li><li class="nav-item"></li></ul></nav><div class="search-form-wrap"><div class="local-search local-search-plugin"><input type="search" id="local-search-input" class="local-search-input" placeholder="Search..."><div id="local-search-result" class="local-search-result"></div></div></div></aside><script src="/js/jquery-2.0.3.min.js"></script><script src="/js/jquery.justifiedGallery.min.js"></script><script src="/js/lazyload.min.js"></script><script src="/js/busuanzi-2.3.pure.min.js"></script><script src="/fancybox/jquery.fancybox.min.js"></script><script src="/js/copybtn.js"></script><script src="/js/tocbot.min.js"></script><script>tocbot.init({tocSelector:".tocbot",contentSelector:".article-entry",headingSelector:"h1, h2, h3, h4, h5, h6",hasInnerContainers:!0,scrollSmooth:!0,positionFixedSelector:".tocbot",positionFixedClass:"is-position-fixed",fixedSidebarOffset:"auto"})</script><script src="/js/ocean.js"></script><script src="/js/cursor.js"></script></body></html>